{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation Strategy - ExpansiÃ³n Inteligente del Dataset\n",
        "**Sistema de AnotaciÃ³n de Video - Entrega 2**\n",
        "\n",
        "## AnÃ¡lisis del EDA Actualizado (90 videos):\n",
        "- **Dataset base:** 6,443 frames (90 videos)\n",
        "- **Balance ratio actual:** 0.51 (Girar: 942 / Caminar Hacia: 1,844)\n",
        "- **Desbalance:** MODERADO (objetivo: >0.90)\n",
        "- **DistribuciÃ³n:**\n",
        "  - Caminar Hacia: 1,844 frames (28.6%)\n",
        "  - Caminar Regreso: 1,301 frames (20.2%)\n",
        "  - Sentarse: 1,253 frames (19.4%)\n",
        "  - Ponerse Pie: 1,103 frames (17.1%)\n",
        "  - Girar: 942 frames (14.6%)\n",
        "\n",
        "## Objetivo:\n",
        "- Balancear a ratio >0.90\n",
        "- Target: ~1,660 frames por actividad\n",
        "- Frames a generar: ~902 adicionales\n",
        "- Dataset final: ~7,345 frames\n",
        "\n",
        "## Estrategia:\n",
        "1. SMOTE para balanceo (prioridad a Girar)\n",
        "2. Rotaciones espaciales (robustez)\n",
        "3. InterpolaciÃ³n temporal (opcional)\n"
      ],
      "metadata": {
        "id": "-4zlnziWaLYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: InstalaciÃ³n y Setup\n",
        "Instalar librerÃ­as necesarias para data augmentation.\n"
      ],
      "metadata": {
        "id": "Mz9CMJL4aRXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YauzHxgOM2B2",
        "outputId": "b9deb82d-0e49-4010-a2df-8898ffee5d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "âœ… LibrerÃ­as para augmentation cargadas\n",
            "ğŸ“Š Pandas: 2.2.2\n",
            "ğŸ”¢ NumPy: 2.0.2\n",
            "ğŸ”¬ Imbalanced-learn: 0.14.0\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias para augmentation\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn scipy\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import imblearn # Import imblearn directly\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.interpolate import CubicSpline\n",
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… LibrerÃ­as para augmentation cargadas\")\n",
        "print(f\"ğŸ“Š Pandas: {pd.__version__}\")\n",
        "print(f\"ğŸ”¢ NumPy: {np.__version__}\")\n",
        "print(f\"ğŸ”¬ Imbalanced-learn: {imblearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Cargar Dataset Base\n",
        "Cargar el dataset limpio de la Entrega 1 para anÃ¡lisis de desbalance.\n"
      ],
      "metadata": {
        "id": "pVBAHlEsaVQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR DATASET BASE DE ENTREGA 1\n",
        "def load_base_dataset():\n",
        "    \"\"\"Cargar dataset base desde Entrega 1\"\"\"\n",
        "    print(\"ğŸ“‚ CARGANDO DATASET BASE DESDE ENTREGA 1\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Ruta relativa desde Entrega2 hacia Entrega1 - Adjusting to load from current directory\n",
        "    landmarks_path = Path(\".\") # Search in the current directory\n",
        "\n",
        "    if not landmarks_path.exists():\n",
        "        print(f\"âŒ No se encontrÃ³: {landmarks_path}\")\n",
        "        print(\"ğŸ’¡ AsegÃºrate de ejecutar desde Entrega2/notebooks/\")\n",
        "        return None\n",
        "\n",
        "    # Cargar todos los CSVs ending with _landmarks.csv\n",
        "    csv_files = list(landmarks_path.glob(\"*_landmarks.csv\"))\n",
        "    print(f\"ğŸ“ Archivos encontrados: {len(csv_files)}\")\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"âŒ No se encontraron archivos CSV de landmarks en el directorio actual.\")\n",
        "        return None\n",
        "\n",
        "    dataframes = []\n",
        "    for csv_file in csv_files:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    # Limpiar datos (remover frames sin detecciÃ³n)\n",
        "    landmark_cols = [col for col in combined_df.columns\n",
        "                    if col not in ['activity', 'video_file', 'frame_number']]\n",
        "\n",
        "    # Solo frames con detecciÃ³n\n",
        "    df_clean = combined_df[(combined_df[landmark_cols] != 0.0).any(axis=1)].copy()\n",
        "\n",
        "    print(f\"ğŸ“Š DATASET CARGADO:\")\n",
        "    print(f\"   Total frames: {len(df_clean):,}\")\n",
        "    print(f\"   Videos: {df_clean['video_file'].nunique()}\")\n",
        "    print(f\"   Actividades: {df_clean['activity'].nunique()}\")\n",
        "\n",
        "    # Mostrar distribuciÃ³n actual\n",
        "    print(f\"\\nğŸ“ˆ DISTRIBUCIÃ“N ACTUAL:\")\n",
        "    activity_counts = df_clean['activity'].value_counts()\n",
        "    for activity, count in activity_counts.items():\n",
        "        pct = count / len(df_clean) * 100\n",
        "        print(f\"   {activity.replace('_', ' ').title()}: {count:,} frames ({pct:.1f}%)\")\n",
        "\n",
        "    balance_ratio = activity_counts.min() / activity_counts.max()\n",
        "    print(f\"\\nâš–ï¸ Balance actual: {balance_ratio:.2f}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Cargar dataset base\n",
        "base_df = load_base_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FIu9U7EaqX3",
        "outputId": "1e5f86e3-54b5-4aff-8d09-5c917557091c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ CARGANDO DATASET BASE DESDE ENTREGA 1\n",
            "==================================================\n",
            "ğŸ“ Archivos encontrados: 90\n",
            "ğŸ“Š DATASET CARGADO:\n",
            "   Total frames: 6,443\n",
            "   Videos: 90\n",
            "   Actividades: 5\n",
            "\n",
            "ğŸ“ˆ DISTRIBUCIÃ“N ACTUAL:\n",
            "   Caminar Hacia: 1,844 frames (28.6%)\n",
            "   Caminar Regreso: 1,301 frames (20.2%)\n",
            "   Sentarse: 1,253 frames (19.4%)\n",
            "   Ponerse Pie: 1,103 frames (17.1%)\n",
            "   Girar: 942 frames (14.6%)\n",
            "\n",
            "âš–ï¸ Balance actual: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 2.5: SPLIT ESTRATIFICADO (INSERTAR ANTES DE PASO 4)\n",
        "# ============================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def split_dataset_before_augmentation(df):\n",
        "    \"\"\"\n",
        "    Split estratificado ANTES de augmentation para evitar leakage\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ”€ SPLIT ESTRATIFICADO DEL DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Preparar features y labels\n",
        "    landmark_cols = [col for col in df.columns\n",
        "                    if col not in ['activity', 'video_file', 'frame_number']]\n",
        "\n",
        "    X = df[landmark_cols]\n",
        "    y = df['activity']\n",
        "\n",
        "    # Split: 70% train, 15% validation, 15% test\n",
        "    # Primero: separar test (15%)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.15,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Segundo: separar train y validation del 85% restante\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=0.1765,  # 15% del total = 17.65% del 85%\n",
        "        stratify=y_temp,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ“Š DISTRIBUCIÃ“N DEL SPLIT:\")\n",
        "    print(f\"   Train:      {len(X_train):,} frames ({len(X_train)/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Validation: {len(X_val):,} frames ({len(X_val)/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Test:       {len(X_test):,} frames ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Verificar balance en cada split\n",
        "    print(f\"\\nâš–ï¸ BALANCE POR SPLIT:\")\n",
        "\n",
        "    for split_name, y_split in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
        "        counts = y_split.value_counts()\n",
        "        ratio = counts.min() / counts.max()\n",
        "        print(f\"\\n   {split_name}:\")\n",
        "        for activity, count in counts.items():\n",
        "            pct = count / len(y_split) * 100\n",
        "            print(f\"      {activity:20s}: {count:4d} ({pct:4.1f}%)\")\n",
        "        print(f\"      Balance ratio: {ratio:.3f}\")\n",
        "\n",
        "    # Crear DataFrames completos\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    val_df = pd.concat([X_val, y_val], axis=1)\n",
        "    test_df = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "    # Agregar metadata\n",
        "    train_df['split'] = 'train'\n",
        "    val_df['split'] = 'validation'\n",
        "    test_df['split'] = 'test'\n",
        "\n",
        "    print(f\"\\nâœ… SPLIT COMPLETADO\")\n",
        "    print(f\"ğŸ’¡ IMPORTANTE: Augmentation se aplicarÃ¡ SOLO al train set\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Ejecutar split\n",
        "if base_df is not None:\n",
        "    train_df, val_df, test_df = split_dataset_before_augmentation(base_df)\n",
        "\n",
        "    print(f\"\\nğŸ“ SETS CREADOS:\")\n",
        "    print(f\"   train_df: {len(train_df):,} frames (para augmentation)\")\n",
        "    print(f\"   val_df:   {len(val_df):,} frames (sin tocar)\")\n",
        "    print(f\"   test_df:  {len(test_df):,} frames (sin tocar)\")\n",
        "else:\n",
        "    print(\"âŒ No hay dataset base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkweYiQGHfZ1",
        "outputId": "ce87c00f-c07f-4e4b-a2c5-694d37608fae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”€ SPLIT ESTRATIFICADO DEL DATASET\n",
            "============================================================\n",
            "ğŸ“Š DISTRIBUCIÃ“N DEL SPLIT:\n",
            "   Train:      4,509 frames (70.0%)\n",
            "   Validation: 967 frames (15.0%)\n",
            "   Test:       967 frames (15.0%)\n",
            "\n",
            "âš–ï¸ BALANCE POR SPLIT:\n",
            "\n",
            "   Train:\n",
            "      caminar_hacia       : 1290 (28.6%)\n",
            "      caminar_regreso     :  911 (20.2%)\n",
            "      sentarse            :  877 (19.4%)\n",
            "      ponerse_pie         :  771 (17.1%)\n",
            "      girar               :  660 (14.6%)\n",
            "      Balance ratio: 0.512\n",
            "\n",
            "   Validation:\n",
            "      caminar_hacia       :  277 (28.6%)\n",
            "      caminar_regreso     :  195 (20.2%)\n",
            "      sentarse            :  188 (19.4%)\n",
            "      ponerse_pie         :  166 (17.2%)\n",
            "      girar               :  141 (14.6%)\n",
            "      Balance ratio: 0.509\n",
            "\n",
            "   Test:\n",
            "      caminar_hacia       :  277 (28.6%)\n",
            "      caminar_regreso     :  195 (20.2%)\n",
            "      sentarse            :  188 (19.4%)\n",
            "      ponerse_pie         :  166 (17.2%)\n",
            "      girar               :  141 (14.6%)\n",
            "      Balance ratio: 0.509\n",
            "\n",
            "âœ… SPLIT COMPLETADO\n",
            "ğŸ’¡ IMPORTANTE: Augmentation se aplicarÃ¡ SOLO al train set\n",
            "\n",
            "ğŸ“ SETS CREADOS:\n",
            "   train_df: 4,509 frames (para augmentation)\n",
            "   val_df:   967 frames (sin tocar)\n",
            "   test_df:  967 frames (sin tocar)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: AnÃ¡lisis de Desbalance\n",
        "Analizar en detalle el desbalance de clases y calcular targets de augmentation.\n"
      ],
      "metadata": {
        "id": "z3drYsopbhfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 3 MODIFICADO: ANÃLISIS SOLO DEL TRAIN SET\n",
        "# ============================================\n",
        "\n",
        "def analyze_train_imbalance(train_df):\n",
        "    \"\"\"Analizar desbalance SOLO del train set\"\"\"\n",
        "    print(\"\\nâš–ï¸ ANÃLISIS DE DESBALANCE - TRAIN SET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    activity_counts = train_df['activity'].value_counts()\n",
        "\n",
        "    # Calcular target conservador (80% de la clase mayoritaria)\n",
        "    # Esto evita over-augmentation\n",
        "    target_frames = int(activity_counts.max() * 0.80)\n",
        "\n",
        "    print(f\"ğŸ¯ ESTRATEGIA CONSERVADORA:\")\n",
        "    print(f\"   Clase mayoritaria: {activity_counts.max():,} frames\")\n",
        "    print(f\"   Target ajustado: {target_frames:,} frames (80% de max)\")\n",
        "    print(f\"   RazÃ³n: Evitar overfitting a datos sintÃ©ticos\")\n",
        "\n",
        "    augmentation_needed = {}\n",
        "    total_augmentation = 0\n",
        "\n",
        "    print(f\"\\nğŸ“Š PLAN DE BALANCEO PARA TRAIN SET:\")\n",
        "\n",
        "    for activity, current_count in activity_counts.items():\n",
        "        needed = max(0, target_frames - current_count)\n",
        "        augmentation_needed[activity] = needed\n",
        "        total_augmentation += needed\n",
        "\n",
        "        if needed > 0:\n",
        "            print(f\"   {activity:20s}: {current_count:4d} â†’ {target_frames:4d} (+{needed:3d})\")\n",
        "        else:\n",
        "            print(f\"   {activity:20s}: {current_count:4d} (âœ… OK)\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ RESUMEN:\")\n",
        "    print(f\"   Train original: {len(train_df):,} frames\")\n",
        "    print(f\"   Frames a generar: {total_augmentation:,}\")\n",
        "    print(f\"   Train final: {len(train_df) + total_augmentation:,} frames\")\n",
        "    print(f\"   Ratio sintÃ©tico: {(total_augmentation/(len(train_df)+total_augmentation))*100:.1f}%\")\n",
        "\n",
        "    # Verificar que ratio sintÃ©tico < 20%\n",
        "    if (total_augmentation / (len(train_df) + total_augmentation)) > 0.20:\n",
        "        print(f\"   âš ï¸  WARNING: Ratio sintÃ©tico alto, considerar reducir target\")\n",
        "    else:\n",
        "        print(f\"   âœ… Ratio sintÃ©tico seguro (<20%)\")\n",
        "\n",
        "    return augmentation_needed, target_frames\n",
        "\n",
        "# Ejecutar anÃ¡lisis del train set\n",
        "if 'train_df' in locals():\n",
        "    train_aug_needed, train_target = analyze_train_imbalance(train_df)\n",
        "else:\n",
        "    print(\"âŒ Primero ejecutar split del dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8of2np6hbXoE",
        "outputId": "03ea9f3f-e1db-4b28-a3d3-102bfb91985d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš–ï¸ ANÃLISIS DE DESBALANCE - TRAIN SET\n",
            "============================================================\n",
            "ğŸ¯ ESTRATEGIA CONSERVADORA:\n",
            "   Clase mayoritaria: 1,290 frames\n",
            "   Target ajustado: 1,032 frames (80% de max)\n",
            "   RazÃ³n: Evitar overfitting a datos sintÃ©ticos\n",
            "\n",
            "ğŸ“Š PLAN DE BALANCEO PARA TRAIN SET:\n",
            "   caminar_hacia       : 1290 (âœ… OK)\n",
            "   caminar_regreso     :  911 â†’ 1032 (+121)\n",
            "   sentarse            :  877 â†’ 1032 (+155)\n",
            "   ponerse_pie         :  771 â†’ 1032 (+261)\n",
            "   girar               :  660 â†’ 1032 (+372)\n",
            "\n",
            "ğŸ¯ RESUMEN:\n",
            "   Train original: 4,509 frames\n",
            "   Frames a generar: 909\n",
            "   Train final: 5,418 frames\n",
            "   Ratio sintÃ©tico: 16.8%\n",
            "   âœ… Ratio sintÃ©tico seguro (<20%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Implementar SMOTE para Balanceo\n",
        "Aplicar SMOTE (Synthetic Minority Oversampling Technique) para generar datos sintÃ©ticos de clases minoritarias.\n"
      ],
      "metadata": {
        "id": "I43OvUCwbo9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 4: APLICAR SMOTE AL TRAIN SET\n",
        "# ============================================\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def apply_smote_to_train(train_df, target_per_class=1032):\n",
        "    \"\"\"\n",
        "    Aplicar SMOTE SOLO al train set segÃºn el plan de balanceo\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ”„ APLICANDO SMOTE AL TRAIN SET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Separar features y labels\n",
        "    landmark_cols = [col for col in train_df.columns\n",
        "                    if col not in ['activity', 'video_file', 'frame_number', 'split']]\n",
        "\n",
        "    X_train = train_df[landmark_cols].values\n",
        "    y_train = train_df['activity'].values\n",
        "\n",
        "    # Mostrar distribuciÃ³n ANTES\n",
        "    print(\"ğŸ“Š DISTRIBUCIÃ“N ANTES DE SMOTE:\")\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    for activity, count in zip(unique, counts):\n",
        "        print(f\"   {activity:20s}: {count:4d} frames\")\n",
        "\n",
        "    original_balance = counts.min() / counts.max()\n",
        "    print(f\"   Balance ratio: {original_balance:.3f}\")\n",
        "\n",
        "    # Calcular estrategia de sampling\n",
        "    activity_counts = train_df['activity'].value_counts()\n",
        "    sampling_strategy = {}\n",
        "\n",
        "    for activity in activity_counts.index:\n",
        "        current_count = activity_counts[activity]\n",
        "        if current_count < target_per_class:\n",
        "            sampling_strategy[activity] = target_per_class\n",
        "\n",
        "    print(f\"\\nğŸ¯ ESTRATEGIA SMOTE:\")\n",
        "    for activity, target in sampling_strategy.items():\n",
        "        current = activity_counts[activity]\n",
        "        print(f\"   {activity:20s}: {current:4d} â†’ {target:4d} (+{target-current:3d})\")\n",
        "\n",
        "    # Aplicar SMOTE\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Convertir sampling_strategy a Ã­ndices\n",
        "    sampling_strategy_encoded = {\n",
        "        label_encoder.transform([activity])[0]: target\n",
        "        for activity, target in sampling_strategy.items()\n",
        "    }\n",
        "\n",
        "    smote = SMOTE(\n",
        "        sampling_strategy=sampling_strategy_encoded,\n",
        "        random_state=42,\n",
        "        k_neighbors=min(5, min(counts)-1)  # Adaptativo\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nâ³ Generando datos sintÃ©ticos con SMOTE...\")\n",
        "        X_train_balanced, y_encoded_balanced = smote.fit_resample(X_train, y_encoded)\n",
        "        y_train_balanced = label_encoder.inverse_transform(y_encoded_balanced)\n",
        "\n",
        "        # Mostrar distribuciÃ³n DESPUÃ‰S\n",
        "        print(f\"\\nğŸ“Š DISTRIBUCIÃ“N DESPUÃ‰S DE SMOTE:\")\n",
        "        unique_after, counts_after = np.unique(y_train_balanced, return_counts=True)\n",
        "\n",
        "        total_generated = 0\n",
        "        for activity, count in zip(unique_after, counts_after):\n",
        "            original_count = activity_counts[activity]\n",
        "            generated = count - original_count\n",
        "            total_generated += generated\n",
        "\n",
        "            if generated > 0:\n",
        "                print(f\"   {activity:20s}: {original_count:4d} â†’ {count:4d} (+{generated:3d} sintÃ©ticos)\")\n",
        "            else:\n",
        "                print(f\"   {activity:20s}: {count:4d} (sin cambios)\")\n",
        "\n",
        "        new_balance = counts_after.min() / counts_after.max()\n",
        "\n",
        "        print(f\"\\nğŸ“ˆ RESUMEN:\")\n",
        "        print(f\"   Frames originales: {len(X_train):,}\")\n",
        "        print(f\"   Frames generados: {total_generated:,}\")\n",
        "        print(f\"   Train final: {len(X_train_balanced):,}\")\n",
        "        print(f\"   Ratio sintÃ©tico: {(total_generated/len(X_train_balanced))*100:.1f}%\")\n",
        "        print(f\"   Balance anterior: {original_balance:.3f}\")\n",
        "        print(f\"   Balance nuevo: {new_balance:.3f}\")\n",
        "        print(f\"   Mejora: {((new_balance-original_balance)/original_balance)*100:+.1f}%\")\n",
        "\n",
        "        # Crear DataFrame balanceado\n",
        "        train_balanced_df = pd.DataFrame(X_train_balanced, columns=landmark_cols)\n",
        "        train_balanced_df['activity'] = y_train_balanced\n",
        "        train_balanced_df['split'] = 'train'\n",
        "\n",
        "        # Marcar datos sintÃ©ticos\n",
        "        train_balanced_df['data_type'] = ['original'] * len(X_train) + \\\n",
        "                                         ['smote'] * (len(X_train_balanced) - len(X_train))\n",
        "\n",
        "        print(f\"\\nâœ… SMOTE APLICADO EXITOSAMENTE AL TRAIN SET\")\n",
        "\n",
        "        return train_balanced_df, total_generated\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error aplicando SMOTE: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "# Ejecutar SMOTE\n",
        "if 'train_df' in locals() and train_df is not None:\n",
        "    train_balanced, smote_generated = apply_smote_to_train(train_df, target_per_class=1032)\n",
        "\n",
        "    if train_balanced is not None:\n",
        "        print(f\"\\nğŸ‰ Train set balanceado listo para entrenamiento\")\n",
        "else:\n",
        "    print(\"âŒ Primero debes tener train_df del split\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy247P31b1D8",
        "outputId": "e6e03931-78d2-4752-a37b-4a8d13bf30f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”„ APLICANDO SMOTE AL TRAIN SET\n",
            "============================================================\n",
            "ğŸ“Š DISTRIBUCIÃ“N ANTES DE SMOTE:\n",
            "   caminar_hacia       : 1290 frames\n",
            "   caminar_regreso     :  911 frames\n",
            "   girar               :  660 frames\n",
            "   ponerse_pie         :  771 frames\n",
            "   sentarse            :  877 frames\n",
            "   Balance ratio: 0.512\n",
            "\n",
            "ğŸ¯ ESTRATEGIA SMOTE:\n",
            "   caminar_regreso     :  911 â†’ 1032 (+121)\n",
            "   sentarse            :  877 â†’ 1032 (+155)\n",
            "   ponerse_pie         :  771 â†’ 1032 (+261)\n",
            "   girar               :  660 â†’ 1032 (+372)\n",
            "\n",
            "â³ Generando datos sintÃ©ticos con SMOTE...\n",
            "\n",
            "ğŸ“Š DISTRIBUCIÃ“N DESPUÃ‰S DE SMOTE:\n",
            "   caminar_hacia       : 1290 (sin cambios)\n",
            "   caminar_regreso     :  911 â†’ 1032 (+121 sintÃ©ticos)\n",
            "   girar               :  660 â†’ 1032 (+372 sintÃ©ticos)\n",
            "   ponerse_pie         :  771 â†’ 1032 (+261 sintÃ©ticos)\n",
            "   sentarse            :  877 â†’ 1032 (+155 sintÃ©ticos)\n",
            "\n",
            "ğŸ“ˆ RESUMEN:\n",
            "   Frames originales: 4,509\n",
            "   Frames generados: 909\n",
            "   Train final: 5,418\n",
            "   Ratio sintÃ©tico: 16.8%\n",
            "   Balance anterior: 0.512\n",
            "   Balance nuevo: 0.800\n",
            "   Mejora: +56.4%\n",
            "\n",
            "âœ… SMOTE APLICADO EXITOSAMENTE AL TRAIN SET\n",
            "\n",
            "ğŸ‰ Train set balanceado listo para entrenamiento\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 7: Combinar Dataset Final Aumentado\n",
        "Combinar todos los datos aumentados (original + SMOTE + rotaciones + interpolaciÃ³n) en dataset final.\n"
      ],
      "metadata": {
        "id": "AW0sWvmLjM58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 7: GUARDAR DATASETS FINALES (CORREGIDO)\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_final_datasets():\n",
        "    \"\"\"\n",
        "    Guardar train/val/test en archivos separados\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ’¾ GUARDANDO DATASETS FINALES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Crear directorio\n",
        "    output_dir = 'data/processed'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Rutas de archivos\n",
        "    train_path = f'{output_dir}/train_balanced.csv'\n",
        "    val_path = f'{output_dir}/val_original.csv'\n",
        "    test_path = f'{output_dir}/test_original.csv'\n",
        "\n",
        "    # Guardar CSVs\n",
        "    print(f\"\\nğŸ“ Guardando archivos...\")\n",
        "    train_balanced.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    # Calcular estadÃ­sticas\n",
        "    train_balance = train_balanced['activity'].value_counts().min() / train_balanced['activity'].value_counts().max()\n",
        "    val_balance = val_df['activity'].value_counts().min() / val_df['activity'].value_counts().max()\n",
        "    test_balance = test_df['activity'].value_counts().min() / test_df['activity'].value_counts().max()\n",
        "\n",
        "    # Balance ORIGINAL (antes de SMOTE) - AÃ‘ADIDO\n",
        "    original_balance = 0.512  # Del anÃ¡lisis inicial\n",
        "\n",
        "    # Contar sintÃ©ticos\n",
        "    smote_count = len(train_balanced[train_balanced['data_type'] == 'smote'])\n",
        "    original_train_count = len(train_balanced[train_balanced['data_type'] == 'original'])\n",
        "\n",
        "    print(f\"âœ… ARCHIVOS GUARDADOS:\")\n",
        "    print(f\"\\n   ğŸ“ {train_path}\")\n",
        "    print(f\"      Total: {len(train_balanced):,} frames\")\n",
        "    print(f\"      â€¢ Original: {original_train_count:,} (83.2%)\")\n",
        "    print(f\"      â€¢ SMOTE: {smote_count:,} (16.8%)\")\n",
        "    print(f\"      â€¢ Balance: {train_balance:.3f}\")\n",
        "\n",
        "    print(f\"\\n   ğŸ“ {val_path}\")\n",
        "    print(f\"      Total: {len(val_df):,} frames\")\n",
        "    print(f\"      â€¢ 100% datos reales\")\n",
        "    print(f\"      â€¢ Balance: {val_balance:.3f}\")\n",
        "\n",
        "    print(f\"\\n   ğŸ“ {test_path}\")\n",
        "    print(f\"      Total: {len(test_df):,} frames\")\n",
        "    print(f\"      â€¢ 100% datos reales\")\n",
        "    print(f\"      â€¢ Balance: {test_balance:.3f}\")\n",
        "\n",
        "    # TamaÃ±os de archivos\n",
        "    train_size = os.path.getsize(train_path) / (1024**2)\n",
        "    val_size = os.path.getsize(val_path) / (1024**2)\n",
        "    test_size = os.path.getsize(test_path) / (1024**2)\n",
        "    total_size = train_size + val_size + test_size\n",
        "\n",
        "    print(f\"\\nğŸ’¾ TAMAÃ‘OS DE ARCHIVOS:\")\n",
        "    print(f\"   Train: {train_size:.2f} MB\")\n",
        "    print(f\"   Validation: {val_size:.2f} MB\")\n",
        "    print(f\"   Test: {test_size:.2f} MB\")\n",
        "    print(f\"   Total: {total_size:.2f} MB\")\n",
        "\n",
        "    # Crear metadata\n",
        "    metadata = {\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'project': 'Sistema de AnotaciÃ³n de Video - Entrega 2',\n",
        "        'total_frames': len(train_balanced) + len(val_df) + len(test_df),\n",
        "        'augmentation_strategy': 'SMOTE only (no leakage)',\n",
        "        'splits': {\n",
        "            'train': {\n",
        "                'file': 'train_balanced.csv',\n",
        "                'frames': len(train_balanced),\n",
        "                'original_frames': original_train_count,\n",
        "                'smote_frames': smote_count,\n",
        "                'balance_ratio': float(train_balance),\n",
        "                'percentage': 70.0,\n",
        "                'activities': train_balanced['activity'].value_counts().to_dict()\n",
        "            },\n",
        "            'validation': {\n",
        "                'file': 'val_original.csv',\n",
        "                'frames': len(val_df),\n",
        "                'data_type': '100% real',\n",
        "                'balance_ratio': float(val_balance),\n",
        "                'percentage': 15.0,\n",
        "                'activities': val_df['activity'].value_counts().to_dict()\n",
        "            },\n",
        "            'test': {\n",
        "                'file': 'test_original.csv',\n",
        "                'frames': len(test_df),\n",
        "                'data_type': '100% real',\n",
        "                'balance_ratio': float(test_balance),\n",
        "                'percentage': 15.0,\n",
        "                'activities': test_df['activity'].value_counts().to_dict()\n",
        "            }\n",
        "        },\n",
        "        'quality_checks': {\n",
        "            'no_data_leakage': True,\n",
        "            'synthetic_ratio_train': float(smote_count / len(train_balanced)),\n",
        "            'synthetic_ratio_total': float(smote_count / (len(train_balanced) + len(val_df) + len(test_df))),\n",
        "            'balance_improvement': f\"{original_balance:.3f} â†’ {train_balance:.3f} (+{((train_balance-original_balance)/original_balance)*100:.1f}%)\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Guardar metadata\n",
        "    metadata_path = f'{output_dir}/metadata.json'\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Metadata guardada: {metadata_path}\")\n",
        "\n",
        "    # Crear README\n",
        "    readme_path = f'{output_dir}/README.txt'\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        f.write(\"DATASET DE LANDMARKS - PROYECTO IA1 ENTREGA 2\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "        f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "        f.write(\"ARCHIVOS:\\n\")\n",
        "        f.write(f\"â€¢ train_balanced.csv: {len(train_balanced):,} frames (con SMOTE)\\n\")\n",
        "        f.write(f\"â€¢ val_original.csv: {len(val_df):,} frames (100% real)\\n\")\n",
        "        f.write(f\"â€¢ test_original.csv: {len(test_df):,} frames (100% real)\\n\\n\")\n",
        "        f.write(\"CARACTERÃSTICAS:\\n\")\n",
        "        f.write(f\"â€¢ Balance train: {train_balance:.3f}\\n\")\n",
        "        f.write(f\"â€¢ Ratio sintÃ©tico train: {(smote_count/len(train_balanced))*100:.1f}%\\n\")\n",
        "        f.write(f\"â€¢ Val/Test: 100% datos reales\\n\")\n",
        "        f.write(\"â€¢ Sin data leakage: SMOTE aplicado solo a train\\n\\n\")\n",
        "        f.write(\"USO EN NOTEBOOK 4 (MODELADO):\\n\")\n",
        "        f.write(\"1. train_balanced.csv â†’ Entrenamiento + cross-validation\\n\")\n",
        "        f.write(\"2. val_original.csv â†’ Ajuste de hiperparÃ¡metros\\n\")\n",
        "        f.write(\"3. test_original.csv â†’ EvaluaciÃ³n final ÃšNICA\\n\")\n",
        "\n",
        "    print(f\"ğŸ“„ README creado: {readme_path}\")\n",
        "\n",
        "    print(f\"\\nâœ… GUARDADO COMPLETADO\")\n",
        "\n",
        "    return {\n",
        "        'train': train_path,\n",
        "        'val': val_path,\n",
        "        'test': test_path,\n",
        "        'metadata': metadata_path\n",
        "    }\n",
        "\n",
        "# EJECUTAR GUARDADO\n",
        "paths = save_final_datasets()\n",
        "\n",
        "if paths:\n",
        "    print(f\"\\n\" + \"ğŸ‰\" * 30)\n",
        "    print(f\"ğŸ‰ NOTEBOOK 3 - DATA AUGMENTATION COMPLETADO ğŸ‰\")\n",
        "    print(\"ğŸ‰\" * 30)\n",
        "\n",
        "    print(f\"\\nğŸ“Š RESUMEN EJECUTIVO:\")\n",
        "    print(f\"   Dataset original: 6,443 frames\")\n",
        "    print(f\"   Dataset final: 7,352 frames\")\n",
        "    print(f\"   â€¢ Train: 5,418 (con SMOTE)\")\n",
        "    print(f\"   â€¢ Validation: 967 (real)\")\n",
        "    print(f\"   â€¢ Test: 967 (real)\")\n",
        "\n",
        "    print(f\"\\nâœ… GARANTÃAS DE CALIDAD:\")\n",
        "    print(f\"   âœ“ Sin data leakage\")\n",
        "    print(f\"   âœ“ Balance mejorado: 0.512 â†’ 0.800\")\n",
        "    print(f\"   âœ“ Ratio sintÃ©tico seguro: 16.8%\")\n",
        "    print(f\"   âœ“ Val/Test 100% reales\")\n",
        "\n",
        "    print(f\"\\nğŸ“ ARCHIVOS LISTOS EN: data/processed/\")\n",
        "    print(f\"   â€¢ train_balanced.csv\")\n",
        "    print(f\"   â€¢ val_original.csv\")\n",
        "    print(f\"   â€¢ test_original.csv\")\n",
        "    print(f\"   â€¢ metadata.json\")\n",
        "    print(f\"   â€¢ README.txt\")\n",
        "\n",
        "    print(f\"\\nğŸš€ SIGUIENTE PASO:\")\n",
        "    print(f\"   Notebook 4: Entrenar modelos con train_balanced.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQgZCFEUoxRa",
        "outputId": "11bf0ce6-bbae-41bd-8069-3d34e69649d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ GUARDANDO DATASETS FINALES\n",
            "============================================================\n",
            "\n",
            "ğŸ“ Guardando archivos...\n",
            "âœ… ARCHIVOS GUARDADOS:\n",
            "\n",
            "   ğŸ“ data/processed/train_balanced.csv\n",
            "      Total: 5,418 frames\n",
            "      â€¢ Original: 4,509 (83.2%)\n",
            "      â€¢ SMOTE: 909 (16.8%)\n",
            "      â€¢ Balance: 0.800\n",
            "\n",
            "   ğŸ“ data/processed/val_original.csv\n",
            "      Total: 967 frames\n",
            "      â€¢ 100% datos reales\n",
            "      â€¢ Balance: 0.509\n",
            "\n",
            "   ğŸ“ data/processed/test_original.csv\n",
            "      Total: 967 frames\n",
            "      â€¢ 100% datos reales\n",
            "      â€¢ Balance: 0.509\n",
            "\n",
            "ğŸ’¾ TAMAÃ‘OS DE ARCHIVOS:\n",
            "   Train: 6.44 MB\n",
            "   Validation: 1.14 MB\n",
            "   Test: 1.14 MB\n",
            "   Total: 8.72 MB\n",
            "\n",
            "ğŸ“‹ Metadata guardada: data/processed/metadata.json\n",
            "ğŸ“„ README creado: data/processed/README.txt\n",
            "\n",
            "âœ… GUARDADO COMPLETADO\n",
            "\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "ğŸ‰ NOTEBOOK 3 - DATA AUGMENTATION COMPLETADO ğŸ‰\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "\n",
            "ğŸ“Š RESUMEN EJECUTIVO:\n",
            "   Dataset original: 6,443 frames\n",
            "   Dataset final: 7,352 frames\n",
            "   â€¢ Train: 5,418 (con SMOTE)\n",
            "   â€¢ Validation: 967 (real)\n",
            "   â€¢ Test: 967 (real)\n",
            "\n",
            "âœ… GARANTÃAS DE CALIDAD:\n",
            "   âœ“ Sin data leakage\n",
            "   âœ“ Balance mejorado: 0.512 â†’ 0.800\n",
            "   âœ“ Ratio sintÃ©tico seguro: 16.8%\n",
            "   âœ“ Val/Test 100% reales\n",
            "\n",
            "ğŸ“ ARCHIVOS LISTOS EN: data/processed/\n",
            "   â€¢ train_balanced.csv\n",
            "   â€¢ val_original.csv\n",
            "   â€¢ test_original.csv\n",
            "   â€¢ metadata.json\n",
            "   â€¢ README.txt\n",
            "\n",
            "ğŸš€ SIGUIENTE PASO:\n",
            "   Notebook 4: Entrenar modelos con train_balanced.csv\n"
          ]
        }
      ]
    }
  ]
}