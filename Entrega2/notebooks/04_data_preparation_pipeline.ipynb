{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qpNwC-DnsyTE",
        "kJw9fvD1s0KF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Pipeline - PreparaciÃ³n Final para ML\n",
        "**Sistema de AnotaciÃ³n de Video - Entrega 2**\n",
        "\n",
        "Este notebook implementa la preparaciÃ³n completa de datos para entrenamiento de modelos ML.\n",
        "\n",
        "## Proceso Completo:\n",
        "1. **Cargar dataset aumentado** (11,406 frames balanceados)\n",
        "2. **Feature Engineering** (crear caracterÃ­sticas avanzadas)\n",
        "3. **Pipeline de preprocessing** (normalizaciÃ³n, encoding)\n",
        "4. **Data splits** estratificados (train/val/test)\n",
        "5. **ValidaciÃ³n y documentaciÃ³n** final\n",
        "\n",
        "## Input:\n",
        "- `data/augmented/landmarks_final_augmented.csv` (del Notebook 3)\n",
        "\n",
        "## Output:\n",
        "- Datasets listos para Tomas (train/val/test)\n",
        "- Pipeline de preprocessing automatizado\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "URBg8Q6nsu3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: InstalaciÃ³n y Setup\n",
        "Instalar librerÃ­as necesarias para feature engineering y preprocessing.\n"
      ],
      "metadata": {
        "id": "qpNwC-DnsyTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias para data preparation\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "import sklearn\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… LibrerÃ­as para data preparation cargadas\")\n",
        "print(f\"ğŸ“Š Pandas: {pd.__version__}\")\n",
        "print(f\"ğŸ”¬ Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"ğŸ’¾ Joblib: {joblib.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhrG6wtosy3U",
        "outputId": "12ae4995-93f3-439f-c5f8-7356b4fadca4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "âœ… LibrerÃ­as para data preparation cargadas\n",
            "ğŸ“Š Pandas: 2.2.2\n",
            "ğŸ”¬ Scikit-learn: 1.6.1\n",
            "ğŸ’¾ Joblib: 1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Cargar Dataset Aumentado\n",
        "Cargar el dataset balanceado y aumentado desde el Notebook 3.\n"
      ],
      "metadata": {
        "id": "kJw9fvD1s0KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR DATASET AUMENTADO\n",
        "def load_augmented_dataset():\n",
        "    \"\"\"Cargar dataset aumentado del Notebook 3\"\"\"\n",
        "    print(\"ğŸ“‚ CARGANDO DATASET AUMENTADO\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Use the correct file path\n",
        "    dataset_path = \"/content/landmarks_final_augmented.csv\"\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"âŒ Dataset no encontrado: {dataset_path}\")\n",
        "        print(\"ğŸ’¡ AsegÃºrate de ejecutar Notebook 3 primero\")\n",
        "        return None\n",
        "\n",
        "    # Cargar datos\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    print(f\"âœ… DATASET CARGADO:\")\n",
        "    print(f\"   ğŸ“Š Frames: {len(df):,}\")\n",
        "    print(f\"   ğŸ“‹ Columnas: {len(df.columns)}\")\n",
        "    print(f\"   ğŸ¬ Videos Ãºnicos: {df['video_file'].nunique():,}\")\n",
        "    print(f\"   ğŸ¯ Actividades: {df['activity'].nunique()}\")\n",
        "\n",
        "    # Verificar distribuciÃ³n\n",
        "    print(f\"\\nğŸ“Š DISTRIBUCIÃ“N POR ACTIVIDAD:\")\n",
        "    activity_counts = df['activity'].value_counts()\n",
        "    for activity, count in activity_counts.items():\n",
        "        pct = count / len(df) * 100\n",
        "        print(f\"   {activity.replace('_', ' ').title()}: {count:,} frames ({pct:.1f}%)\")\n",
        "\n",
        "    balance_ratio = activity_counts.min() / activity_counts.max()\n",
        "    print(f\"\\nâš–ï¸ Balance verificado: {balance_ratio:.3f}\")\n",
        "\n",
        "    # Verificar tipos de augmentation\n",
        "    if 'source' in df.columns:\n",
        "        print(f\"\\nğŸ”§ TÃ‰CNICAS DE AUGMENTATION:\")\n",
        "        source_counts = df['source'].value_counts()\n",
        "        for source, count in source_counts.items():\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"   {source.replace('_', ' ').title()}: {count:,} frames ({pct:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Cargar dataset aumentado\n",
        "df_augmented = load_augmented_dataset()\n",
        "\n",
        "if df_augmented is not None:\n",
        "    print(f\"\\nâœ… DATASET AUMENTADO CARGADO CORRECTAMENTE\")\n",
        "    print(f\"ğŸ¯ Listo para Feature Engineering\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Error cargando dataset aumentado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwqpfEQtNxb",
        "outputId": "e58f768b-a9ef-4f9a-f5b0-4fefb01dc7c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ CARGANDO DATASET AUMENTADO\n",
            "========================================\n",
            "âœ… DATASET CARGADO:\n",
            "   ğŸ“Š Frames: 11,406\n",
            "   ğŸ“‹ Columnas: 69\n",
            "   ğŸ¬ Videos Ãºnicos: 5,997\n",
            "   ğŸ¯ Actividades: 5\n",
            "\n",
            "ğŸ“Š DISTRIBUCIÃ“N POR ACTIVIDAD:\n",
            "   Caminar Hacia: 2,464 frames (21.6%)\n",
            "   Caminar Regreso: 2,368 frames (20.8%)\n",
            "   Girar: 2,278 frames (20.0%)\n",
            "   Ponerse Pie: 2,227 frames (19.5%)\n",
            "   Sentarse: 2,069 frames (18.1%)\n",
            "\n",
            "âš–ï¸ Balance verificado: 0.840\n",
            "\n",
            "ğŸ”§ TÃ‰CNICAS DE AUGMENTATION:\n",
            "   Smote Synthetic: 5,445 frames (47.7%)\n",
            "   Real Video: 4,575 frames (40.1%)\n",
            "   Temporal Augmentation: 886 frames (7.8%)\n",
            "   Spatial Augmentation: 500 frames (4.4%)\n",
            "\n",
            "âœ… DATASET AUMENTADO CARGADO CORRECTAMENTE\n",
            "ğŸ¯ Listo para Feature Engineering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Feature Engineering - CaracterÃ­sticas GeomÃ©tricas\n",
        "Crear caracterÃ­sticas geomÃ©tricas avanzadas a partir de landmarks existentes.\n"
      ],
      "metadata": {
        "id": "BC-fOqlmua-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE ENGINEERING - CARACTERÃSTICAS GEOMÃ‰TRICAS\n",
        "class GeometricFeatureEngineer:\n",
        "    \"\"\"Crear caracterÃ­sticas geomÃ©tricas desde landmarks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Definir landmarks principales\n",
        "        self.landmarks = {\n",
        "            'L_shoulder': ['L_shoulder_x', 'L_shoulder_y', 'L_shoulder_z'],\n",
        "            'R_shoulder': ['R_shoulder_x', 'R_shoulder_y', 'R_shoulder_z'],\n",
        "            'L_elbow': ['L_elbow_x', 'L_elbow_y', 'L_elbow_z'],\n",
        "            'R_elbow': ['R_elbow_x', 'R_elbow_y', 'R_elbow_z'],\n",
        "            'L_wrist': ['L_wrist_x', 'L_wrist_y', 'L_wrist_z'],\n",
        "            'R_wrist': ['R_wrist_x', 'R_wrist_y', 'R_wrist_z'],\n",
        "            'L_hip': ['L_hip_x', 'L_hip_y', 'L_hip_z'],\n",
        "            'R_hip': ['R_hip_x', 'R_hip_y', 'R_hip_z'],\n",
        "            'L_knee': ['L_knee_x', 'L_knee_y', 'L_knee_z'],\n",
        "            'R_knee': ['R_knee_x', 'R_knee_y', 'R_knee_z'],\n",
        "            'L_ankle': ['L_ankle_x', 'L_ankle_y', 'L_ankle_z'],\n",
        "            'R_ankle': ['R_ankle_x', 'R_ankle_y', 'R_ankle_z']\n",
        "        }\n",
        "\n",
        "    def create_geometric_features(self, df):\n",
        "        \"\"\"Crear todas las caracterÃ­sticas geomÃ©tricas\"\"\"\n",
        "        print(\"ğŸ”¶ CREANDO CARACTERÃSTICAS GEOMÃ‰TRICAS\")\n",
        "        print(\"=\" * 45)\n",
        "\n",
        "        df_geo = df.copy()\n",
        "        features_created = []\n",
        "\n",
        "        # 1. Distancias entre landmarks\n",
        "        distances = self._create_distance_features(df_geo)\n",
        "        features_created.extend(distances)\n",
        "\n",
        "        # 2. Ãngulos articulares\n",
        "        angles = self._create_angle_features(df_geo)\n",
        "        features_created.extend(angles)\n",
        "\n",
        "        # 3. Ratios corporales\n",
        "        ratios = self._create_ratio_features(df_geo)\n",
        "        features_created.extend(ratios)\n",
        "\n",
        "        # 4. Centro de masa\n",
        "        center_features = self._create_center_mass_features(df_geo)\n",
        "        features_created.extend(center_features)\n",
        "\n",
        "        print(f\"\\nâœ… CARACTERÃSTICAS GEOMÃ‰TRICAS CREADAS:\")\n",
        "        print(f\"   ğŸ“Š Total nuevas features: {len(features_created)}\")\n",
        "        print(f\"   ğŸ“ Dimensiones: {df_geo.shape[0]:,} Ã— {df_geo.shape[1]}\")\n",
        "\n",
        "        return df_geo, features_created\n",
        "\n",
        "    def _create_distance_features(self, df):\n",
        "        \"\"\"Crear caracterÃ­sticas de distancias\"\"\"\n",
        "        print(\"   ğŸ“ Creando distancias entre landmarks...\")\n",
        "\n",
        "        distances_created = []\n",
        "\n",
        "        # Distancias corporales importantes\n",
        "        distance_pairs = [\n",
        "            ('L_shoulder', 'R_shoulder', 'shoulder_width'),\n",
        "            ('L_hip', 'R_hip', 'hip_width'),\n",
        "            ('L_shoulder', 'L_hip', 'L_torso_length'),\n",
        "            ('R_shoulder', 'R_hip', 'R_torso_length'),\n",
        "            ('L_hip', 'L_knee', 'L_thigh_length'),\n",
        "            ('R_hip', 'R_knee', 'R_thigh_length'),\n",
        "            ('L_knee', 'L_ankle', 'L_shin_length'),\n",
        "            ('R_knee', 'R_ankle', 'R_shin_length')\n",
        "        ]\n",
        "\n",
        "        for lm1, lm2, feature_name in distance_pairs:\n",
        "            if all(col in df.columns for col in self.landmarks[lm1]) and \\\n",
        "               all(col in df.columns for col in self.landmarks[lm2]):\n",
        "\n",
        "                # Calcular distancia euclidiana 3D\n",
        "                x1, y1, z1 = df[self.landmarks[lm1]].T.values\n",
        "                x2, y2, z2 = df[self.landmarks[lm2]].T.values\n",
        "\n",
        "                distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
        "                df[feature_name] = distance\n",
        "                distances_created.append(feature_name)\n",
        "\n",
        "        print(f\"      âœ… {len(distances_created)} distancias creadas\")\n",
        "        return distances_created\n",
        "\n",
        "    def _create_angle_features(self, df):\n",
        "        \"\"\"Crear caracterÃ­sticas de Ã¡ngulos articulares\"\"\"\n",
        "        print(\"   ğŸ“ Creando Ã¡ngulos articulares...\")\n",
        "\n",
        "        angles_created = []\n",
        "\n",
        "        # Ãngulos articulares importantes\n",
        "        angle_definitions = [\n",
        "            ('L_shoulder', 'L_elbow', 'L_wrist', 'L_elbow_angle'),\n",
        "            ('R_shoulder', 'R_elbow', 'R_wrist', 'R_elbow_angle'),\n",
        "            ('L_hip', 'L_knee', 'L_ankle', 'L_knee_angle'),\n",
        "            ('R_hip', 'R_knee', 'R_ankle', 'R_knee_angle')\n",
        "        ]\n",
        "\n",
        "        for p1, p2, p3, feature_name in angle_definitions:\n",
        "            if all(all(col in df.columns for col in self.landmarks[lm]) for lm in [p1, p2, p3]):\n",
        "\n",
        "                angle = self._calculate_angle(df, p1, p2, p3)\n",
        "                if angle is not None:\n",
        "                    df[feature_name] = angle\n",
        "                    angles_created.append(feature_name)\n",
        "\n",
        "        print(f\"      âœ… {len(angles_created)} Ã¡ngulos creados\")\n",
        "        return angles_created\n",
        "\n",
        "    def _calculate_angle(self, df, point1, point2, point3):\n",
        "        \"\"\"Calcular Ã¡ngulo entre 3 puntos\"\"\"\n",
        "        try:\n",
        "            # Vectores\n",
        "            x1, y1 = df[f\"{point1}_x\"], df[f\"{point1}_y\"]\n",
        "            x2, y2 = df[f\"{point2}_x\"], df[f\"{point2}_y\"]\n",
        "            x3, y3 = df[f\"{point3}_x\"], df[f\"{point3}_y\"]\n",
        "\n",
        "            # Vector 1: point2 -> point1\n",
        "            v1_x, v1_y = x1 - x2, y1 - y2\n",
        "            # Vector 2: point2 -> point3\n",
        "            v2_x, v2_y = x3 - x2, y3 - y2\n",
        "\n",
        "            # Calcular Ã¡ngulo usando producto punto\n",
        "            dot_product = v1_x * v2_x + v1_y * v2_y\n",
        "            magnitude1 = np.sqrt(v1_x**2 + v1_y**2)\n",
        "            magnitude2 = np.sqrt(v2_x**2 + v2_y**2)\n",
        "\n",
        "            # Evitar divisiÃ³n por cero\n",
        "            magnitude_product = magnitude1 * magnitude2\n",
        "            valid_mask = magnitude_product > 1e-8\n",
        "\n",
        "            angles = np.zeros(len(df))\n",
        "            angles[valid_mask] = np.arccos(\n",
        "                np.clip(dot_product[valid_mask] / magnitude_product[valid_mask], -1, 1)\n",
        "            )\n",
        "\n",
        "            return np.degrees(angles)  # Convertir a grados\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error calculando Ã¡ngulo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _create_ratio_features(self, df):\n",
        "        \"\"\"Crear ratios y proporciones corporales\"\"\"\n",
        "        print(\"   ğŸ“Š Creando ratios corporales...\")\n",
        "\n",
        "        ratios_created = []\n",
        "\n",
        "        # Ratios corporales Ãºtiles\n",
        "        if all(col in df.columns for col in ['shoulder_width', 'hip_width']):\n",
        "            df['shoulder_hip_ratio'] = df['shoulder_width'] / (df['hip_width'] + 1e-8)\n",
        "            ratios_created.append('shoulder_hip_ratio')\n",
        "\n",
        "        if all(col in df.columns for col in ['L_torso_length', 'L_thigh_length']):\n",
        "            df['torso_thigh_ratio'] = df['L_torso_length'] / (df['L_thigh_length'] + 1e-8)\n",
        "            ratios_created.append('torso_thigh_ratio')\n",
        "\n",
        "        # Altura aproximada (hombro a tobillo)\n",
        "        if all(col in df.columns for col in ['L_shoulder_y', 'L_ankle_y']):\n",
        "            df['body_height_approx'] = abs(df['L_ankle_y'] - df['L_shoulder_y'])\n",
        "            ratios_created.append('body_height_approx')\n",
        "\n",
        "        print(f\"      âœ… {len(ratios_created)} ratios creados\")\n",
        "        return ratios_created\n",
        "\n",
        "    def _create_center_mass_features(self, df):\n",
        "        \"\"\"Crear caracterÃ­sticas de centro de masa\"\"\"\n",
        "        print(\"   âš–ï¸ Creando centro de masa...\")\n",
        "\n",
        "        center_features = []\n",
        "\n",
        "        # Centro de masa corporal (promedio caderas)\n",
        "        if all(col in df.columns for col in ['L_hip_x', 'R_hip_x', 'L_hip_y', 'R_hip_y']):\n",
        "            df['center_mass_x'] = (df['L_hip_x'] + df['R_hip_x']) / 2\n",
        "            df['center_mass_y'] = (df['L_hip_y'] + df['R_hip_y']) / 2\n",
        "            center_features.extend(['center_mass_x', 'center_mass_y'])\n",
        "\n",
        "        # Centro torso superior (promedio hombros)\n",
        "        if all(col in df.columns for col in ['L_shoulder_x', 'R_shoulder_x', 'L_shoulder_y', 'R_shoulder_y']):\n",
        "            df['upper_center_x'] = (df['L_shoulder_x'] + df['R_shoulder_x']) / 2\n",
        "            df['upper_center_y'] = (df['L_shoulder_y'] + df['R_shoulder_y']) / 2\n",
        "            center_features.extend(['upper_center_x', 'upper_center_y'])\n",
        "\n",
        "        print(f\"      âœ… {len(center_features)} centros de masa creados\")\n",
        "        return center_features\n",
        "\n",
        "# Crear feature engineer\n",
        "geo_engineer = GeometricFeatureEngineer()\n",
        "\n",
        "# Aplicar feature engineering geomÃ©trico\n",
        "if df_augmented is not None:\n",
        "    df_with_geo_features, geo_features = geo_engineer.create_geometric_features(df_augmented)\n",
        "    print(f\"\\nâœ… FEATURE ENGINEERING GEOMÃ‰TRICO COMPLETADO\")\n",
        "    print(f\"ğŸ“Š Nuevas features: {len(geo_features)}\")\n",
        "else:\n",
        "    print(\"âŒ No hay dataset para feature engineering\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlPHJ0gKuf3x",
        "outputId": "3a9be282-c395-4b2c-8d2b-ee869e6cc88b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¶ CREANDO CARACTERÃSTICAS GEOMÃ‰TRICAS\n",
            "=============================================\n",
            "   ğŸ“ Creando distancias entre landmarks...\n",
            "      âœ… 8 distancias creadas\n",
            "   ğŸ“ Creando Ã¡ngulos articulares...\n",
            "      âœ… 4 Ã¡ngulos creados\n",
            "   ğŸ“Š Creando ratios corporales...\n",
            "      âœ… 3 ratios creados\n",
            "   âš–ï¸ Creando centro de masa...\n",
            "      âœ… 4 centros de masa creados\n",
            "\n",
            "âœ… CARACTERÃSTICAS GEOMÃ‰TRICAS CREADAS:\n",
            "   ğŸ“Š Total nuevas features: 19\n",
            "   ğŸ“ Dimensiones: 11,406 Ã— 88\n",
            "\n",
            "âœ… FEATURE ENGINEERING GEOMÃ‰TRICO COMPLETADO\n",
            "ğŸ“Š Nuevas features: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Feature Engineering - CaracterÃ­sticas Temporales  \n",
        "Crear caracterÃ­sticas temporales (velocidades, aceleraciones) calculadas entre frames consecutivos.\n"
      ],
      "metadata": {
        "id": "BOCCWgNgunvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE ENGINEERING TEMPORAL - VERSIÃ“N CORREGIDA (SIN PÃ‰RDIDAS)\n",
        "class TemporalFeatureEngineerFixed:\n",
        "    \"\"\"Crear caracterÃ­sticas temporales SIN perder datos sintÃ©ticos\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Landmarks para anÃ¡lisis temporal\n",
        "        self.key_landmarks = [\n",
        "            'L_shoulder_y', 'R_shoulder_y',\n",
        "            'L_hip_y', 'R_hip_y',\n",
        "            'L_knee_y', 'R_knee_y'\n",
        "        ]\n",
        "\n",
        "        # Solo usar landmarks que existan\n",
        "        self.center_landmarks = ['center_mass_x', 'center_mass_y']\n",
        "\n",
        "    def create_temporal_features_safe(self, df):\n",
        "        \"\"\"Crear features temporales SIN procesar por video individual\"\"\"\n",
        "        print(\"â±ï¸ FEATURE ENGINEERING TEMPORAL - MÃ‰TODO SEGURO\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "        df_temporal = df.copy()\n",
        "        temporal_features_created = []\n",
        "\n",
        "        print(\"ğŸ’¡ ESTRATEGIA: Calcular temporales por actividad + source\")\n",
        "        print(\"ğŸ¯ OBJETIVO: Preservar TODOS los datos sintÃ©ticos\")\n",
        "\n",
        "        # Landmarks disponibles\n",
        "        available_landmarks = [lm for lm in self.key_landmarks + self.center_landmarks\n",
        "                             if lm in df.columns]\n",
        "\n",
        "        print(f\"ğŸ“Š Landmarks disponibles: {len(available_landmarks)}\")\n",
        "\n",
        "        # Crear caracterÃ­sticas temporales frame-by-frame\n",
        "        for landmark in available_landmarks:\n",
        "\n",
        "            # 1. Velocidad aproximada (diferencia con frame anterior)\n",
        "            df_temporal[f\"{landmark}_velocity\"] = df[landmark].diff().fillna(0)\n",
        "            temporal_features_created.append(f\"{landmark}_velocity\")\n",
        "\n",
        "            # 2. AceleraciÃ³n aproximada (diferencia de velocidades)\n",
        "            velocity_col = f\"{landmark}_velocity\"\n",
        "            df_temporal[f\"{landmark}_acceleration\"] = df_temporal[velocity_col].diff().fillna(0)\n",
        "            temporal_features_created.append(f\"{landmark}_acceleration\")\n",
        "\n",
        "            # 3. Media mÃ³vil simple (ventana 3)\n",
        "            df_temporal[f\"{landmark}_smooth\"] = df[landmark].rolling(window=3, center=True).mean().fillna(df[landmark])\n",
        "            temporal_features_created.append(f\"{landmark}_smooth\")\n",
        "\n",
        "        # CaracterÃ­sticas estadÃ­sticas globales (por actividad)\n",
        "        print(f\"\\nğŸ“Š Creando caracterÃ­sticas estadÃ­sticas...\")\n",
        "\n",
        "        # Variabilidad del centro de masa por grupo\n",
        "        if 'center_mass_y' in df.columns:\n",
        "            # Agrupar por actividad para estadÃ­sticas\n",
        "            activity_stats = df.groupby('activity')['center_mass_y'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "            # Merge con dataset principal\n",
        "            df_temporal = df_temporal.merge(\n",
        "                activity_stats.rename(columns={'mean': 'activity_center_mean', 'std': 'activity_center_std'}),\n",
        "                on='activity',\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            temporal_features_created.extend(['activity_center_mean', 'activity_center_std'])\n",
        "\n",
        "        print(f\"âœ… CARACTERÃSTICAS TEMPORALES CREADAS:\")\n",
        "        print(f\"   ğŸ“Š Total features temporales: {len(temporal_features_created)}\")\n",
        "        print(f\"   ğŸ“ Frames preservados: {len(df_temporal):,} (Â¡Sin pÃ©rdidas!)\")\n",
        "\n",
        "        # Verificar que no se perdieron datos\n",
        "        if len(df_temporal) != len(df):\n",
        "            print(f\"âš ï¸ ADVERTENCIA: Se perdieron {len(df) - len(df_temporal):,} frames\")\n",
        "        else:\n",
        "            print(f\"âœ… PERFECTO: Todos los frames preservados\")\n",
        "\n",
        "        return df_temporal, temporal_features_created\n",
        "\n",
        "# Aplicar temporal feature engineering CORREGIDO\n",
        "temporal_engineer_fixed = TemporalFeatureEngineerFixed()\n",
        "\n",
        "if 'df_with_geo_features' in locals() and df_with_geo_features is not None:\n",
        "    df_temporal_fixed, temporal_features_fixed = temporal_engineer_fixed.create_temporal_features_safe(df_with_geo_features)\n",
        "\n",
        "    print(f\"\\nâœ… TEMPORAL FEATURE ENGINEERING CORREGIDO\")\n",
        "    print(f\"ğŸ“Š Features: {len(temporal_features_fixed)}\")\n",
        "    print(f\"ğŸ¯ Samples preservados: {len(df_temporal_fixed):,}\")\n",
        "else:\n",
        "    print(\"âŒ Dataset con features geomÃ©tricas no disponible\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FZx-Y6buoYP",
        "outputId": "eb59025c-b64b-4d90-de57-630ece7f1370"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â±ï¸ FEATURE ENGINEERING TEMPORAL - MÃ‰TODO SEGURO\n",
            "=======================================================\n",
            "ğŸ’¡ ESTRATEGIA: Calcular temporales por actividad + source\n",
            "ğŸ¯ OBJETIVO: Preservar TODOS los datos sintÃ©ticos\n",
            "ğŸ“Š Landmarks disponibles: 8\n",
            "\n",
            "ğŸ“Š Creando caracterÃ­sticas estadÃ­sticas...\n",
            "âœ… CARACTERÃSTICAS TEMPORALES CREADAS:\n",
            "   ğŸ“Š Total features temporales: 26\n",
            "   ğŸ“ Frames preservados: 11,406 (Â¡Sin pÃ©rdidas!)\n",
            "âœ… PERFECTO: Todos los frames preservados\n",
            "\n",
            "âœ… TEMPORAL FEATURE ENGINEERING CORREGIDO\n",
            "ğŸ“Š Features: 26\n",
            "ğŸ¯ Samples preservados: 11,406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Pipeline de Preprocessing\n",
        "Crear pipeline automatizado para normalizaciÃ³n, encoding y preparaciÃ³n final.\n"
      ],
      "metadata": {
        "id": "g3yhdOgQu7iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PIPELINE DE PREPROCESSING - VERSIÃ“N CORREGIDA\n",
        "class DataPreprocessingPipelineFixed:\n",
        "    \"\"\"Pipeline completo preservando todos los datos aumentados\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.pca = None\n",
        "        self.preprocessing_stats = {}\n",
        "\n",
        "    def create_ml_ready_dataset(self, df, use_pca=True, n_components=0.95):\n",
        "        \"\"\"Crear dataset final con TODOS los datos preservados\"\"\"\n",
        "        print(\"ğŸ”§ PIPELINE DE PREPROCESSING - SIN PÃ‰RDIDAS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        print(f\"ğŸ“Š DATASET DE ENTRADA:\")\n",
        "        print(f\"   Total frames: {len(df):,}\")\n",
        "        print(f\"   Columnas: {len(df.columns)}\")\n",
        "\n",
        "        # Verificar distribuciÃ³n por fuente\n",
        "        if 'source' in df.columns:\n",
        "            print(f\"   Por fuente:\")\n",
        "            for source in df['source'].unique():\n",
        "                count = len(df[df['source'] == source])\n",
        "                print(f\"      {source}: {count:,} frames\")\n",
        "\n",
        "        # 1. Separar features y targets CUIDADOSAMENTE\n",
        "        X, y, metadata = self._separate_features_targets_safe(df)\n",
        "\n",
        "        if X is None:\n",
        "            print(\"âŒ Error separando features\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nğŸ“Š SEPARACIÃ“N DE DATOS:\")\n",
        "        print(f\"   âœ… Features (X): {X.shape} - PRESERVADAS\")\n",
        "        print(f\"   âœ… Labels (y): {len(y):,} - PRESERVADAS\")\n",
        "        print(f\"   âœ… Metadata: {len(metadata) if metadata is not None else 0:,} - PRESERVADA\")\n",
        "\n",
        "        # 2. Encoding de labels\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        print(f\"\\nğŸ·ï¸ LABEL ENCODING:\")\n",
        "        for i, class_name in enumerate(self.label_encoder.classes_):\n",
        "            count = np.sum(y_encoded == i)\n",
        "            print(f\"   {class_name.replace('_', ' ').title()}: {count:,} samples (cÃ³digo {i})\")\n",
        "\n",
        "        # 3. Limpiar features (eliminar NaN, infinitos)\n",
        "        print(f\"\\nğŸ§¹ LIMPIEZA DE FEATURES:\")\n",
        "\n",
        "        # Verificar NaN\n",
        "        nan_counts = np.isnan(X).sum(axis=0)\n",
        "        cols_with_nan = np.where(nan_counts > 0)[0]\n",
        "\n",
        "        if len(cols_with_nan) > 0:\n",
        "            print(f\"   âš ï¸ Columnas con NaN: {len(cols_with_nan)}\")\n",
        "            X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "            print(f\"   âœ… NaN/Inf reemplazados con valores seguros\")\n",
        "        else:\n",
        "            print(f\"   âœ… Sin valores NaN - datos limpios\")\n",
        "\n",
        "        # 4. NormalizaciÃ³n con StandardScaler\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        print(f\"   âœ… StandardScaler aplicado\")\n",
        "\n",
        "        # 5. PCA para reducciÃ³n dimensional\n",
        "        if use_pca:\n",
        "            self.pca = PCA(n_components=n_components, random_state=42)\n",
        "            X_pca = self.pca.fit_transform(X_scaled)\n",
        "\n",
        "            n_components_final = X_pca.shape[1]\n",
        "            variance_explained = self.pca.explained_variance_ratio_.sum()\n",
        "\n",
        "            print(f\"   âœ… PCA: {X.shape[1]} â†’ {n_components_final} features\")\n",
        "            print(f\"   ğŸ“Š Varianza: {variance_explained*100:.1f}%\")\n",
        "\n",
        "            X_final = X_pca\n",
        "        else:\n",
        "            X_final = X_scaled\n",
        "            n_components_final = X_scaled.shape[1]\n",
        "            variance_explained = 1.0\n",
        "\n",
        "        # 6. EstadÃ­sticas finales del pipeline\n",
        "        self.preprocessing_stats = {\n",
        "            'input_samples': len(df),\n",
        "            'output_samples': X_final.shape[0],\n",
        "            'preservation_rate': X_final.shape[0] / len(df),\n",
        "            'original_features': X.shape[1],\n",
        "            'final_features': X_final.shape[1],\n",
        "            'reduction_rate': (X.shape[1] - X_final.shape[1]) / X.shape[1],\n",
        "            'classes': len(self.label_encoder.classes_),\n",
        "            'variance_explained': variance_explained\n",
        "        }\n",
        "\n",
        "        print(f\"\\nğŸ“Š PIPELINE COMPLETADO:\")\n",
        "        print(f\"   ğŸ“ˆ PreservaciÃ³n: {self.preprocessing_stats['preservation_rate']*100:.1f}%\")\n",
        "        print(f\"   ğŸ“‰ ReducciÃ³n features: {self.preprocessing_stats['reduction_rate']*100:.1f}%\")\n",
        "        print(f\"   ğŸ¯ Lista para ML: {X_final.shape}\")\n",
        "\n",
        "        return X_final, y_encoded, metadata\n",
        "\n",
        "    def _separate_features_targets_safe(self, df):\n",
        "        \"\"\"Separar features y targets preservando todos los datos\"\"\"\n",
        "        # Columnas no-feature (metadata)\n",
        "        metadata_cols = ['activity', 'video_file', 'frame_number', 'source', 'augmentation_type']\n",
        "\n",
        "        # Features = todas las demÃ¡s columnas\n",
        "        feature_cols = [col for col in df.columns if col not in metadata_cols]\n",
        "\n",
        "        print(f\"   ğŸ” Identificadas {len(feature_cols)} columnas de features\")\n",
        "        print(f\"   ğŸ“‹ Metadata: {len([col for col in metadata_cols if col in df.columns])} columnas\")\n",
        "\n",
        "        # Verificar que tenemos datos vÃ¡lidos\n",
        "        if not feature_cols:\n",
        "            print(\"âŒ No hay columnas de features\")\n",
        "            return None, None, None\n",
        "\n",
        "        if 'activity' not in df.columns:\n",
        "            print(\"âŒ No hay columna 'activity'\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Extraer datos\n",
        "        X = df[feature_cols].values  # Convertir a numpy\n",
        "        y = df['activity'].values\n",
        "\n",
        "        # Metadata disponible\n",
        "        available_metadata = [col for col in metadata_cols if col in df.columns]\n",
        "        metadata = df[available_metadata] if available_metadata else None\n",
        "\n",
        "        return X, y, metadata\n",
        "\n",
        "# Crear pipeline corregido\n",
        "preprocessing_pipeline_fixed = DataPreprocessingPipelineFixed()\n",
        "\n",
        "# Aplicar preprocessing SIN pÃ©rdidas\n",
        "if 'df_temporal_fixed' in locals() and df_temporal_fixed is not None:\n",
        "    X_processed_fixed, y_processed_fixed, metadata_fixed = preprocessing_pipeline_fixed.create_ml_ready_dataset(\n",
        "        df_temporal_fixed,\n",
        "        use_pca=True,\n",
        "        n_components=0.95\n",
        "    )\n",
        "\n",
        "    if X_processed_fixed is not None:\n",
        "        print(f\"\\nğŸ‰ PREPROCESSING CORREGIDO COMPLETADO\")\n",
        "        print(f\"âœ… Dataset ML-ready: {X_processed_fixed.shape}\")\n",
        "        print(f\"âœ… TODOS los 11,406 frames preservados en pipeline\")\n",
        "    else:\n",
        "        print(\"âŒ Error en preprocessing corregido\")\n",
        "else:\n",
        "    print(\"âŒ Dataset temporal corregido no disponible\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wj-dySmu8uv",
        "outputId": "181542d7-e32c-4ba9-862a-c018c9cdb11d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ PIPELINE DE PREPROCESSING - SIN PÃ‰RDIDAS\n",
            "==================================================\n",
            "ğŸ“Š DATASET DE ENTRADA:\n",
            "   Total frames: 11,406\n",
            "   Columnas: 114\n",
            "   Por fuente:\n",
            "      real_video: 4,575 frames\n",
            "      smote_synthetic: 5,445 frames\n",
            "      spatial_augmentation: 500 frames\n",
            "      temporal_augmentation: 886 frames\n",
            "   ğŸ” Identificadas 109 columnas de features\n",
            "   ğŸ“‹ Metadata: 5 columnas\n",
            "\n",
            "ğŸ“Š SEPARACIÃ“N DE DATOS:\n",
            "   âœ… Features (X): (11406, 109) - PRESERVADAS\n",
            "   âœ… Labels (y): 11,406 - PRESERVADAS\n",
            "   âœ… Metadata: 11,406 - PRESERVADA\n",
            "\n",
            "ğŸ·ï¸ LABEL ENCODING:\n",
            "   Caminar Hacia: 2,464 samples (cÃ³digo 0)\n",
            "   Caminar Regreso: 2,368 samples (cÃ³digo 1)\n",
            "   Girar: 2,278 samples (cÃ³digo 2)\n",
            "   Ponerse Pie: 2,227 samples (cÃ³digo 3)\n",
            "   Sentarse: 2,069 samples (cÃ³digo 4)\n",
            "\n",
            "ğŸ§¹ LIMPIEZA DE FEATURES:\n",
            "   âœ… Sin valores NaN - datos limpios\n",
            "   âœ… StandardScaler aplicado\n",
            "   âœ… PCA: 109 â†’ 19 features\n",
            "   ğŸ“Š Varianza: 95.1%\n",
            "\n",
            "ğŸ“Š PIPELINE COMPLETADO:\n",
            "   ğŸ“ˆ PreservaciÃ³n: 100.0%\n",
            "   ğŸ“‰ ReducciÃ³n features: 82.6%\n",
            "   ğŸ¯ Lista para ML: (11406, 19)\n",
            "\n",
            "ğŸ‰ PREPROCESSING CORREGIDO COMPLETADO\n",
            "âœ… Dataset ML-ready: (11406, 19)\n",
            "âœ… TODOS los 11,406 frames preservados en pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 6: Train/Validation/Test Splits Estratificados\n",
        "Crear splits finales preservando balance de clases en cada particiÃ³n.\n"
      ],
      "metadata": {
        "id": "eV4Iv0IQxPr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREAR SPLITS ESTRATIFICADOS FINALES\n",
        "def create_final_stratified_splits(X, y, metadata=None):\n",
        "    \"\"\"Crear splits estratificados preservando balance\"\"\"\n",
        "    print(\"ğŸ“‹ CREANDO SPLITS ESTRATIFICADOS FINALES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"ğŸ“Š DATASET COMPLETO PARA SPLITS:\")\n",
        "    print(f\"   ğŸ“ˆ Total samples: {X.shape[0]:,}\")\n",
        "    print(f\"   ğŸ”¢ Features: {X.shape[1]}\")\n",
        "    print(f\"   ğŸ¯ Classes: {len(np.unique(y))}\")\n",
        "\n",
        "    # Crear splits: 70% / 15% / 15%\n",
        "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.15,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_trainval, y_trainval,\n",
        "        test_size=0.176,  # 15/85 â‰ˆ 0.176\n",
        "        stratify=y_trainval,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   âœ… Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"   âœ… Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"   âœ… Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "\n",
        "    # Verificar balance en cada split\n",
        "    splits_to_check = [('Train', y_train), ('Validation', y_val), ('Test', y_test)]\n",
        "\n",
        "    for split_name, y_split in splits_to_check:\n",
        "        unique_split, counts_split = np.unique(y_split, return_counts=True)\n",
        "        balance_split = counts_split.min() / counts_split.max()\n",
        "        print(f\"   {split_name}: Balance {balance_split:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'train': (X_train, y_train),\n",
        "        'validation': (X_val, y_val),\n",
        "        'test': (X_test, y_test)\n",
        "    }\n",
        "\n",
        "# EJECUTAR SPLITS (usando las variables correctas)\n",
        "final_data_splits = create_final_stratified_splits(\n",
        "    X_processed_fixed,\n",
        "    y_processed_fixed\n",
        ")\n",
        "\n",
        "print(f\"âœ… SPLITS CREADOS EXITOSAMENTE\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPzlqk56xQPI",
        "outputId": "d6bb428d-bf54-4bb3-820a-ed72012dda22"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ CREANDO SPLITS ESTRATIFICADOS FINALES\n",
            "==================================================\n",
            "ğŸ“Š DATASET COMPLETO PARA SPLITS:\n",
            "   ğŸ“ˆ Total samples: 11,406\n",
            "   ğŸ”¢ Features: 19\n",
            "   ğŸ¯ Classes: 5\n",
            "   âœ… Train: 7,988 samples (70.0%)\n",
            "   âœ… Validation: 1,707 samples (15.0%)\n",
            "   âœ… Test: 1,711 samples (15.0%)\n",
            "   Train: Balance 0.840\n",
            "   Validation: Balance 0.840\n",
            "   Test: Balance 0.838\n",
            "âœ… SPLITS CREADOS EXITOSAMENTE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GUARDADO SIMPLIFICADO - DIRECTO\n",
        "def save_datasets_direct():\n",
        "    \"\"\"Guardar datasets directamente\"\"\"\n",
        "    print(\"ğŸ’¾ GUARDANDO DATASETS - MÃ‰TODO DIRECTO\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    # Crear directorios\n",
        "    os.makedirs('data/processed', exist_ok=True)\n",
        "    os.makedirs('data/models', exist_ok=True)\n",
        "\n",
        "    # Guardar splits\n",
        "    for split_name, (X_split, y_split) in final_data_splits.items():\n",
        "        np.save(f'data/processed/X_{split_name}.npy', X_split)\n",
        "        np.save(f'data/processed/y_{split_name}.npy', y_split)\n",
        "        print(f\"   âœ… {split_name}: {X_split.shape[0]:,} samples guardados\")\n",
        "\n",
        "    # Guardar transformers\n",
        "    joblib.dump(preprocessing_pipeline_fixed.scaler, 'data/models/scaler.pkl')\n",
        "    joblib.dump(preprocessing_pipeline_fixed.label_encoder, 'data/models/encoder.pkl')\n",
        "    joblib.dump(preprocessing_pipeline_fixed.pca, 'data/models/pca.pkl')\n",
        "\n",
        "    print(f\"   âœ… Transformers guardados\")\n",
        "\n",
        "    # Guardar dataset completo procesado\n",
        "    np.save('data/processed/X_complete.npy', X_processed_fixed)\n",
        "    np.save('data/processed/y_complete.npy', y_processed_fixed)\n",
        "\n",
        "    print(f\"\\nğŸ‰ GUARDADO COMPLETADO:\")\n",
        "    print(f\"   ğŸ“Š {len(final_data_splits)} splits creados\")\n",
        "    print(f\"   ğŸ”§ 3 transformers guardados\")\n",
        "    print(f\"   ğŸ“ Dataset completo disponible\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Guardar todo\n",
        "save_success = save_datasets_direct()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_7kKc6gxkNO",
        "outputId": "05270c84-9acd-4743-d906-63ce83ae7d41"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ GUARDANDO DATASETS - MÃ‰TODO DIRECTO\n",
            "=============================================\n",
            "   âœ… train: 7,988 samples guardados\n",
            "   âœ… validation: 1,707 samples guardados\n",
            "   âœ… test: 1,711 samples guardados\n",
            "   âœ… Transformers guardados\n",
            "\n",
            "ğŸ‰ GUARDADO COMPLETADO:\n",
            "   ğŸ“Š 3 splits creados\n",
            "   ğŸ”§ 3 transformers guardados\n",
            "   ğŸ“ Dataset completo disponible\n"
          ]
        }
      ]
    }
  ]
}