{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qpNwC-DnsyTE",
        "kJw9fvD1s0KF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Pipeline - Preparaci√≥n Final para ML\n",
        "**Sistema de Anotaci√≥n de Video - Entrega 2**\n",
        "\n",
        "Este notebook implementa la preparaci√≥n completa de datos para entrenamiento de modelos ML **sin data leakage**.\n",
        "\n",
        "## Proceso Completo:\n",
        "1. **Cargar 3 datasets separados** (train/val/test del Notebook 3)\n",
        "2. **Feature Engineering** (crear caracter√≠sticas geom√©tricas)\n",
        "3. **Pipeline de preprocessing** (normalizaci√≥n SOLO en train)\n",
        "4. **Validaci√≥n y guardar** archivos finales para modelado\n",
        "\n",
        "## Input:\n",
        "- `train_balanced.csv` (5,418 frames con SMOTE)\n",
        "- `val_original.csv` (967 frames 100% reales)\n",
        "- `test_original.csv` (967 frames 100% reales)\n",
        "\n",
        "## Output:\n",
        "- **X_train, y_train** (preprocesados con PCA)\n",
        "- **X_val, y_val** (transformados con pipeline de train)\n",
        "- **X_test, y_test** (transformados con pipeline de train)\n",
        "- **Pipelines guardados** (scaler, label_encoder, pca)\n",
        "\n",
        "## ‚ö†Ô∏è GARANT√çA:\n",
        "\n",
        "‚úÖ **Sin data leakage:** Val/Test nunca participan en fit()\n",
        "\n",
        "‚úÖ **Balance correcto:** Train 0.800, Val/Test 0.509\n",
        "\n",
        "‚úÖ **Reproducible:** Pipelines guardados para producci√≥n\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "URBg8Q6nsu3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Instalaci√≥n y Setup\n",
        "Instalar librer√≠as necesarias para feature engineering y preprocessing.\n"
      ],
      "metadata": {
        "id": "qpNwC-DnsyTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 1: INSTALACI√ìN Y SETUP\n",
        "# ============================================\n",
        "\n",
        "# Instalar dependencias\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn joblib -q\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "import sklearn\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Librer√≠as para data preparation cargadas\")\n",
        "print(f\"üìä Pandas: {pd.__version__}\")\n",
        "print(f\"üî¨ Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"üíæ Joblib: {joblib.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhrG6wtosy3U",
        "outputId": "29d08a86-b05d-4550-e2aa-63d4e9aad050"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Librer√≠as para data preparation cargadas\n",
            "üìä Pandas: 2.2.2\n",
            "üî¨ Scikit-learn: 1.6.1\n",
            "üíæ Joblib: 1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Cargar Dataset Aumentado\n",
        "Cargar el dataset balanceado y aumentado desde el Notebook 3.\n"
      ],
      "metadata": {
        "id": "kJw9fvD1s0KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 2: CARGAR DATASETS SEPARADOS (CORREGIDO)\n",
        "# ============================================\n",
        "\n",
        "def load_prepared_datasets():\n",
        "    \"\"\"\n",
        "    Cargar los 3 datasets del Notebook 3 (YA SEPARADOS)\n",
        "    \"\"\"\n",
        "    print(\"\\nüìÇ CARGANDO DATASETS PREPARADOS (NOTEBOOK 3)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Rutas de archivos\n",
        "    train_path = 'train_balanced.csv'\n",
        "    val_path = 'val_original.csv'\n",
        "    test_path = 'test_original.csv'\n",
        "\n",
        "    # Verificar existencia\n",
        "    missing_files = []\n",
        "    for path in [train_path, val_path, test_path]:\n",
        "        if not os.path.exists(path):\n",
        "            missing_files.append(path)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"‚ùå ARCHIVOS NO ENCONTRADOS:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"   ‚Ä¢ {f}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Cargar CSVs\n",
        "    print(f\"üìÅ Cargando archivos...\")\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ DATASETS CARGADOS:\")\n",
        "    print(f\"   Train: {len(train_df):,} frames\")\n",
        "    print(f\"   Validation: {len(val_df):,} frames\")\n",
        "    print(f\"   Test: {len(test_df):,} frames\")\n",
        "    print(f\"   Total: {len(train_df) + len(val_df) + len(test_df):,} frames\")\n",
        "\n",
        "    # Verificar estructura\n",
        "    print(f\"\\nüìã VERIFICACI√ìN DE ESTRUCTURA:\")\n",
        "    print(f\"   Columnas train: {len(train_df.columns)}\")\n",
        "    print(f\"   Columnas val: {len(val_df.columns)}\")\n",
        "    print(f\"   Columnas test: {len(test_df.columns)}\")\n",
        "\n",
        "    # Verificar composici√≥n de TRAIN\n",
        "    if 'data_type' in train_df.columns:\n",
        "        print(f\"\\nüîç COMPOSICI√ìN TRAIN:\")\n",
        "        for dtype, count in train_df['data_type'].value_counts().items():\n",
        "            pct = count / len(train_df) * 100\n",
        "            print(f\"   {dtype:10s}: {count:5,} ({pct:4.1f}%)\")\n",
        "\n",
        "    # Verificar balance\n",
        "    print(f\"\\n‚öñÔ∏è  BALANCE POR DATASET:\")\n",
        "    for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
        "        counts = df['activity'].value_counts()\n",
        "        balance = counts.min() / counts.max()\n",
        "\n",
        "        print(f\"\\n   {name}:\")\n",
        "        for activity, count in counts.items():\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"      {activity:20s}: {count:4,} ({pct:4.1f}%)\")\n",
        "        print(f\"      Balance ratio: {balance:.3f}\")\n",
        "\n",
        "    # Verificar data leakage (SOLO si existe video_file)\n",
        "    print(f\"\\nüîí VERIFICACI√ìN DE LEAKAGE:\")\n",
        "\n",
        "    # Chequear qu√© datasets tienen video_file\n",
        "    has_video_file = {\n",
        "        'train': 'video_file' in train_df.columns,\n",
        "        'val': 'video_file' in val_df.columns,\n",
        "        'test': 'video_file' in test_df.columns\n",
        "    }\n",
        "\n",
        "    if all(has_video_file.values()):\n",
        "        # Todos tienen video_file, verificar overlap\n",
        "        train_videos = set(train_df['video_file'].unique())\n",
        "        val_videos = set(val_df['video_file'].unique())\n",
        "        test_videos = set(test_df['video_file'].unique())\n",
        "\n",
        "        overlap_train_val = train_videos & val_videos\n",
        "        overlap_train_test = train_videos & test_videos\n",
        "        overlap_val_test = val_videos & test_videos\n",
        "\n",
        "        if overlap_train_val or overlap_train_test or overlap_val_test:\n",
        "            print(f\"   ‚ö†Ô∏è  POSIBLE LEAKAGE DETECTADO:\")\n",
        "            if overlap_train_val:\n",
        "                print(f\"      Train-Val overlap: {len(overlap_train_val)} videos\")\n",
        "            if overlap_train_test:\n",
        "                print(f\"      Train-Test overlap: {len(overlap_train_test)} videos\")\n",
        "            if overlap_val_test:\n",
        "                print(f\"      Val-Test overlap: {len(overlap_val_test)} videos\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ SIN LEAKAGE: Videos √∫nicos en cada split\")\n",
        "    else:\n",
        "        # Algunos no tienen video_file\n",
        "        print(f\"   ‚ÑπÔ∏è  Columna 'video_file' no disponible en todos los datasets:\")\n",
        "        for name, has_col in has_video_file.items():\n",
        "            status = \"‚úÖ\" if has_col else \"‚ùå\"\n",
        "            print(f\"      {status} {name}\")\n",
        "        print(f\"   üí° Verificaci√≥n de leakage omitida\")\n",
        "        print(f\"   ‚úÖ Datasets ya est√°n correctamente separados por Notebook 3\")\n",
        "\n",
        "    print(f\"\\n‚úÖ DATASETS LISTOS PARA FEATURE ENGINEERING\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# EJECUTAR CARGA\n",
        "train_df, val_df, test_df = load_prepared_datasets()\n",
        "\n",
        "if all(df is not None for df in [train_df, val_df, test_df]):\n",
        "    print(f\"\\nüéâ CARGA EXITOSA\")\n",
        "\n",
        "    # Mostrar columnas √∫nicas para debug\n",
        "    print(f\"\\nüîç DIFERENCIAS DE COLUMNAS:\")\n",
        "    train_cols = set(train_df.columns)\n",
        "    val_cols = set(val_df.columns)\n",
        "    test_cols = set(test_df.columns)\n",
        "\n",
        "    only_in_train = train_cols - val_cols - test_cols\n",
        "    if only_in_train:\n",
        "        print(f\"   üìã Solo en train: {only_in_train}\")\n",
        "\n",
        "    common_cols = train_cols & val_cols & test_cols\n",
        "    print(f\"   ‚úÖ Columnas comunes: {len(common_cols)}\")\n",
        "\n",
        "    print(f\"\\nüöÄ Listo para continuar con Feature Engineering\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERROR EN CARGA\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwqpfEQtNxb",
        "outputId": "bb77b5fd-6943-4fb2-83ab-ec6de5d7c803"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÇ CARGANDO DATASETS PREPARADOS (NOTEBOOK 3)\n",
            "============================================================\n",
            "üìÅ Cargando archivos...\n",
            "\n",
            "‚úÖ DATASETS CARGADOS:\n",
            "   Train: 5,418 frames\n",
            "   Validation: 967 frames\n",
            "   Test: 967 frames\n",
            "   Total: 7,352 frames\n",
            "\n",
            "üìã VERIFICACI√ìN DE ESTRUCTURA:\n",
            "   Columnas train: 67\n",
            "   Columnas val: 66\n",
            "   Columnas test: 66\n",
            "\n",
            "üîç COMPOSICI√ìN TRAIN:\n",
            "   original  : 4,509 (83.2%)\n",
            "   smote     :   909 (16.8%)\n",
            "\n",
            "‚öñÔ∏è  BALANCE POR DATASET:\n",
            "\n",
            "   Train:\n",
            "      caminar_hacia       : 1,290 (23.8%)\n",
            "      girar               : 1,032 (19.0%)\n",
            "      ponerse_pie         : 1,032 (19.0%)\n",
            "      sentarse            : 1,032 (19.0%)\n",
            "      caminar_regreso     : 1,032 (19.0%)\n",
            "      Balance ratio: 0.800\n",
            "\n",
            "   Val:\n",
            "      caminar_hacia       :  277 (28.6%)\n",
            "      caminar_regreso     :  195 (20.2%)\n",
            "      sentarse            :  188 (19.4%)\n",
            "      ponerse_pie         :  166 (17.2%)\n",
            "      girar               :  141 (14.6%)\n",
            "      Balance ratio: 0.509\n",
            "\n",
            "   Test:\n",
            "      caminar_hacia       :  277 (28.6%)\n",
            "      caminar_regreso     :  195 (20.2%)\n",
            "      sentarse            :  188 (19.4%)\n",
            "      ponerse_pie         :  166 (17.2%)\n",
            "      girar               :  141 (14.6%)\n",
            "      Balance ratio: 0.509\n",
            "\n",
            "üîí VERIFICACI√ìN DE LEAKAGE:\n",
            "   ‚ÑπÔ∏è  Columna 'video_file' no disponible en todos los datasets:\n",
            "      ‚ùå train\n",
            "      ‚ùå val\n",
            "      ‚ùå test\n",
            "   üí° Verificaci√≥n de leakage omitida\n",
            "   ‚úÖ Datasets ya est√°n correctamente separados por Notebook 3\n",
            "\n",
            "‚úÖ DATASETS LISTOS PARA FEATURE ENGINEERING\n",
            "\n",
            "üéâ CARGA EXITOSA\n",
            "\n",
            "üîç DIFERENCIAS DE COLUMNAS:\n",
            "   üìã Solo en train: {'data_type'}\n",
            "   ‚úÖ Columnas comunes: 66\n",
            "\n",
            "üöÄ Listo para continuar con Feature Engineering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Feature Engineering - Caracter√≠sticas Geom√©tricas\n",
        "Crear caracter√≠sticas geom√©tricas avanzadas a partir de landmarks existentes.\n"
      ],
      "metadata": {
        "id": "BC-fOqlmua-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 3: FEATURE ENGINEERING (SIN LEAKAGE)\n",
        "# ============================================\n",
        "\n",
        "class GeometricFeatureEngineer:\n",
        "    \"\"\"Crear caracter√≠sticas geom√©tricas desde landmarks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Definir landmarks principales\n",
        "        self.landmarks = {\n",
        "            'L_shoulder': ['L_shoulder_x', 'L_shoulder_y', 'L_shoulder_z'],\n",
        "            'R_shoulder': ['R_shoulder_x', 'R_shoulder_y', 'R_shoulder_z'],\n",
        "            'L_elbow': ['L_elbow_x', 'L_elbow_y', 'L_elbow_z'],\n",
        "            'R_elbow': ['R_elbow_x', 'R_elbow_y', 'R_elbow_z'],\n",
        "            'L_wrist': ['L_wrist_x', 'L_wrist_y', 'L_wrist_z'],\n",
        "            'R_wrist': ['R_wrist_x', 'R_wrist_y', 'R_wrist_z'],\n",
        "            'L_hip': ['L_hip_x', 'L_hip_y', 'L_hip_z'],\n",
        "            'R_hip': ['R_hip_x', 'R_hip_y', 'R_hip_z'],\n",
        "            'L_knee': ['L_knee_x', 'L_knee_y', 'L_knee_z'],\n",
        "            'R_knee': ['R_knee_x', 'R_knee_y', 'R_knee_z'],\n",
        "            'L_ankle': ['L_ankle_x', 'L_ankle_y', 'L_ankle_z'],\n",
        "            'R_ankle': ['R_ankle_x', 'R_ankle_y', 'R_ankle_z']\n",
        "        }\n",
        "\n",
        "    def create_geometric_features(self, df, dataset_name='Dataset'):\n",
        "        \"\"\"Crear todas las caracter√≠sticas geom√©tricas\"\"\"\n",
        "        print(f\"\\nüî∂ FEATURE ENGINEERING: {dataset_name}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        df_geo = df.copy()\n",
        "        features_created = []\n",
        "\n",
        "        # 1. Distancias entre landmarks\n",
        "        distances = self._create_distance_features(df_geo)\n",
        "        features_created.extend(distances)\n",
        "\n",
        "        # 2. √Ångulos articulares\n",
        "        angles = self._create_angle_features(df_geo)\n",
        "        features_created.extend(angles)\n",
        "\n",
        "        # 3. Ratios corporales\n",
        "        ratios = self._create_ratio_features(df_geo)\n",
        "        features_created.extend(ratios)\n",
        "\n",
        "        # 4. Centro de masa\n",
        "        center_features = self._create_center_mass_features(df_geo)\n",
        "        features_created.extend(center_features)\n",
        "\n",
        "        print(f\"\\n‚úÖ FEATURES CREADAS PARA {dataset_name}:\")\n",
        "        print(f\"   üìä Total nuevas features: {len(features_created)}\")\n",
        "        print(f\"   üìê Dimensiones finales: {df_geo.shape[0]:,} √ó {df_geo.shape[1]}\")\n",
        "\n",
        "        return df_geo, features_created\n",
        "\n",
        "    def _create_distance_features(self, df):\n",
        "        \"\"\"Crear caracter√≠sticas de distancias\"\"\"\n",
        "        print(\"   üìè Creando distancias entre landmarks...\")\n",
        "\n",
        "        distances_created = []\n",
        "\n",
        "        # Distancias corporales importantes\n",
        "        distance_pairs = [\n",
        "            ('L_shoulder', 'R_shoulder', 'shoulder_width'),\n",
        "            ('L_hip', 'R_hip', 'hip_width'),\n",
        "            ('L_shoulder', 'L_hip', 'L_torso_length'),\n",
        "            ('R_shoulder', 'R_hip', 'R_torso_length'),\n",
        "            ('L_hip', 'L_knee', 'L_thigh_length'),\n",
        "            ('R_hip', 'R_knee', 'R_thigh_length'),\n",
        "            ('L_knee', 'L_ankle', 'L_shin_length'),\n",
        "            ('R_knee', 'R_ankle', 'R_shin_length')\n",
        "        ]\n",
        "\n",
        "        for lm1, lm2, feature_name in distance_pairs:\n",
        "            if all(col in df.columns for col in self.landmarks[lm1]) and \\\n",
        "               all(col in df.columns for col in self.landmarks[lm2]):\n",
        "\n",
        "                # Calcular distancia euclidiana 3D\n",
        "                x1, y1, z1 = df[self.landmarks[lm1]].T.values\n",
        "                x2, y2, z2 = df[self.landmarks[lm2]].T.values\n",
        "\n",
        "                distance = np.sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
        "                df[feature_name] = distance\n",
        "                distances_created.append(feature_name)\n",
        "\n",
        "        print(f\"      ‚úÖ {len(distances_created)} distancias creadas\")\n",
        "        return distances_created\n",
        "\n",
        "    def _create_angle_features(self, df):\n",
        "        \"\"\"Crear caracter√≠sticas de √°ngulos articulares\"\"\"\n",
        "        print(\"   üìê Creando √°ngulos articulares...\")\n",
        "\n",
        "        angles_created = []\n",
        "\n",
        "        # √Ångulos articulares importantes\n",
        "        angle_definitions = [\n",
        "            ('L_shoulder', 'L_elbow', 'L_wrist', 'L_elbow_angle'),\n",
        "            ('R_shoulder', 'R_elbow', 'R_wrist', 'R_elbow_angle'),\n",
        "            ('L_hip', 'L_knee', 'L_ankle', 'L_knee_angle'),\n",
        "            ('R_hip', 'R_knee', 'R_ankle', 'R_knee_angle')\n",
        "        ]\n",
        "\n",
        "        for p1, p2, p3, feature_name in angle_definitions:\n",
        "            if all(all(col in df.columns for col in self.landmarks[lm]) for lm in [p1, p2, p3]):\n",
        "\n",
        "                angle = self._calculate_angle(df, p1, p2, p3)\n",
        "                if angle is not None:\n",
        "                    df[feature_name] = angle\n",
        "                    angles_created.append(feature_name)\n",
        "\n",
        "        print(f\"      ‚úÖ {len(angles_created)} √°ngulos creados\")\n",
        "        return angles_created\n",
        "\n",
        "    def _calculate_angle(self, df, point1, point2, point3):\n",
        "        \"\"\"Calcular √°ngulo entre 3 puntos\"\"\"\n",
        "        try:\n",
        "            # Vectores\n",
        "            x1, y1 = df[f\"{point1}_x\"], df[f\"{point1}_y\"]\n",
        "            x2, y2 = df[f\"{point2}_x\"], df[f\"{point2}_y\"]\n",
        "            x3, y3 = df[f\"{point3}_x\"], df[f\"{point3}_y\"]\n",
        "\n",
        "            # Vector 1: point2 -> point1\n",
        "            v1_x, v1_y = x1 - x2, y1 - y2\n",
        "            # Vector 2: point2 -> point3\n",
        "            v2_x, v2_y = x3 - x2, y3 - y2\n",
        "\n",
        "            # Calcular √°ngulo usando producto punto\n",
        "            dot_product = v1_x * v2_x + v1_y * v2_y\n",
        "            magnitude1 = np.sqrt(v1_x**2 + v1_y**2)\n",
        "            magnitude2 = np.sqrt(v2_x**2 + v2_y**2)\n",
        "\n",
        "            # Evitar divisi√≥n por cero\n",
        "            magnitude_product = magnitude1 * magnitude2\n",
        "            valid_mask = magnitude_product > 1e-8\n",
        "\n",
        "            angles = np.zeros(len(df))\n",
        "            angles[valid_mask] = np.arccos(\n",
        "                np.clip(dot_product[valid_mask] / magnitude_product[valid_mask], -1, 1)\n",
        "            )\n",
        "\n",
        "            return np.degrees(angles)  # Convertir a grados\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error calculando √°ngulo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _create_ratio_features(self, df):\n",
        "        \"\"\"Crear ratios y proporciones corporales\"\"\"\n",
        "        print(\"   üìä Creando ratios corporales...\")\n",
        "\n",
        "        ratios_created = []\n",
        "\n",
        "        # Ratios corporales √∫tiles\n",
        "        if all(col in df.columns for col in ['shoulder_width', 'hip_width']):\n",
        "            df['shoulder_hip_ratio'] = df['shoulder_width'] / (df['hip_width'] + 1e-8)\n",
        "            ratios_created.append('shoulder_hip_ratio')\n",
        "\n",
        "        if all(col in df.columns for col in ['L_torso_length', 'L_thigh_length']):\n",
        "            df['torso_thigh_ratio'] = df['L_torso_length'] / (df['L_thigh_length'] + 1e-8)\n",
        "            ratios_created.append('torso_thigh_ratio')\n",
        "\n",
        "        # Altura aproximada (hombro a tobillo)\n",
        "        if all(col in df.columns for col in ['L_shoulder_y', 'L_ankle_y']):\n",
        "            df['body_height_approx'] = abs(df['L_ankle_y'] - df['L_shoulder_y'])\n",
        "            ratios_created.append('body_height_approx')\n",
        "\n",
        "        print(f\"      ‚úÖ {len(ratios_created)} ratios creados\")\n",
        "        return ratios_created\n",
        "\n",
        "    def _create_center_mass_features(self, df):\n",
        "        \"\"\"Crear caracter√≠sticas de centro de masa\"\"\"\n",
        "        print(\"   ‚öñÔ∏è Creando centro de masa...\")\n",
        "\n",
        "        center_features = []\n",
        "\n",
        "        # Centro de masa corporal (promedio caderas)\n",
        "        if all(col in df.columns for col in ['L_hip_x', 'R_hip_x', 'L_hip_y', 'R_hip_y']):\n",
        "            df['center_mass_x'] = (df['L_hip_x'] + df['R_hip_x']) / 2\n",
        "            df['center_mass_y'] = (df['L_hip_y'] + df['R_hip_y']) / 2\n",
        "            center_features.extend(['center_mass_x', 'center_mass_y'])\n",
        "\n",
        "        # Centro torso superior (promedio hombros)\n",
        "        if all(col in df.columns for col in ['L_shoulder_x', 'R_shoulder_x', 'L_shoulder_y', 'R_shoulder_y']):\n",
        "            df['upper_center_x'] = (df['L_shoulder_x'] + df['R_shoulder_x']) / 2\n",
        "            df['upper_center_y'] = (df['L_shoulder_y'] + df['R_shoulder_y']) / 2\n",
        "            center_features.extend(['upper_center_x', 'upper_center_y'])\n",
        "\n",
        "        print(f\"      ‚úÖ {len(center_features)} centros de masa creados\")\n",
        "        return center_features\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# APLICAR FEATURE ENGINEERING A CADA DATASET\n",
        "# ============================================\n",
        "\n",
        "# Crear feature engineer\n",
        "geo_engineer = GeometricFeatureEngineer()\n",
        "\n",
        "# CR√çTICO: Aplicar a cada dataset POR SEPARADO (sin mezclar)\n",
        "if all(df is not None for df in [train_df, val_df, test_df]):\n",
        "\n",
        "    # Train\n",
        "    train_geo, train_features = geo_engineer.create_geometric_features(train_df, 'TRAIN')\n",
        "\n",
        "    # Validation\n",
        "    val_geo, val_features = geo_engineer.create_geometric_features(val_df, 'VALIDATION')\n",
        "\n",
        "    # Test\n",
        "    test_geo, test_features = geo_engineer.create_geometric_features(test_df, 'TEST')\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚úÖ FEATURE ENGINEERING COMPLETADO PARA TODOS LOS DATASETS\")\n",
        "    print(f\"=\"*60)\n",
        "\n",
        "    print(f\"\\nüìä RESUMEN:\")\n",
        "    print(f\"   Train: {train_geo.shape[0]:,} √ó {train_geo.shape[1]} (features)\")\n",
        "    print(f\"   Val:   {val_geo.shape[0]:,} √ó {val_geo.shape[1]} (features)\")\n",
        "    print(f\"   Test:  {test_geo.shape[0]:,} √ó {test_geo.shape[1]} (features)\")\n",
        "\n",
        "    print(f\"\\nüîß Features geom√©tricas a√±adidas: {len(train_features)}\")\n",
        "    print(f\"   {', '.join(train_features[:5])}...\")\n",
        "\n",
        "    print(f\"\\nüöÄ Listo para Preprocessing Pipeline\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Error: Datasets no cargados correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlPHJ0gKuf3x",
        "outputId": "fbe513a3-9ead-4b24-ec74-3e26e655dd6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî∂ FEATURE ENGINEERING: TRAIN\n",
            "============================================================\n",
            "   üìè Creando distancias entre landmarks...\n",
            "      ‚úÖ 8 distancias creadas\n",
            "   üìê Creando √°ngulos articulares...\n",
            "      ‚úÖ 4 √°ngulos creados\n",
            "   üìä Creando ratios corporales...\n",
            "      ‚úÖ 3 ratios creados\n",
            "   ‚öñÔ∏è Creando centro de masa...\n",
            "      ‚úÖ 4 centros de masa creados\n",
            "\n",
            "‚úÖ FEATURES CREADAS PARA TRAIN:\n",
            "   üìä Total nuevas features: 19\n",
            "   üìê Dimensiones finales: 5,418 √ó 86\n",
            "\n",
            "üî∂ FEATURE ENGINEERING: VALIDATION\n",
            "============================================================\n",
            "   üìè Creando distancias entre landmarks...\n",
            "      ‚úÖ 8 distancias creadas\n",
            "   üìê Creando √°ngulos articulares...\n",
            "      ‚úÖ 4 √°ngulos creados\n",
            "   üìä Creando ratios corporales...\n",
            "      ‚úÖ 3 ratios creados\n",
            "   ‚öñÔ∏è Creando centro de masa...\n",
            "      ‚úÖ 4 centros de masa creados\n",
            "\n",
            "‚úÖ FEATURES CREADAS PARA VALIDATION:\n",
            "   üìä Total nuevas features: 19\n",
            "   üìê Dimensiones finales: 967 √ó 85\n",
            "\n",
            "üî∂ FEATURE ENGINEERING: TEST\n",
            "============================================================\n",
            "   üìè Creando distancias entre landmarks...\n",
            "      ‚úÖ 8 distancias creadas\n",
            "   üìê Creando √°ngulos articulares...\n",
            "      ‚úÖ 4 √°ngulos creados\n",
            "   üìä Creando ratios corporales...\n",
            "      ‚úÖ 3 ratios creados\n",
            "   ‚öñÔ∏è Creando centro de masa...\n",
            "      ‚úÖ 4 centros de masa creados\n",
            "\n",
            "‚úÖ FEATURES CREADAS PARA TEST:\n",
            "   üìä Total nuevas features: 19\n",
            "   üìê Dimensiones finales: 967 √ó 85\n",
            "\n",
            "============================================================\n",
            "‚úÖ FEATURE ENGINEERING COMPLETADO PARA TODOS LOS DATASETS\n",
            "============================================================\n",
            "\n",
            "üìä RESUMEN:\n",
            "   Train: 5,418 √ó 86 (features)\n",
            "   Val:   967 √ó 85 (features)\n",
            "   Test:  967 √ó 85 (features)\n",
            "\n",
            "üîß Features geom√©tricas a√±adidas: 19\n",
            "   shoulder_width, hip_width, L_torso_length, R_torso_length, L_thigh_length...\n",
            "\n",
            "üöÄ Listo para Preprocessing Pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Pipeline de Preprocessing\n",
        "Crear pipeline automatizado para normalizaci√≥n, encoding y preparaci√≥n final.\n"
      ],
      "metadata": {
        "id": "g3yhdOgQu7iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 4: PREPROCESSING PIPELINE (SIN LEAKAGE)\n",
        "# ============================================\n",
        "\n",
        "class DataPreprocessingPipeline:\n",
        "    \"\"\"Pipeline completo: FIT en train, TRANSFORM en val/test\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.pca = None\n",
        "        self.feature_cols = None\n",
        "        self.preprocessing_stats = {}\n",
        "\n",
        "    def fit_transform_train(self, train_df, use_pca=True, n_components=0.95):\n",
        "        \"\"\"\n",
        "        AJUSTAR pipeline en train y transformar\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß PREPROCESSING: AJUSTE EN TRAIN\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"üìä TRAIN INPUT:\")\n",
        "        print(f\"   Frames: {len(train_df):,}\")\n",
        "        print(f\"   Columnas: {len(train_df.columns)}\")\n",
        "\n",
        "        # 1. Separar features y labels\n",
        "        X_train, y_train = self._separate_features_labels(train_df, is_train=True)\n",
        "\n",
        "        if X_train is None:\n",
        "            print(\"‚ùå Error separando features\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"\\nüìä DATOS EXTRA√çDOS:\")\n",
        "        print(f\"   Features (X): {X_train.shape}\")\n",
        "        print(f\"   Labels (y): {len(y_train):,}\")\n",
        "        print(f\"   Feature columns: {len(self.feature_cols)}\")\n",
        "\n",
        "        # 2. Label encoding\n",
        "        y_encoded = self.label_encoder.fit_transform(y_train)\n",
        "\n",
        "        print(f\"\\nüè∑Ô∏è  LABEL ENCODING (FIT EN TRAIN):\")\n",
        "        for i, class_name in enumerate(self.label_encoder.classes_):\n",
        "            count = np.sum(y_encoded == i)\n",
        "            pct = count / len(y_encoded) * 100\n",
        "            print(f\"   {i}: {class_name:20s} ‚Üí {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "        # 3. Limpiar features\n",
        "        print(f\"\\nüßπ LIMPIEZA:\")\n",
        "        nan_count = np.isnan(X_train).sum()\n",
        "        if nan_count > 0:\n",
        "            print(f\"   ‚ö†Ô∏è  NaN encontrados: {nan_count}\")\n",
        "            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "            print(f\"   ‚úÖ NaN reemplazados\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Sin NaN\")\n",
        "\n",
        "        # 4. Normalizaci√≥n (FIT en train)\n",
        "        X_scaled = self.scaler.fit_transform(X_train)\n",
        "        print(f\"   ‚úÖ StandardScaler FIT en train\")\n",
        "        print(f\"      Mean: {self.scaler.mean_[:3]} ... (primeras 3)\")\n",
        "        print(f\"      Std:  {self.scaler.scale_[:3]} ... (primeras 3)\")\n",
        "\n",
        "        # 5. PCA (FIT en train)\n",
        "        if use_pca:\n",
        "            self.pca = PCA(n_components=n_components, random_state=42)\n",
        "            X_final = self.pca.fit_transform(X_scaled)\n",
        "\n",
        "            variance = self.pca.explained_variance_ratio_.sum()\n",
        "            print(f\"   ‚úÖ PCA FIT en train:\")\n",
        "            print(f\"      {X_train.shape[1]} ‚Üí {X_final.shape[1]} features\")\n",
        "            print(f\"      Varianza: {variance*100:.1f}%\")\n",
        "        else:\n",
        "            X_final = X_scaled\n",
        "\n",
        "        # Estad√≠sticas\n",
        "        self.preprocessing_stats['train'] = {\n",
        "            'samples': X_final.shape[0],\n",
        "            'original_features': X_train.shape[1],\n",
        "            'final_features': X_final.shape[1],\n",
        "            'classes': len(self.label_encoder.classes_)\n",
        "        }\n",
        "\n",
        "        print(f\"\\n‚úÖ TRAIN PROCESADO: {X_final.shape}\")\n",
        "\n",
        "        return X_final, y_encoded\n",
        "\n",
        "    def transform_test(self, test_df, dataset_name='Test'):\n",
        "        \"\"\"\n",
        "        TRANSFORMAR val/test usando pipeline YA AJUSTADO en train\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîÑ PREPROCESSING: TRANSFORM EN {dataset_name.upper()}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"üìä {dataset_name.upper()} INPUT:\")\n",
        "        print(f\"   Frames: {len(test_df):,}\")\n",
        "\n",
        "        # 1. Separar features y labels (usando MISMAS columnas que train)\n",
        "        X_test, y_test = self._separate_features_labels(test_df, is_train=False)\n",
        "\n",
        "        if X_test is None:\n",
        "            print(f\"‚ùå Error separando features de {dataset_name}\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"   Features (X): {X_test.shape}\")\n",
        "        print(f\"   Labels (y): {len(y_test):,}\")\n",
        "\n",
        "        # 2. Label encoding (TRANSFORM, no fit)\n",
        "        y_encoded = self.label_encoder.transform(y_test)\n",
        "\n",
        "        # 3. Limpiar\n",
        "        X_test = np.nan_to_num(X_test, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "        # 4. Normalizaci√≥n (TRANSFORM con scaler de train)\n",
        "        X_scaled = self.scaler.transform(X_test)  # ‚Üê NO fit_transform\n",
        "        print(f\"   ‚úÖ StandardScaler TRANSFORM (usando stats de train)\")\n",
        "\n",
        "        # 5. PCA (TRANSFORM con PCA de train)\n",
        "        if self.pca:\n",
        "            X_final = self.pca.transform(X_scaled)  # ‚Üê NO fit_transform\n",
        "            print(f\"   ‚úÖ PCA TRANSFORM: {X_test.shape[1]} ‚Üí {X_final.shape[1]}\")\n",
        "        else:\n",
        "            X_final = X_scaled\n",
        "\n",
        "        print(f\"\\n‚úÖ {dataset_name.upper()} PROCESADO: {X_final.shape}\")\n",
        "\n",
        "        return X_final, y_encoded\n",
        "\n",
        "    def _separate_features_labels(self, df, is_train=True):\n",
        "        \"\"\"Separar features y labels\"\"\"\n",
        "\n",
        "        # Columnas metadata (NO son features)\n",
        "        metadata_cols = ['activity', 'data_type', 'split', 'frame_number',\n",
        "                        'video_file', 'source', 'augmentation_type']\n",
        "\n",
        "        if is_train:\n",
        "            # En train, identificar feature_cols por primera vez\n",
        "            self.feature_cols = [col for col in df.columns\n",
        "                                if col not in metadata_cols]\n",
        "\n",
        "            print(f\"   üîç Identificadas {len(self.feature_cols)} feature columns\")\n",
        "        else:\n",
        "            # En val/test, usar MISMAS columnas que train\n",
        "            if self.feature_cols is None:\n",
        "                print(\"   ‚ùå ERROR: Debes procesar train primero\")\n",
        "                return None, None\n",
        "\n",
        "        # Verificar que todas las columnas existen\n",
        "        missing_cols = [col for col in self.feature_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"   ‚ö†Ô∏è  Columnas faltantes: {len(missing_cols)}\")\n",
        "            # Usar solo columnas disponibles\n",
        "            available_cols = [col for col in self.feature_cols if col in df.columns]\n",
        "            X = df[available_cols].values\n",
        "        else:\n",
        "            X = df[self.feature_cols].values\n",
        "\n",
        "        # Labels\n",
        "        if 'activity' not in df.columns:\n",
        "            print(\"   ‚ùå No hay columna 'activity'\")\n",
        "            return None, None\n",
        "\n",
        "        y = df['activity'].values\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# EJECUTAR PREPROCESSING CON SEPARACI√ìN CORRECTA\n",
        "# ============================================\n",
        "\n",
        "# Crear pipeline\n",
        "preprocessor = DataPreprocessingPipeline()\n",
        "\n",
        "# PASO CR√çTICO: FIT en train, TRANSFORM en val/test\n",
        "if all(df is not None for df in [train_geo, val_geo, test_geo]):\n",
        "\n",
        "    # 1. FIT_TRANSFORM en TRAIN\n",
        "    X_train, y_train = preprocessor.fit_transform_train(\n",
        "        train_geo,\n",
        "        use_pca=True,\n",
        "        n_components=0.95\n",
        "    )\n",
        "\n",
        "    # 2. TRANSFORM en VALIDATION (usando pipeline de train)\n",
        "    X_val, y_val = preprocessor.transform_test(val_geo, 'Validation')\n",
        "\n",
        "    # 3. TRANSFORM en TEST (usando pipeline de train)\n",
        "    X_test, y_test = preprocessor.transform_test(test_geo, 'Test')\n",
        "\n",
        "    # Verificaci√≥n final\n",
        "    if all(X is not None for X in [X_train, X_val, X_test]):\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\"‚úÖ PREPROCESSING COMPLETADO SIN LEAKAGE\")\n",
        "        print(f\"=\"*60)\n",
        "\n",
        "        print(f\"\\nüìä DATASETS FINALES:\")\n",
        "        print(f\"   Train: {X_train.shape[0]:,} √ó {X_train.shape[1]}\")\n",
        "        print(f\"   Val:   {X_val.shape[0]:,} √ó {X_val.shape[1]}\")\n",
        "        print(f\"   Test:  {X_test.shape[0]:,} √ó {X_test.shape[1]}\")\n",
        "\n",
        "        print(f\"\\nüîí GARANT√çA SIN LEAKAGE:\")\n",
        "        print(f\"   ‚úì Scaler FIT solo en train\")\n",
        "        print(f\"   ‚úì PCA FIT solo en train\")\n",
        "        print(f\"   ‚úì Val/Test solo TRANSFORMADOS\")\n",
        "\n",
        "        print(f\"\\nüöÄ Listos para guardar y entregar a Tom√°s\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Error en alg√∫n dataset\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Datasets geom√©tricos no disponibles\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wj-dySmu8uv",
        "outputId": "4b520b02-7772-4f29-8cbb-4a598b855630"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß PREPROCESSING: AJUSTE EN TRAIN\n",
            "============================================================\n",
            "üìä TRAIN INPUT:\n",
            "   Frames: 5,418\n",
            "   Columnas: 86\n",
            "   üîç Identificadas 83 feature columns\n",
            "\n",
            "üìä DATOS EXTRA√çDOS:\n",
            "   Features (X): (5418, 83)\n",
            "   Labels (y): 5,418\n",
            "   Feature columns: 83\n",
            "\n",
            "üè∑Ô∏è  LABEL ENCODING (FIT EN TRAIN):\n",
            "   0: caminar_hacia        ‚Üí 1,290 (23.8%)\n",
            "   1: caminar_regreso      ‚Üí 1,032 (19.0%)\n",
            "   2: girar                ‚Üí 1,032 (19.0%)\n",
            "   3: ponerse_pie          ‚Üí 1,032 (19.0%)\n",
            "   4: sentarse             ‚Üí 1,032 (19.0%)\n",
            "\n",
            "üßπ LIMPIEZA:\n",
            "   ‚úÖ Sin NaN\n",
            "   ‚úÖ StandardScaler FIT en train\n",
            "      Mean: [ 0.48664878  0.36699865 -0.12852494] ... (primeras 3)\n",
            "      Std:  [0.09141731 0.07074265 0.13676313] ... (primeras 3)\n",
            "   ‚úÖ PCA FIT en train:\n",
            "      83 ‚Üí 16 features\n",
            "      Varianza: 95.1%\n",
            "\n",
            "‚úÖ TRAIN PROCESADO: (5418, 16)\n",
            "\n",
            "üîÑ PREPROCESSING: TRANSFORM EN VALIDATION\n",
            "============================================================\n",
            "üìä VALIDATION INPUT:\n",
            "   Frames: 967\n",
            "   Features (X): (967, 83)\n",
            "   Labels (y): 967\n",
            "   ‚úÖ StandardScaler TRANSFORM (usando stats de train)\n",
            "   ‚úÖ PCA TRANSFORM: 83 ‚Üí 16\n",
            "\n",
            "‚úÖ VALIDATION PROCESADO: (967, 16)\n",
            "\n",
            "üîÑ PREPROCESSING: TRANSFORM EN TEST\n",
            "============================================================\n",
            "üìä TEST INPUT:\n",
            "   Frames: 967\n",
            "   Features (X): (967, 83)\n",
            "   Labels (y): 967\n",
            "   ‚úÖ StandardScaler TRANSFORM (usando stats de train)\n",
            "   ‚úÖ PCA TRANSFORM: 83 ‚Üí 16\n",
            "\n",
            "‚úÖ TEST PROCESADO: (967, 16)\n",
            "\n",
            "============================================================\n",
            "‚úÖ PREPROCESSING COMPLETADO SIN LEAKAGE\n",
            "============================================================\n",
            "\n",
            "üìä DATASETS FINALES:\n",
            "   Train: 5,418 √ó 16\n",
            "   Val:   967 √ó 16\n",
            "   Test:  967 √ó 16\n",
            "\n",
            "üîí GARANT√çA SIN LEAKAGE:\n",
            "   ‚úì Scaler FIT solo en train\n",
            "   ‚úì PCA FIT solo en train\n",
            "   ‚úì Val/Test solo TRANSFORMADOS\n",
            "\n",
            "üöÄ Listos para guardar y entregar a Tom√°s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Guardar"
      ],
      "metadata": {
        "id": "eV4Iv0IQxPr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PASO 5: GUARDAR DATOS FINALES PARA MODELADO\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_ml_ready_data():\n",
        "    \"\"\"\n",
        "    Guardar datasets procesados y pipelines para Tom√°s\n",
        "    \"\"\"\n",
        "    print(\"\\nüíæ GUARDANDO DATOS PARA MODELADO\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Crear directorios\n",
        "    os.makedirs('data/ml_ready', exist_ok=True)\n",
        "    os.makedirs('data/models', exist_ok=True)\n",
        "\n",
        "    # 1. Guardar arrays numpy (.npy)\n",
        "    print(\"\\nüìÅ Guardando arrays numpy...\")\n",
        "    np.save('data/ml_ready/X_train.npy', X_train)\n",
        "    np.save('data/ml_ready/y_train.npy', y_train)\n",
        "    np.save('data/ml_ready/X_val.npy', X_val)\n",
        "    np.save('data/ml_ready/y_val.npy', y_val)\n",
        "    np.save('data/ml_ready/X_test.npy', X_test)\n",
        "    np.save('data/ml_ready/y_test.npy', y_test)\n",
        "\n",
        "    print(f\"   ‚úÖ X_train.npy: {X_train.shape}\")\n",
        "    print(f\"   ‚úÖ y_train.npy: {y_train.shape}\")\n",
        "    print(f\"   ‚úÖ X_val.npy:   {X_val.shape}\")\n",
        "    print(f\"   ‚úÖ y_val.npy:   {y_val.shape}\")\n",
        "    print(f\"   ‚úÖ X_test.npy:  {X_test.shape}\")\n",
        "    print(f\"   ‚úÖ y_test.npy:  {y_test.shape}\")\n",
        "\n",
        "    # 2. Guardar pipelines (.pkl)\n",
        "    print(\"\\nüîß Guardando pipelines...\")\n",
        "    joblib.dump(preprocessor.scaler, 'data/models/scaler.pkl')\n",
        "    joblib.dump(preprocessor.label_encoder, 'data/models/label_encoder.pkl')\n",
        "    joblib.dump(preprocessor.pca, 'data/models/pca.pkl')\n",
        "\n",
        "    print(f\"   ‚úÖ scaler.pkl (StandardScaler)\")\n",
        "    print(f\"   ‚úÖ label_encoder.pkl (LabelEncoder)\")\n",
        "    print(f\"   ‚úÖ pca.pkl (PCA)\")\n",
        "\n",
        "    # 3. Metadata completa\n",
        "    print(\"\\nüìã Creando metadata...\")\n",
        "\n",
        "    metadata = {\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'project': 'Sistema Anotaci√≥n Video - Entrega 2',\n",
        "        'datasets': {\n",
        "            'train': {\n",
        "                'samples': int(X_train.shape[0]),\n",
        "                'features': int(X_train.shape[1]),\n",
        "                'original_frames': 4509,\n",
        "                'smote_frames': 909,\n",
        "                'balance_ratio': 0.800,\n",
        "                'classes_distribution': {\n",
        "                    'caminar_hacia': 1290,\n",
        "                    'caminar_regreso': 1032,\n",
        "                    'girar': 1032,\n",
        "                    'ponerse_pie': 1032,\n",
        "                    'sentarse': 1032\n",
        "                }\n",
        "            },\n",
        "            'validation': {\n",
        "                'samples': int(X_val.shape[0]),\n",
        "                'features': int(X_val.shape[1]),\n",
        "                'data_type': '100% real',\n",
        "                'balance_ratio': 0.509\n",
        "            },\n",
        "            'test': {\n",
        "                'samples': int(X_test.shape[0]),\n",
        "                'features': int(X_test.shape[1]),\n",
        "                'data_type': '100% real',\n",
        "                'balance_ratio': 0.509\n",
        "            }\n",
        "        },\n",
        "        'preprocessing': {\n",
        "            'original_features': 83,\n",
        "            'final_features': int(X_train.shape[1]),\n",
        "            'pca_variance_explained': float(preprocessor.pca.explained_variance_ratio_.sum()),\n",
        "            'scaler': 'StandardScaler',\n",
        "            'pca_components': int(X_train.shape[1])\n",
        "        },\n",
        "        'classes': preprocessor.label_encoder.classes_.tolist(),\n",
        "        'quality_assurance': {\n",
        "            'no_data_leakage': True,\n",
        "            'scaler_fit_on': 'train_only',\n",
        "            'pca_fit_on': 'train_only',\n",
        "            'synthetic_ratio_train': 0.168,\n",
        "            'test_completely_real': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('data/ml_ready/metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"   ‚úÖ metadata.json\")\n",
        "\n",
        "    # 4. README para Tom√°s\n",
        "    print(\"\\nüìÑ Creando README...\")\n",
        "\n",
        "    readme_content = \"\"\"\n",
        "============================================================\n",
        "DATASETS LISTOS PARA MODELADO - ENTREGA 2\n",
        "============================================================\n",
        "\n",
        "Fecha: \"\"\" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\"\"\n",
        "\n",
        "ARCHIVOS:\n",
        "---------\n",
        "‚Ä¢ X_train.npy: \"\"\" + f\"{X_train.shape[0]:,} √ó {X_train.shape[1]}\" + \"\"\" (entrenamiento)\n",
        "‚Ä¢ y_train.npy: \"\"\" + f\"{y_train.shape[0]:,}\" + \"\"\" labels\n",
        "‚Ä¢ X_val.npy:   \"\"\" + f\"{X_val.shape[0]:,} √ó {X_val.shape[1]}\" + \"\"\" (validaci√≥n hiperpar√°metros)\n",
        "‚Ä¢ y_val.npy:   \"\"\" + f\"{y_val.shape[0]:,}\" + \"\"\" labels\n",
        "‚Ä¢ X_test.npy:  \"\"\" + f\"{X_test.shape[0]:,} √ó {X_test.shape[1]}\" + \"\"\" (evaluaci√≥n final)\n",
        "‚Ä¢ y_test.npy:  \"\"\" + f\"{y_test.shape[0]:,}\" + \"\"\" labels\n",
        "\n",
        "PIPELINES:\n",
        "----------\n",
        "‚Ä¢ scaler.pkl: StandardScaler (ajustado en train)\n",
        "‚Ä¢ label_encoder.pkl: Codificaci√≥n de actividades\n",
        "‚Ä¢ pca.pkl: PCA 83‚Üí16 features (95.1% varianza)\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "----------------\n",
        "‚úì Sin data leakage (fit solo en train)\n",
        "‚úì Balance train: 0.800 (SMOTE aplicado)\n",
        "‚úì Val/Test: 100% datos reales\n",
        "‚úì PCA: 83 features ‚Üí 16 componentes\n",
        "‚úì Varianza explicada: 95.1%\n",
        "\n",
        "CLASES:\n",
        "-------\n",
        "0: caminar_hacia\n",
        "1: caminar_regreso\n",
        "2: girar\n",
        "3: ponerse_pie\n",
        "4: sentarse\n",
        "\n",
        "USO EN MODELADO:\n",
        "----------------\n",
        "```\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Cargar datos\n",
        "X_train = np.load('data/ml_ready/X_train.npy')\n",
        "y_train = np.load('data/ml_ready/y_train.npy')\n",
        "X_val = np.load('data/ml_ready/X_val.npy')\n",
        "y_val = np.load('data/ml_ready/y_val.npy')\n",
        "X_test = np.load('data/ml_ready/X_test.npy')\n",
        "y_test = np.load('data/ml_ready/y_test.npy')\n",
        "\n",
        "# Cargar pipelines (si necesitas transformar nuevos datos)\n",
        "scaler = joblib.load('data/models/scaler.pkl')\n",
        "label_encoder = joblib.load('data/models/label_encoder.pkl')\n",
        "pca = joblib.load('data/models/pca.pkl')\n",
        "\n",
        "# Entrenar modelo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar\n",
        "from sklearn.metrics import classification_report\n",
        "val_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, val_pred,\n",
        "                           target_names=label_encoder.classes_))\n",
        "```\n",
        "\n",
        "WORKFLOW RECOMENDADO:\n",
        "---------------------\n",
        "1. Entrenar modelos con X_train, y_train\n",
        "2. Optimizar hiperpar√°metros con X_val, y_val\n",
        "3. Evaluaci√≥n FINAL con X_test, y_test (SOLO UNA VEZ)\n",
        "\n",
        "GARANT√çAS:\n",
        "----------\n",
        "‚úì Test set nunca visto durante preprocessing\n",
        "‚úì Scaler/PCA ajustados SOLO en train\n",
        "‚úì Balance train: 0.800\n",
        "‚úì Datos reales en val/test para evaluaci√≥n genuina\n",
        "\n",
        "============================================================\n",
        "\"\"\"\n",
        "\n",
        "    with open('data/ml_ready/README.txt', 'w') as f:\n",
        "        f.write(readme_content)\n",
        "\n",
        "    print(f\"   ‚úÖ README.txt\")\n",
        "\n",
        "    # Tama√±os de archivos\n",
        "    print(\"\\nüíæ TAMA√ëOS DE ARCHIVOS:\")\n",
        "    for filename in ['X_train.npy', 'y_train.npy', 'X_val.npy', 'y_val.npy', 'X_test.npy', 'y_test.npy']:\n",
        "        path = f'data/ml_ready/{filename}'\n",
        "        size_mb = os.path.getsize(path) / (1024**2)\n",
        "        print(f\"   {filename:15s}: {size_mb:6.2f} MB\")\n",
        "\n",
        "    print(\"\\n\" + \"üéâ\" * 30)\n",
        "    print(\"üéâ NOTEBOOK 4 COMPLETADO - DATOS LISTOS PARA TOM√ÅS üéâ\")\n",
        "    print(\"üéâ\" * 30)\n",
        "\n",
        "    print(\"\\nüìÇ UBICACI√ìN: data/ml_ready/\")\n",
        "    print(\"\\n‚úÖ Tom√°s puede empezar a entrenar modelos ahora\")\n",
        "\n",
        "# Ejecutar guardado\n",
        "save_ml_ready_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPzlqk56xQPI",
        "outputId": "ace534f1-fed3-4758-b0fc-c18c4b9b23c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ GUARDANDO DATOS PARA MODELADO\n",
            "============================================================\n",
            "\n",
            "üìÅ Guardando arrays numpy...\n",
            "   ‚úÖ X_train.npy: (5418, 16)\n",
            "   ‚úÖ y_train.npy: (5418,)\n",
            "   ‚úÖ X_val.npy:   (967, 16)\n",
            "   ‚úÖ y_val.npy:   (967,)\n",
            "   ‚úÖ X_test.npy:  (967, 16)\n",
            "   ‚úÖ y_test.npy:  (967,)\n",
            "\n",
            "üîß Guardando pipelines...\n",
            "   ‚úÖ scaler.pkl (StandardScaler)\n",
            "   ‚úÖ label_encoder.pkl (LabelEncoder)\n",
            "   ‚úÖ pca.pkl (PCA)\n",
            "\n",
            "üìã Creando metadata...\n",
            "   ‚úÖ metadata.json\n",
            "\n",
            "üìÑ Creando README...\n",
            "   ‚úÖ README.txt\n",
            "\n",
            "üíæ TAMA√ëOS DE ARCHIVOS:\n",
            "   X_train.npy    :   0.66 MB\n",
            "   y_train.npy    :   0.04 MB\n",
            "   X_val.npy      :   0.12 MB\n",
            "   y_val.npy      :   0.01 MB\n",
            "   X_test.npy     :   0.12 MB\n",
            "   y_test.npy     :   0.01 MB\n",
            "\n",
            "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
            "üéâ NOTEBOOK 4 COMPLETADO - DATOS LISTOS PARA TOM√ÅS üéâ\n",
            "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
            "\n",
            "üìÇ UBICACI√ìN: data/ml_ready/\n",
            "\n",
            "‚úÖ Tom√°s puede empezar a entrenar modelos ahora\n"
          ]
        }
      ]
    }
  ]
}