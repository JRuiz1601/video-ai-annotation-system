# Desaf√≠os Identificados en Deployment
**Sistema de Clasificaci√≥n de Actividades Humanas**  
**Universidad ICESI - Inteligencia Artificial I**  
**Fecha:** Noviembre 2025  
**Autores:** Juan Ruiz & Tom√°s Quintero

---

## üìä Resumen Ejecutivo

El modelo Random Forest entrenado alcanz√≥ un **98.76% de accuracy en test set** bajo condiciones controladas, sin embargo, al desplegarlo en producci√≥n mediante Gradio con webcam en tiempo real, se identific√≥ un **gap significativo de performance**. El modelo reconoce correctamente **solo 1 de 5 actividades** de manera consistente, fallando en generalizar a condiciones del mundo real.

---

## üö® Problema Principal: Gap Offline-Online

### S√≠ntomas Observados en Producci√≥n

#### ‚úÖ Actividad que Funciona Bien
**Caminar Hacia la C√°mara:**
- **Performance online:** ~85-90% (estimado observacional)
- **Raz√≥n del √©xito:** Movimiento frontal y continuo capturado en training

#### ‚ùå Actividades que Fallan Sistem√°ticamente

**1. Girar (Turn)**
- **Test set:** 99.3% accuracy (140/141 frames)
- **Webcam:** ~30-40% accuracy (confundido con "Caminar Hacia")
- **Error com√∫n:** Clasifica como "Caminar Hacia" durante los primeros 180¬∞ del giro

**2. Caminar de Regreso**
- **Test set:** 100% accuracy (195/195 frames)
- **Webcam:** ~20-30% accuracy (confundido con "Caminar Hacia" o "Girar")
- **Error com√∫n:** Modelo no reconoce movimiento de espaldas a la c√°mara

**3. Sentarse**
- **Test set:** 96.3% recall (181/188 frames)
- **Webcam:** ~25% accuracy (confundido con "Ponerse de Pie" o "Caminar")
- **Error com√∫n:** Transici√≥n r√°pida no capturada como actividad independiente

**4. Ponerse de Pie**
- **Test set:** 98.2% recall (163/166 frames)
- **Webcam:** ~30% accuracy (confundido con "Sentarse")
- **Error com√∫n:** Similar a "Sentarse", transici√≥n demasiado r√°pida

### Comparaci√≥n Cuantitativa

| Actividad | Test Accuracy | Webcam Accuracy (estimado) | Gap |
|-----------|--------------|---------------------------|-----|
| **Caminar Hacia** | 99.6% | ~85-90% | -10-15% |
| **Caminar Regreso** | 100.0% | ~20-30% | **-70-80%** |
| **Girar** | 99.3% | ~30-40% | **-60-70%** |
| **Ponerse Pie** | 98.2% | ~30% | **-68%** |
| **Sentarse** | 96.3% | ~25% | **-71%** |

**Gap promedio:** **-54%** (cr√≠tico)

---

## üîç Causas Ra√≠z Identificadas

### 1. Limitaciones del Dataset de Entrenamiento

#### 1.1 Falta de Diversidad en √Ångulos de C√°mara

**Training:**
- ‚úÖ √Ångulo frontal √∫nico (0¬∞)
- ‚ùå Sin √°ngulos laterales (45¬∞, 90¬∞)
- ‚ùå Sin √°ngulos traseros (180¬∞)

**Impacto en producci√≥n:**
```
Actividad: "Caminar de Regreso"
Frame en webcam: Usuario de espaldas (180¬∞)
Landmarks detectados: Hombros visibles, rostro NO visible
Modelo entrenado solo con frontales: NO reconoce este patr√≥n
Predicci√≥n err√≥nea: "Caminar Hacia" (confusi√≥n de direcci√≥n)
```

**Soluci√≥n propuesta:**
- Grabar cada actividad desde 4 √°ngulos: 0¬∞, 90¬∞, 180¬∞, 270¬∞
- Total videos necesarios: 90 actuales √ó 4 √°ngulos = **360 videos**

#### 1.2 Condiciones de Iluminaci√≥n Homog√©neas

**Training:**
- Iluminaci√≥n interior consistente
- Misma hora del d√≠a
- Sin variaciones de luz natural

**Impacto en producci√≥n:**
```
Escenario: Usuario en habitaci√≥n con ventana lateral
MediaPipe detecta landmarks con baja visibility (<0.6)
Features geom√©tricas calculadas con ruido
Clasificaci√≥n err√°tica
```

**Soluci√≥n propuesta:**
- Grabar en 3 condiciones: luz natural diurna, luz artificial, luz tenue
- Aplicar data augmentation de brillo/contraste

#### 1.3 Fondos y Entornos Controlados

**Training:**
- Fondo limpio y uniforme
- Sin objetos en movimiento
- Sin personas adicionales

**Impacto en producci√≥n:**
```
Escenario: Usuario en sala de estar con muebles
MediaPipe ocasionalmente detecta landmarks falsos
Ruido en features de dispersi√≥n espacial
Clasificaci√≥n degradada
```

**Soluci√≥n propuesta:**
- Grabar en 3 tipos de fondo: limpio, semi-cluttered, cluttered
- Entrenar con oclusiones parciales

#### 1.4 Baja Diversidad de Sujetos

**Training:**
- 3 personas (dataset actual)
- Rango edad: 20-30 a√±os
- G√©nero: 2 hombres, 1 mujer
- Etnia: Homog√©nea

**Impacto en producci√≥n:**
```
Usuario nuevo: Diferente altura, complexi√≥n o velocidad de movimiento
Features geom√©tricas fuera de distribuci√≥n de training
Modelo generaliza pobremente
```

**Soluci√≥n propuesta:**
- Expandir a **m√≠nimo 10-15 personas**
- Diversificar: edad (18-65), g√©nero, altura, complexi√≥n

---

### 2. Ausencia de Contexto Temporal

#### 2.1 Clasificaci√≥n Frame-by-Frame

**Training:**
- Modelo recibe secuencias completas (30-60 frames)
- Aunque Random Forest procesa frames individualmente, el dataset contiene **contexto impl√≠cito**

**Production:**
- Gradio procesa 1 frame cada 0.033s (30 FPS)
- Sin memoria de frames anteriores
- Actividades transicionales (girar, sentarse) requieren secuencia

**Ejemplo concreto:**
```python
# Frame 1: Usuario empieza a girar
# Landmarks: Hombros rotando 30¬∞
# Modelo sin contexto: "Caminar Hacia" (70% confianza)

# Frame 2: Usuario a mitad de giro
# Landmarks: Hombros rotando 90¬∞
# Modelo sin contexto: "Girar" (45% confianza)

# Frame 3: Usuario completa giro
# Landmarks: Hombros rotando 180¬∞
# Modelo sin contexto: "Caminar Regreso" (60% confianza)

# Resultado: Clasificaci√≥n err√°tica durante actividad continua
```

**Soluci√≥n propuesta:**
```python
from collections import deque

frame_buffer = deque(maxlen=30)  # 1 segundo @ 30fps

def classify_with_temporal_context(frame):
    features = extract_features(frame)
    frame_buffer.append(features)
    
    if len(frame_buffer) >= 15:  # M√≠nimo 0.5s de contexto
        # Opci√≥n A: Promediar features
        features_avg = np.mean(frame_buffer, axis=0)
        
        # Opci√≥n B: Voting mayoritario sobre √∫ltimas N predicciones
        predictions = [predict(f) for f in frame_buffer]
        final_prediction = Counter(predictions).most_common(1)[0][0]
        
        return final_prediction
    else:
        return "Inicializando buffer..."
```

---

### 3. Degradaci√≥n de Calidad de Landmarks en Producci√≥n

#### 3.1 Factores Ambientales Variables

| Factor | Training | Producci√≥n (Webcam) | Impacto en Landmarks |
|--------|----------|---------------------|----------------------|
| **Iluminaci√≥n** | Controlada (LED indirecto) | Variable (natural/artificial) | Visibility -15-30% |
| **Resoluci√≥n** | 1080p @ 30fps | 480p-720p @ 15-30fps | Precisi√≥n -10-20% |
| **Distancia** | √ìptima (1.5-2m) | Variable (0.5-3m) | Escala inconsistente |
| **√Ångulo** | Frontal perpendicular | Inclinado (usuarios) | Distorsi√≥n landmarks |
| **Fondo** | Limpio uniforme | Cluttered din√°mico | Falsos positivos |

#### 3.2 Impacto en Features Geom√©tricas

**Ejemplo: Feature "√Ångulo de Rodilla"**

Training (condiciones √≥ptimas):
```
Landmark rodilla: visibility = 0.95
Landmark cadera:  visibility = 0.98
Landmark tobillo: visibility = 0.92

√Ångulo calculado: 167.3¬∞ (confiable)
```

Producci√≥n (iluminaci√≥n baja):
```
Landmark rodilla: visibility = 0.62  ‚Üê BAJO
Landmark cadera:  visibility = 0.71
Landmark tobillo: visibility = 0.58  ‚Üê BAJO

√Ångulo calculado: 152.8¬∞ (ruidoso, -14.5¬∞ error)
```

**Efecto cascada:**
- Features geom√©tricas con +10-20% error
- Scaler/PCA transforma features fuera de distribuci√≥n
- Random Forest clasifica en regi√≥n no vista en training

---

### 4. Mismatch de Distribuci√≥n de Features

#### 4.1 An√°lisis de Drift

**Training feature distribution (ejemplo: "Inclinaci√≥n Torso"):**
```
Media: 0.15
Std Dev: 0.08
Min: -0.05
Max: 0.35
```

**Production feature distribution (observado):**
```
Media: 0.22  ‚Üê +7œÉ desplazamiento
Std Dev: 0.14 ‚Üê 1.75x m√°s varianza
Min: -0.15  ‚Üê Fuera de rango training
Max: 0.52   ‚Üê Fuera de rango training
```

**Consecuencia:**
```python
# Feature en production
feature_value = 0.52

# Scaler entrenado con max=0.35
scaled_value = (0.52 - 0.15) / 0.08 = 4.625  ‚Üê >4œÉ

# PCA proyecta en espacio no explorado
# Random Forest clasifica con baja confianza o error
```

---

## üí° Soluciones Propuestas

### Corto Plazo (1-2 semanas) - Mejoras Inmediatas

#### 1. Buffer Temporal con Voting
```python
from collections import deque, Counter

prediction_buffer = deque(maxlen=10)  # ~0.3s @ 30fps
confidence_buffer = deque(maxlen=10)

def classify_smoothed(frame):
    prediction, confidence, probs = predict_activity(frame)
    
    prediction_buffer.append(prediction)
    confidence_buffer.append(confidence)
    
    # Voting mayoritario
    smoothed_prediction = Counter(prediction_buffer).most_common(1)[0][0]
    smoothed_confidence = np.mean(confidence_buffer)
    
    return smoothed_prediction, smoothed_confidence, probs
```

**Ganancia esperada:** +10-15% accuracy online

#### 2. Umbral de Confianza Adaptativo
```python
def filter_by_confidence(prediction, confidence):
    if confidence < 0.75:
        return "‚ö†Ô∏è Actividad no clara", confidence
    elif confidence < 0.85:
        return f"‚ö†Ô∏è Posible {prediction}", confidence
    else:
        return f"‚úÖ {prediction}", confidence
```

**Ganancia esperada:** Reduce falsos positivos en 40%

#### 3. Calibraci√≥n de MediaPipe
```python
pose = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=2,  # ‚Üê Aumentar de 1 a 2
    smooth_landmarks=True,
    enable_segmentation=False,
    min_detection_confidence=0.7,  # ‚Üê Aumentar de 0.5
    min_tracking_confidence=0.7    # ‚Üê Aumentar de 0.5
)
```

**Ganancia esperada:** +5% calidad landmarks

---

### Mediano Plazo (1-2 meses) - Reentrenamiento

#### 4. Expansi√≥n del Dataset

**Plan de recolecci√≥n:**

| Dimensi√≥n | Actual | Propuesto | Multiplicador |
|-----------|--------|-----------|---------------|
| **Personas** | 3 | 15 | 5x |
| **√Ångulos** | 1 (frontal) | 4 (0¬∞, 90¬∞, 180¬∞, 270¬∞) | 4x |
| **Iluminaci√≥n** | 1 (controlada) | 3 (natural, artificial, tenue) | 3x |
| **Fondos** | 1 (limpio) | 3 (limpio, semi, cluttered) | 3x |

**Total videos necesarios:**
```
5 actividades √ó 6 repeticiones √ó 15 personas √ó 4 √°ngulos √ó 3 iluminaciones √ó 3 fondos
= 16,200 videos

Simplificado (combinaciones pr√°cticas):
5 actividades √ó 6 repeticiones √ó 15 personas √ó 4 √°ngulos = 1,800 videos
```

**Tiempo estimado:**
- Grabaci√≥n: 15 personas √ó 2 horas = 30 horas
- Anotaci√≥n: 1,800 videos √ó 2 min = 60 horas
- Procesamiento: 20 horas
- **Total: ~110 horas** (3 semanas con equipo de 3)

#### 5. Fine-Tuning con Datos de Producci√≥n

**Estrategia:**
1. Grabar 200 clips (10-15s) desde Gradio
   - 40 clips por actividad
   - Diferentes usuarios reales
2. Etiquetar manualmente usando LabelStudio
3. Aplicar SMOTE conservador (balance 0.70)
4. Re-entrenar solo capas finales de Random Forest
5. Validar con holdout de webcam

**Ganancia esperada:** +20-30% accuracy online

---

### Largo Plazo (3-6 meses) - Arquitectura Mejorada

#### 6. Migraci√≥n a Modelo Temporal

**Limitaci√≥n actual: Random Forest no captura temporal dependencies**

**Propuesta: LSTM bidireccional**

```python
import tensorflow as tf

# Entrada: secuencia de 30 frames √ó 83 features
model = tf.keras.Sequential([
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(128, return_sequences=True),
        input_shape=(30, 83)
    ),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(64)
    ),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(5, activation='softmax')
])

# Ventaja: Captura contexto temporal expl√≠citamente
# Desventaja: +10x tiempo de inferencia
```

**Ganancia esperada:** +30-40% accuracy en actividades de transici√≥n

#### 7. Ensemble Multi-√Ångulo

**Concepto:**
- Entrenar 4 modelos especializados (uno por √°ngulo)
- En producci√≥n, estimar √°ngulo de usuario
- Seleccionar modelo apropiado

```python
def estimate_camera_angle(landmarks):
    # Usar visibility de landmarks frontales vs traseros
    front_vis = np.mean([landmarks[0], landmarks[11], landmarks[12]])  # Nariz, hombros
    back_vis = np.mean([landmarks[23], landmarks[24]])  # Caderas
    
    if front_vis > 0.8 and back_vis > 0.6:
        return "frontal"  # 0¬∞
    elif front_vis < 0.5 and back_vis > 0.7:
        return "trasero"  # 180¬∞
    else:
        return "lateral"  # 90¬∞ o 270¬∞

# Predicci√≥n
angle = estimate_camera_angle(landmarks)
model = model_ensemble[angle]
prediction = model.predict(features)
```

**Ganancia esperada:** +15-25% accuracy en actividades direccionales

---

## üìö Lecciones Aprendidas

### 1. **"98% offline ‚â† 98% online"**
La evaluaci√≥n en test set con condiciones controladas NO garantiza performance en producci√≥n. El gap de 54% en nuestro caso es evidencia contundente.

### 2. **Contexto temporal es cr√≠tico**
Actividades humanas son secuencias continuas, no snapshots aislados. Random Forest frame-by-frame es insuficiente para movimientos complejos.

### 3. **Diversidad de datos > Cantidad de datos**
90 videos de 18 personas en 1 √°ngulo < 300 videos de 10 personas en 4 √°ngulos.

### 4. **Condiciones controladas son idealizaci√≥n**
El mundo real tiene:
- Iluminaci√≥n variable
- √Ångulos no √≥ptimos
- Fondos desordenados
- Usuarios con diferentes caracter√≠sticas

### 5. **Tests unitarios para feature parity son esenciales**
Asegurar que `compute_geometric_features()` en training == producci√≥n previene bugs silenciosos.

### 6. **Monitoreo continuo es necesario**
Detectar drift de features en producci√≥n permite intervenci√≥n temprana.

### 7. **Prototipo != Producto**
Un demo funcional en condiciones ideales requiere **√≥rdenes de magnitud m√°s trabajo** para ser robusto en producci√≥n.

---

## üéØ Priorizaci√≥n de Acciones

### Implementaci√≥n Inmediata (Esta Semana)
1. ‚úÖ Buffer temporal (10 frames)
2. ‚úÖ Umbral de confianza adaptativo
3. ‚úÖ Calibraci√≥n MediaPipe (model_complexity=2)

**Esfuerzo:** 4-6 horas  
**Ganancia esperada:** +15-20% accuracy online

### Implementaci√≥n Corto Plazo (2-4 Semanas)
4. ‚è≥ Fine-tuning con 200 clips de webcam
5. ‚è≥ Expansi√≥n dataset a 15 personas √ó 4 √°ngulos

**Esfuerzo:** 110 horas (equipo de 3)  
**Ganancia esperada:** +30-40% accuracy online

### Implementaci√≥n Largo Plazo (3-6 Meses)
6. üîÆ Migraci√≥n a LSTM bidireccional
7. üîÆ Ensemble multi-√°ngulo

**Esfuerzo:** 200+ horas  
**Ganancia esperada:** +40-50% accuracy online (objetivo: >90%)

---

## üìä M√©tricas de √âxito Post-Mejoras

### Objetivo de Deployment Robusto

| Actividad | Target Accuracy | Actual Online | Gap |
|-----------|----------------|---------------|-----|
| Caminar Hacia | ‚â•90% | ~85% | -5% |
| Caminar Regreso | ‚â•85% | ~25% | **-60%** ‚Üê CR√çTICO |
| Girar | ‚â•85% | ~35% | **-50%** ‚Üê CR√çTICO |
| Ponerse Pie | ‚â•80% | ~30% | **-50%** ‚Üê CR√çTICO |
| Sentarse | ‚â•80% | ~25% | **-55%** ‚Üê CR√çTICO |

**Target promedio:** ‚â•85%  
**Actual promedio:** ~40%  
**Gap a cerrar:** -45%

---

## üîó Referencias

### Trabajos Relacionados que Abordan el Gap Offline-Online

1. **"Bridging the Gap between Training and Inference for Video Super-Resolution"** (CVPR 2022)
   - Propone data augmentation espec√≠fico para condiciones de producci√≥n

2. **"Real-world Human Activity Recognition using Smartphone Sensors"** (IEEE Sensors 2019)
   - Documenta gap 30-40% entre lab y wild data

3. **"Temporal Segment Networks for Action Recognition in Videos"** (ECCV 2016)
   - Demuestra superioridad de modelos temporales para actividades complejas
