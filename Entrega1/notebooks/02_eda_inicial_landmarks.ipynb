{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An√°lisis Exploratorio de Datos (EDA) - Sistema de Anotaci√≥n de Video\n",
        "**Entrega 1 - Inteligencia Artificial**\n",
        "\n",
        "Este notebook realiza el an√°lisis exploratorio completo de los landmarks extra√≠dos de los videos del equipo usando MediaPipe.\n",
        "\n",
        "## Objetivos del EDA:\n",
        "1. **Cargar y explorar** los datasets de landmarks generados\n",
        "2. **Analizar distribuci√≥n** de actividades y participantes\n",
        "3. **Visualizar patrones** de movimiento por actividad\n",
        "4. **Evaluar calidad** de detecci√≥n de MediaPipe\n",
        "5. **Identificar caracter√≠sticas** distintivas entre actividades\n",
        "6. **Preparar datos** para modelado futuro\n",
        "\n",
        "## Dataset Esperado:\n",
        "- **üìÅ 30 videos** del equipo (10 por persona)\n",
        "- **üìä 5 actividades** diferentes\n",
        "- **üéØ 16 landmarks** relevantes por frame\n",
        "- **üë• 3 participantes** diversos\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "U-LkYmeRKeqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQN7Xo5wKcpN",
        "outputId": "d7f47c17-122e-4b1a-9fa3-77ed0a6d8ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Librer√≠as importadas correctamente\n",
            "üìä Pandas version: 2.2.2\n",
            "üìà Matplotlib version: 3.10.0\n",
            "üé® Seaborn version: 0.13.2\n"
          ]
        }
      ],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar visualizaciones\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn version: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n del proyecto para EDA\n",
        "EDA_CONFIG = {\n",
        "    'activities': [\n",
        "        'caminar_hacia',\n",
        "        'caminar_regreso',\n",
        "        'girar',\n",
        "        'sentarse',\n",
        "        'ponerse_pie'\n",
        "    ],\n",
        "    'team_members': {\n",
        "        'P001': 'Juan Esteban Ruiz',\n",
        "        'P002': 'Juan David Quintero',\n",
        "        'P003': 'Tomas Quintero'\n",
        "    },\n",
        "    'landmark_names': [\n",
        "        'L_shoulder', 'R_shoulder', 'L_elbow', 'R_elbow',\n",
        "        'L_wrist', 'R_wrist', 'L_hip', 'R_hip',\n",
        "        'L_knee', 'R_knee', 'L_ankle', 'R_ankle',\n",
        "        'L_heel', 'R_heel', 'L_foot', 'R_foot'\n",
        "    ],\n",
        "    'colors': {\n",
        "        'caminar_hacia': '#1f77b4',\n",
        "        'caminar_regreso': '#ff7f0e',\n",
        "        'girar': '#2ca02c',\n",
        "        'sentarse': '#d62728',\n",
        "        'ponerse_pie': '#9467bd'\n",
        "    },\n",
        "    'paths': {\n",
        "        'landmarks': 'data/landmarks/',\n",
        "        'metadata': 'data/metadata/',\n",
        "        'videos': 'data/videos/',\n",
        "        'results': 'data/eda_results/'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Crear directorio para resultados del EDA\n",
        "os.makedirs(EDA_CONFIG['paths']['results'], exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n EDA cargada\")\n",
        "print(f\"üéØ Actividades objetivo: {len(EDA_CONFIG['activities'])}\")\n",
        "print(f\"üë• Miembros del equipo: {len(EDA_CONFIG['team_members'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H3pfXbSKpxU",
        "outputId": "b8efaaf0-a34e-4302-b32c-7de1f555d3db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuraci√≥n EDA cargada\n",
            "üéØ Actividades objetivo: 5\n",
            "üë• Miembros del equipo: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASE PRINCIPAL PARA EDA\n",
        "class LandmarksEDA:\n",
        "    \"\"\"An√°lisis Exploratorio de Datos de Landmarks\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        if config is None:\n",
        "            config = EDA_CONFIG\n",
        "        self.config = config\n",
        "        self.landmarks_data = None\n",
        "        self.summary_stats = {}\n",
        "\n",
        "    def load_all_landmarks(self):\n",
        "        \"\"\"Cargar todos los archivos CSV de landmarks\"\"\"\n",
        "        print(\"üìÇ CARGANDO DATASETS DE LANDMARKS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        landmarks_dir = Path(self.config['paths']['landmarks'])\n",
        "\n",
        "        if not landmarks_dir.exists():\n",
        "            print(f\"‚ùå Directorio no encontrado: {landmarks_dir}\")\n",
        "            return None\n",
        "\n",
        "        # Buscar todos los archivos CSV\n",
        "        csv_files = list(landmarks_dir.glob(\"*_landmarks.csv\"))\n",
        "\n",
        "        if not csv_files:\n",
        "            print(f\"‚ùå No se encontraron archivos de landmarks en {landmarks_dir}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"üìÅ Archivos encontrados: {len(csv_files)}\")\n",
        "\n",
        "        # Cargar y combinar todos los CSV\n",
        "        all_dataframes = []\n",
        "        loading_stats = {\n",
        "            'total_files': len(csv_files),\n",
        "            'loaded_successfully': 0,\n",
        "            'failed_files': [],\n",
        "            'total_frames': 0\n",
        "        }\n",
        "\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Extraer informaci√≥n del nombre del archivo\n",
        "                filename_parts = csv_file.stem.replace('_landmarks', '').split('_')\n",
        "\n",
        "                # Intentar extraer participante y actividad\n",
        "                if len(filename_parts) >= 2:\n",
        "                    participant = filename_parts[0] if filename_parts[0] in self.config['team_members'] else 'Unknown'\n",
        "                    activity = '_'.join(filename_parts[1:]) if len(filename_parts) > 2 else filename_parts[1]\n",
        "                else:\n",
        "                    participant = 'Unknown'\n",
        "                    activity = 'Unknown'\n",
        "\n",
        "                # Agregar metadata si no existe\n",
        "                if 'activity' not in df.columns:\n",
        "                    df['activity'] = activity\n",
        "                if 'participant' not in df.columns:\n",
        "                    df['participant'] = participant\n",
        "                if 'video_file' not in df.columns:\n",
        "                    df['video_file'] = csv_file.stem.replace('_landmarks', '')\n",
        "\n",
        "                all_dataframes.append(df)\n",
        "                loading_stats['loaded_successfully'] += 1\n",
        "                loading_stats['total_frames'] += len(df)\n",
        "\n",
        "                print(f\"   ‚úÖ {csv_file.name}: {len(df)} frames\")\n",
        "\n",
        "            except Exception as e:\n",
        "                loading_stats['failed_files'].append(f\"{csv_file.name}: {str(e)}\")\n",
        "                print(f\"   ‚ùå Error cargando {csv_file.name}: {e}\")\n",
        "\n",
        "        if not all_dataframes:\n",
        "            print(\"‚ùå No se pudieron cargar datos\")\n",
        "            return None\n",
        "\n",
        "        # Combinar todos los DataFrames\n",
        "        self.landmarks_data = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "        print(f\"\\nüìä RESUMEN DE CARGA:\")\n",
        "        print(f\"   ‚úÖ Archivos cargados: {loading_stats['loaded_successfully']}/{loading_stats['total_files']}\")\n",
        "        print(f\"   üìä Total frames: {loading_stats['total_frames']:,}\")\n",
        "        print(f\"   üé¨ Videos √∫nicos: {self.landmarks_data['video_file'].nunique()}\")\n",
        "        print(f\"   üë• Participantes: {self.landmarks_data['participant'].nunique()}\")\n",
        "        print(f\"   üéØ Actividades: {self.landmarks_data['activity'].nunique()}\")\n",
        "\n",
        "        if loading_stats['failed_files']:\n",
        "            print(f\"\\n‚ö†Ô∏è ARCHIVOS CON ERRORES:\")\n",
        "            for error in loading_stats['failed_files']:\n",
        "                print(f\"   ‚Ä¢ {error}\")\n",
        "\n",
        "        # Guardar estad√≠sticas de carga\n",
        "        with open(f\"{self.config['paths']['results']}loading_stats.json\", 'w') as f:\n",
        "            json.dump(loading_stats, f, indent=2)\n",
        "\n",
        "        return self.landmarks_data\n",
        "\n",
        "    def basic_dataset_info(self):\n",
        "        \"\"\"Informaci√≥n b√°sica del dataset\"\"\"\n",
        "        if self.landmarks_data is None:\n",
        "            print(\"‚ùå Primero ejecuta load_all_landmarks()\")\n",
        "            return\n",
        "\n",
        "        print(\"üìã INFORMACI√ìN B√ÅSICA DEL DATASET\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        df = self.landmarks_data\n",
        "\n",
        "        print(f\"üìä DIMENSIONES:\")\n",
        "        print(f\"   Filas (frames): {len(df):,}\")\n",
        "        print(f\"   Columnas: {len(df.columns)}\")\n",
        "        print(f\"   Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        print(f\"\\nüéØ DISTRIBUCI√ìN POR ACTIVIDAD:\")\n",
        "        activity_counts = df['activity'].value_counts()\n",
        "        for activity, count in activity_counts.items():\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"   {activity.replace('_', ' ').title()}: {count:,} frames ({percentage:.1f}%)\")\n",
        "\n",
        "        print(f\"\\nüë• DISTRIBUCI√ìN POR PARTICIPANTE:\")\n",
        "        participant_counts = df['participant'].value_counts()\n",
        "        for participant, count in participant_counts.items():\n",
        "            name = self.config['team_members'].get(participant, participant)\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"   {participant} ({name}): {count:,} frames ({percentage:.1f}%)\")\n",
        "\n",
        "        print(f\"\\nüé¨ VIDEOS POR ACTIVIDAD Y PARTICIPANTE:\")\n",
        "        video_summary = df.groupby(['activity', 'participant']).agg({\n",
        "            'video_file': 'nunique',\n",
        "            'frame_number': 'count'\n",
        "        }).round(2)\n",
        "        print(video_summary)\n",
        "\n",
        "        # Verificar datos faltantes\n",
        "        missing_data = df.isnull().sum()\n",
        "        landmark_columns = [col for col in df.columns if any(landmark in col for landmark in self.config['landmark_names'])]\n",
        "        missing_landmarks = missing_data[landmark_columns]\n",
        "\n",
        "        if missing_landmarks.sum() > 0:\n",
        "            print(f\"\\n‚ö†Ô∏è DATOS FALTANTES EN LANDMARKS:\")\n",
        "            print(f\"   Total missing values: {missing_landmarks.sum():,}\")\n",
        "            print(f\"   Porcentaje: {(missing_landmarks.sum() / (len(df) * len(landmark_columns))) * 100:.2f}%\")\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ SIN DATOS FALTANTES EN LANDMARKS\")\n",
        "\n",
        "        # Estad√≠sticas de calidad\n",
        "        if 'detection_rate' in df.columns:\n",
        "            print(f\"\\nüìà CALIDAD DE DETECCI√ìN MEDIAPIPE:\")\n",
        "            print(f\"   Promedio: {df['detection_rate'].mean():.1f}%\")\n",
        "            print(f\"   Mediana: {df['detection_rate'].median():.1f}%\")\n",
        "            print(f\"   Min/Max: {df['detection_rate'].min():.1f}% / {df['detection_rate'].max():.1f}%\")\n",
        "\n",
        "        return {\n",
        "            'total_frames': len(df),\n",
        "            'total_videos': df['video_file'].nunique(),\n",
        "            'activities': activity_counts.to_dict(),\n",
        "            'participants': participant_counts.to_dict(),\n",
        "            'missing_data_percentage': (missing_landmarks.sum() / (len(df) * len(landmark_columns))) * 100 if len(landmark_columns) > 0 else 0\n",
        "        }\n",
        "\n",
        "# Crear instancia del analizador EDA\n",
        "eda = LandmarksEDA()\n",
        "print(\"‚úÖ Analizador EDA configurado\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB6ozGePKqek",
        "outputId": "8f3c336b-d5c3-410d-dbe3-6a63fa7327ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Analizador EDA configurado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR Y EXPLORAR DATOS\n",
        "print(\"üîÑ CARGANDO DATASETS DE LANDMARKS...\")\n",
        "landmarks_df = eda.load_all_landmarks()\n",
        "\n",
        "if landmarks_df is not None:\n",
        "    print(\"\\n‚úÖ DATOS CARGADOS EXITOSAMENTE\")\n",
        "\n",
        "    # Mostrar informaci√≥n b√°sica\n",
        "    basic_info = eda.basic_dataset_info()\n",
        "\n",
        "    # Mostrar primeras filas\n",
        "    print(f\"\\nüëÄ PRIMERAS 5 FILAS DEL DATASET:\")\n",
        "    display(landmarks_df.head())\n",
        "\n",
        "    # Mostrar estructura de columnas\n",
        "    print(f\"\\nüìã COLUMNAS DEL DATASET:\")\n",
        "    landmark_cols = [col for col in landmarks_df.columns if any(lm in col for lm in EDA_CONFIG['landmark_names'])]\n",
        "    metadata_cols = [col for col in landmarks_df.columns if col not in landmark_cols]\n",
        "\n",
        "    print(f\"   üìä Columnas de landmarks: {len(landmark_cols)}\")\n",
        "    print(f\"   üìù Columnas de metadata: {len(metadata_cols)}\")\n",
        "    print(f\"   üìã Metadata: {metadata_cols}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå ERROR: No se pudieron cargar los datos\")\n",
        "    print(\"üí° Aseg√∫rate de haber ejecutado el notebook 1 y subido videos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7px1Ac4WKviX",
        "outputId": "4820ae0e-703e-43bb-fd3e-733aea47bf21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ CARGANDO DATASETS DE LANDMARKS...\n",
            "üìÇ CARGANDO DATASETS DE LANDMARKS\n",
            "==================================================\n",
            "‚ùå Directorio no encontrado: data/landmarks\n",
            "‚ùå ERROR: No se pudieron cargar los datos\n",
            "üí° Aseg√∫rate de haber ejecutado el notebook 1 y subido videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZACIONES PRINCIPALES - DISTRIBUCIONES\n",
        "def create_distribution_visualizations(df):\n",
        "    \"\"\"Crear visualizaciones de distribuci√≥n del dataset\"\"\"\n",
        "    print(\"üìä CREANDO VISUALIZACIONES DE DISTRIBUCI√ìN\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Configurar subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Distribuci√≥n del Dataset de Actividades Humanas', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Distribuci√≥n por actividad\n",
        "    activity_counts = df['activity'].value_counts()\n",
        "    colors = [EDA_CONFIG['colors'].get(activity, '#gray') for activity in activity_counts.index]\n",
        "\n",
        "    axes[0,0].pie(activity_counts.values, labels=[act.replace('_', ' ').title() for act in activity_counts.index],\n",
        "                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "    axes[0,0].set_title('Distribuci√≥n por Actividad')\n",
        "\n",
        "    # 2. Videos por participante\n",
        "    participant_videos = df.groupby('participant')['video_file'].nunique()\n",
        "    axes[0,1].bar(range(len(participant_videos)), participant_videos.values,\n",
        "                  color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "    axes[0,1].set_title('Videos por Participante')\n",
        "    axes[0,1].set_xlabel('Participante')\n",
        "    axes[0,1].set_ylabel('N√∫mero de Videos')\n",
        "    axes[0,1].set_xticks(range(len(participant_videos)))\n",
        "    axes[0,1].set_xticklabels([f\"{p}\\n({EDA_CONFIG['team_members'].get(p, p)})\" for p in participant_videos.index])\n",
        "\n",
        "    # 3. Frames por actividad\n",
        "    frames_per_activity = df.groupby('activity').size()\n",
        "    axes[1,0].barh(range(len(frames_per_activity)), frames_per_activity.values,\n",
        "                   color=[EDA_CONFIG['colors'].get(activity, '#gray') for activity in frames_per_activity.index])\n",
        "    axes[1,0].set_title('Frames por Actividad')\n",
        "    axes[1,0].set_xlabel('N√∫mero de Frames')\n",
        "    axes[1,0].set_yticks(range(len(frames_per_activity)))\n",
        "    axes[1,0].set_yticklabels([act.replace('_', ' ').title() for act in frames_per_activity.index])\n",
        "\n",
        "    # 4. Matriz de videos por participante y actividad\n",
        "    video_matrix = df.groupby(['activity', 'participant'])['video_file'].nunique().unstack(fill_value=0)\n",
        "    sns.heatmap(video_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
        "    axes[1,1].set_title('Videos por Participante y Actividad')\n",
        "    axes[1,1].set_xlabel('Participante')\n",
        "    axes[1,1].set_ylabel('Actividad')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{EDA_CONFIG['paths']['results']}distribuciones_dataset.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Crear visualizaciones si tenemos datos\n",
        "if landmarks_df is not None:\n",
        "    dist_viz = create_distribution_visualizations(landmarks_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos para visualizar\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MroaSkEKx9N",
        "outputId": "6fac82f1-d17e-4b3a-f329-cc51b758a77d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos para visualizar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AN√ÅLISIS DE CALIDAD DE DETECCI√ìN MEDIAPIPE\n",
        "def analyze_detection_quality(df):\n",
        "    \"\"\"Analizar calidad de detecci√≥n de MediaPipe\"\"\"\n",
        "    print(\"üîç AN√ÅLISIS DE CALIDAD DE DETECCI√ìN MEDIAPIPE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Verificar si tenemos columna de detection_rate\n",
        "    if 'detection_rate' not in df.columns:\n",
        "        print(\"‚ö†Ô∏è No se encontr√≥ columna 'detection_rate'\")\n",
        "        print(\"üí° Calculando calidad basada en datos faltantes...\")\n",
        "\n",
        "        # Calcular calidad basada en NaN values\n",
        "        landmark_cols = [col for col in df.columns if any(lm in col for lm in EDA_CONFIG['landmark_names'])]\n",
        "        df['calculated_quality'] = 100 - (df[landmark_cols].isnull().sum(axis=1) / len(landmark_cols) * 100)\n",
        "        quality_col = 'calculated_quality'\n",
        "    else:\n",
        "        quality_col = 'detection_rate'\n",
        "\n",
        "    # Estad√≠sticas generales de calidad\n",
        "    print(f\"üìä ESTAD√çSTICAS DE CALIDAD:\")\n",
        "    print(f\"   Promedio: {df[quality_col].mean():.1f}%\")\n",
        "    print(f\"   Mediana: {df[quality_col].median():.1f}%\")\n",
        "    print(f\"   Desviaci√≥n est√°ndar: {df[quality_col].std():.1f}%\")\n",
        "    print(f\"   Min/Max: {df[quality_col].min():.1f}% / {df[quality_col].max():.1f}%\")\n",
        "\n",
        "    # Calidad por actividad\n",
        "    print(f\"\\nüéØ CALIDAD POR ACTIVIDAD:\")\n",
        "    quality_by_activity = df.groupby('activity')[quality_col].agg(['mean', 'median', 'std']).round(1)\n",
        "    for activity, stats in quality_by_activity.iterrows():\n",
        "        print(f\"   {activity.replace('_', ' ').title()}: {stats['mean']:.1f}% ¬± {stats['std']:.1f}%\")\n",
        "\n",
        "    # Calidad por participante\n",
        "    print(f\"\\nüë• CALIDAD POR PARTICIPANTE:\")\n",
        "    quality_by_participant = df.groupby('participant')[quality_col].agg(['mean', 'median', 'std']).round(1)\n",
        "    for participant, stats in quality_by_participant.iterrows():\n",
        "        name = EDA_CONFIG['team_members'].get(participant, participant)\n",
        "        print(f\"   {participant} ({name}): {stats['mean']:.1f}% ¬± {stats['std']:.1f}%\")\n",
        "\n",
        "    # Visualizaci√≥n de calidad\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    fig.suptitle('An√°lisis de Calidad de Detecci√≥n MediaPipe', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Histograma de calidad general\n",
        "    axes[0].hist(df[quality_col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0].axvline(df[quality_col].mean(), color='red', linestyle='--', label=f'Promedio: {df[quality_col].mean():.1f}%')\n",
        "    axes[0].set_title('Distribuci√≥n de Calidad de Detecci√≥n')\n",
        "    axes[0].set_xlabel('Calidad de Detecci√≥n (%)')\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Boxplot por actividad\n",
        "    activities = df['activity'].unique()\n",
        "    quality_data = [df[df['activity'] == activity][quality_col].values for activity in activities]\n",
        "    bp1 = axes[1].boxplot(quality_data, labels=[act.replace('_', ' ').title() for act in activities], patch_artist=True)\n",
        "\n",
        "    # Colorear boxplots\n",
        "    for patch, activity in zip(bp1['boxes'], activities):\n",
        "        patch.set_facecolor(EDA_CONFIG['colors'].get(activity, '#gray'))\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    axes[1].set_title('Calidad por Actividad')\n",
        "    axes[1].set_xlabel('Actividad')\n",
        "    axes[1].set_ylabel('Calidad de Detecci√≥n (%)')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    plt.setp(axes[1].get_xticklabels(), rotation=45)\n",
        "\n",
        "    # Calidad por participante\n",
        "    participants = df['participant'].unique()\n",
        "    quality_data_p = [df[df['participant'] == p][quality_col].values for p in participants]\n",
        "    bp2 = axes[2].boxplot(quality_data_p, labels=[f\"{p}\\n{EDA_CONFIG['team_members'].get(p, p)}\" for p in participants],\n",
        "                          patch_artist=True)\n",
        "\n",
        "    colors_p = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "    for patch, color in zip(bp2['boxes'], colors_p):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    axes[2].set_title('Calidad por Participante')\n",
        "    axes[2].set_xlabel('Participante')\n",
        "    axes[2].set_ylabel('Calidad de Detecci√≥n (%)')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{EDA_CONFIG['paths']['results']}calidad_deteccion.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Identificar videos con baja calidad\n",
        "    threshold = 70.0  # 70% threshold\n",
        "    low_quality = df[df[quality_col] < threshold]\n",
        "\n",
        "    if not low_quality.empty:\n",
        "        print(f\"\\n‚ö†Ô∏è VIDEOS CON CALIDAD BAJA (<{threshold}%):\")\n",
        "        low_quality_summary = low_quality.groupby(['activity', 'participant', 'video_file'])[quality_col].mean().reset_index()\n",
        "        low_quality_summary = low_quality_summary.sort_values(quality_col)\n",
        "\n",
        "        for _, row in low_quality_summary.head(10).iterrows():\n",
        "            print(f\"   üìπ {row['video_file']}: {row[quality_col]:.1f}% ({row['activity']}, {row['participant']})\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ TODOS LOS VIDEOS TIENEN CALIDAD ‚â•{threshold}%\")\n",
        "\n",
        "    return {\n",
        "        'average_quality': df[quality_col].mean(),\n",
        "        'quality_by_activity': quality_by_activity.to_dict(),\n",
        "        'quality_by_participant': quality_by_participant.to_dict(),\n",
        "        'low_quality_videos': len(low_quality)\n",
        "    }\n",
        "\n",
        "# Ejecutar an√°lisis de calidad\n",
        "if landmarks_df is not None:\n",
        "    quality_analysis = analyze_detection_quality(landmarks_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos para analizar calidad\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM6VfdTQKz2M",
        "outputId": "e63f82b7-5210-4110-ab6e-bd0b4c6ac97a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos para analizar calidad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AN√ÅLISIS DE PATRONES DE MOVIMIENTO\n",
        "def analyze_movement_patterns(df):\n",
        "    \"\"\"Analizar patrones de movimiento por actividad\"\"\"\n",
        "    print(\"üèÉ AN√ÅLISIS DE PATRONES DE MOVIMIENTO\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Seleccionar landmarks clave para an√°lisis\n",
        "    key_landmarks = ['L_shoulder', 'R_shoulder', 'L_hip', 'R_hip', 'L_knee', 'R_knee']\n",
        "\n",
        "    # Crear figura con subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=3,\n",
        "        subplot_titles=[act.replace('_', ' ').title() for act in EDA_CONFIG['activities']],\n",
        "        specs=[[{'secondary_y': True}]*3, [{'secondary_y': True}]*3]\n",
        "    )\n",
        "\n",
        "    activities = EDA_CONFIG['activities']\n",
        "\n",
        "    for i, activity in enumerate(activities):\n",
        "        activity_data = df[df['activity'] == activity]\n",
        "\n",
        "        if activity_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Calcular posici√≥n del subplot\n",
        "        row = (i // 3) + 1\n",
        "        col = (i % 3) + 1\n",
        "\n",
        "        # Analizar movimiento del centro de masa (promedio de caderas)\n",
        "        if f'L_hip_y' in activity_data.columns and f'R_hip_y' in activity_data.columns:\n",
        "            # Calcular centro de masa vertical\n",
        "            center_of_mass_y = (activity_data['L_hip_y'] + activity_data['R_hip_y']) / 2\n",
        "\n",
        "            # Tomar muestra representativa\n",
        "            sample_size = min(100, len(center_of_mass_y))\n",
        "            sample_indices = np.linspace(0, len(center_of_mass_y)-1, sample_size, dtype=int)\n",
        "            sample_y = center_of_mass_y.iloc[sample_indices]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=list(range(len(sample_y))),\n",
        "                    y=sample_y,\n",
        "                    mode='lines',\n",
        "                    name=f'Centro Y - {activity}',\n",
        "                    line=dict(color=EDA_CONFIG['colors'][activity], width=2)\n",
        "                ),\n",
        "                row=row, col=col\n",
        "            )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=\"Patrones de Movimiento por Actividad (Centro de Masa Vertical)\",\n",
        "        height=800,\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # An√°lisis estad√≠stico de movimiento\n",
        "    movement_stats = {}\n",
        "\n",
        "    print(f\"\\nüìà ESTAD√çSTICAS DE MOVIMIENTO POR ACTIVIDAD:\")\n",
        "\n",
        "    for activity in EDA_CONFIG['activities']:\n",
        "        activity_data = df[df['activity'] == activity]\n",
        "\n",
        "        if activity_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Calcular variabilidad de movimiento\n",
        "        movement_features = []\n",
        "\n",
        "        for landmark in key_landmarks:\n",
        "            for coord in ['x', 'y']:\n",
        "                col_name = f'{landmark}_{coord}'\n",
        "                if col_name in activity_data.columns:\n",
        "                    # Calcular variabilidad (desviaci√≥n est√°ndar)\n",
        "                    variability = activity_data[col_name].std()\n",
        "                    movement_features.append(variability)\n",
        "\n",
        "        if movement_features:\n",
        "            avg_movement = np.mean(movement_features)\n",
        "            movement_stats[activity] = {\n",
        "                'avg_variability': avg_movement,\n",
        "                'total_frames': len(activity_data),\n",
        "                'unique_videos': activity_data['video_file'].nunique()\n",
        "            }\n",
        "\n",
        "            print(f\"   {activity.replace('_', ' ').title()}:\")\n",
        "            print(f\"      Variabilidad promedio: {avg_movement:.4f}\")\n",
        "            print(f\"      Frames totales: {len(activity_data):,}\")\n",
        "            print(f\"      Videos √∫nicos: {activity_data['video_file'].nunique()}\")\n",
        "\n",
        "    return movement_stats\n",
        "\n",
        "# Ejecutar an√°lisis de patrones de movimiento\n",
        "if landmarks_df is not None:\n",
        "    movement_analysis = analyze_movement_patterns(landmarks_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos para analizar patrones de movimiento\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wkVWJIFK2hd",
        "outputId": "d0435a1f-1922-4b9a-a9f8-75387f2dc4db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos para analizar patrones de movimiento\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MATRIZ DE CORRELACI√ìN ENTRE LANDMARKS\n",
        "def create_correlation_analysis(df):\n",
        "    \"\"\"Crear an√°lisis de correlaci√≥n entre landmarks\"\"\"\n",
        "    print(\"üîó AN√ÅLISIS DE CORRELACI√ìN ENTRE LANDMARKS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Seleccionar columnas de landmarks\n",
        "    landmark_cols = [col for col in df.columns if any(lm in col for lm in EDA_CONFIG['landmark_names'])]\n",
        "\n",
        "    if not landmark_cols:\n",
        "        print(\"‚ùå No se encontraron columnas de landmarks\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üìä Analizando correlaciones entre {len(landmark_cols)} variables de landmarks\")\n",
        "\n",
        "    # Calcular matriz de correlaci√≥n\n",
        "    landmarks_numeric = df[landmark_cols].select_dtypes(include=[np.number])\n",
        "    correlation_matrix = landmarks_numeric.corr()\n",
        "\n",
        "    # Crear heatmap de correlaci√≥n\n",
        "    plt.figure(figsize=(20, 16))\n",
        "\n",
        "    # Seleccionar subset de landmarks m√°s importantes para visualizaci√≥n\n",
        "    important_landmarks = []\n",
        "    for landmark in EDA_CONFIG['landmark_names'][:8]:  # Primeros 8 landmarks\n",
        "        for coord in ['x', 'y']:\n",
        "            col_name = f'{landmark}_{coord}'\n",
        "            if col_name in landmarks_numeric.columns:\n",
        "                important_landmarks.append(col_name)\n",
        "\n",
        "    if important_landmarks:\n",
        "        subset_corr = correlation_matrix.loc[important_landmarks, important_landmarks]\n",
        "\n",
        "        mask = np.triu(np.ones_like(subset_corr, dtype=bool))\n",
        "\n",
        "        sns.heatmap(subset_corr,\n",
        "                   mask=mask,\n",
        "                   annot=True,\n",
        "                   cmap='RdBu_r',\n",
        "                   center=0,\n",
        "                   square=True,\n",
        "                   fmt='.2f',\n",
        "                   cbar_kws={'label': 'Correlaci√≥n de Pearson'})\n",
        "\n",
        "        plt.title('Matriz de Correlaci√≥n - Landmarks Principales', fontsize=16, fontweight='bold')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{EDA_CONFIG['paths']['results']}correlacion_landmarks.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # Encontrar correlaciones m√°s altas (excluyendo autocorrelaciones)\n",
        "    correlation_pairs = []\n",
        "\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_value = correlation_matrix.iloc[i, j]\n",
        "            if not np.isnan(corr_value):\n",
        "                correlation_pairs.append({\n",
        "                    'var1': correlation_matrix.columns[i],\n",
        "                    'var2': correlation_matrix.columns[j],\n",
        "                    'correlation': corr_value\n",
        "                })\n",
        "\n",
        "    # Ordenar por valor absoluto de correlaci√≥n\n",
        "    correlation_pairs = sorted(correlation_pairs, key=lambda x: abs(x['correlation']), reverse=True)\n",
        "\n",
        "    print(f\"\\nüîù TOP 10 CORRELACIONES M√ÅS ALTAS:\")\n",
        "    for i, pair in enumerate(correlation_pairs[:10]):\n",
        "        print(f\"   {i+1:2d}. {pair['var1']} ‚Üî {pair['var2']}: {pair['correlation']:.3f}\")\n",
        "\n",
        "    print(f\"\\nüîª TOP 5 CORRELACIONES M√ÅS BAJAS (INDEPENDIENTES):\")\n",
        "    low_correlations = [pair for pair in correlation_pairs if abs(pair['correlation']) < 0.3]\n",
        "    for i, pair in enumerate(low_correlations[:5]):\n",
        "        print(f\"   {i+1}. {pair['var1']} ‚Üî {pair['var2']}: {pair['correlation']:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'correlation_matrix': correlation_matrix,\n",
        "        'top_correlations': correlation_pairs[:10],\n",
        "        'low_correlations': low_correlations[:5]\n",
        "    }\n",
        "\n",
        "# Ejecutar an√°lisis de correlaci√≥n\n",
        "if landmarks_df is not None:\n",
        "    correlation_analysis = create_correlation_analysis(landmarks_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos para an√°lisis de correlaci√≥n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrltIHKxK428",
        "outputId": "1e6fe9e5-96bb-4b91-e03f-72ee2a0e3996"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos para an√°lisis de correlaci√≥n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPARACI√ìN ENTRE ACTIVIDADES - AN√ÅLISIS DISCRIMINATIVO\n",
        "def discriminative_analysis(df):\n",
        "    \"\"\"An√°lizar caracter√≠sticas discriminativas entre actividades\"\"\"\n",
        "    print(\"üéØ AN√ÅLISIS DISCRIMINATIVO ENTRE ACTIVIDADES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Seleccionar landmarks clave\n",
        "    key_features = []\n",
        "    for landmark in ['L_shoulder', 'R_shoulder', 'L_hip', 'R_hip', 'L_knee', 'R_knee']:\n",
        "        for coord in ['x', 'y']:\n",
        "            col_name = f'{landmark}_{coord}'\n",
        "            if col_name in df.columns:\n",
        "                key_features.append(col_name)\n",
        "\n",
        "    if not key_features:\n",
        "        print(\"‚ùå No se encontraron caracter√≠sticas clave\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üìä Analizando {len(key_features)} caracter√≠sticas discriminativas\")\n",
        "\n",
        "    # Calcular estad√≠sticas por actividad\n",
        "    activity_stats = {}\n",
        "\n",
        "    for activity in EDA_CONFIG['activities']:\n",
        "        activity_data = df[df['activity'] == activity]\n",
        "\n",
        "        if activity_data.empty:\n",
        "            continue\n",
        "\n",
        "        activity_stats[activity] = {}\n",
        "\n",
        "        for feature in key_features:\n",
        "            if feature in activity_data.columns:\n",
        "                activity_stats[activity][feature] = {\n",
        "                    'mean': activity_data[feature].mean(),\n",
        "                    'std': activity_data[feature].std(),\n",
        "                    'median': activity_data[feature].median()\n",
        "                }\n",
        "\n",
        "    # Crear visualizaci√≥n comparativa\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('An√°lisis Discriminativo - Caracter√≠sticas por Actividad', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Seleccionar caracter√≠sticas m√°s discriminativas\n",
        "    discriminative_features = key_features[:6]  # Top 6 caracter√≠sticas\n",
        "\n",
        "    for i, feature in enumerate(discriminative_features):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "\n",
        "        # Crear boxplot para esta caracter√≠stica\n",
        "        feature_data = []\n",
        "        labels = []\n",
        "        colors_list = []\n",
        "\n",
        "        for activity in EDA_CONFIG['activities']:\n",
        "            activity_data = df[df['activity'] == activity]\n",
        "            if not activity_data.empty and feature in activity_data.columns:\n",
        "                feature_data.append(activity_data[feature].dropna().values)\n",
        "                labels.append(activity.replace('_', ' ').title())\n",
        "                colors_list.append(EDA_CONFIG['colors'][activity])\n",
        "\n",
        "        if feature_data:\n",
        "            bp = axes[row, col].boxplot(feature_data, labels=labels, patch_artist=True)\n",
        "\n",
        "            # Colorear boxplots\n",
        "            for patch, color in zip(bp['boxes'], colors_list):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "\n",
        "            axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "            plt.setp(axes[row, col].get_xticklabels(), rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{EDA_CONFIG['paths']['results']}analisis_discriminativo.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Calcular distancias entre actividades\n",
        "    print(f\"\\nüìè DISTANCIAS ENTRE ACTIVIDADES (Media Euclidiana):\")\n",
        "\n",
        "    activities = list(EDA_CONFIG['activities'])\n",
        "    distances = {}\n",
        "\n",
        "    for i, act1 in enumerate(activities):\n",
        "        for j, act2 in enumerate(activities[i+1:], i+1):\n",
        "            if act1 in activity_stats and act2 in activity_stats:\n",
        "                # Calcular distancia euclidiana entre medias\n",
        "                distance = 0\n",
        "                valid_features = 0\n",
        "\n",
        "                for feature in key_features:\n",
        "                    if feature in activity_stats[act1] and feature in activity_stats[act2]:\n",
        "                        diff = activity_stats[act1][feature]['mean'] - activity_stats[act2][feature]['mean']\n",
        "                        distance += diff ** 2\n",
        "                        valid_features += 1\n",
        "\n",
        "                if valid_features > 0:\n",
        "                    distance = np.sqrt(distance / valid_features)\n",
        "                    distances[f\"{act1} ‚Üî {act2}\"] = distance\n",
        "\n",
        "                    print(f\"   {act1.replace('_', ' ').title()} ‚Üî {act2.replace('_', ' ').title()}: {distance:.4f}\")\n",
        "\n",
        "    # Encontrar actividades m√°s similares y m√°s diferentes\n",
        "    if distances:\n",
        "        most_similar = min(distances.items(), key=lambda x: x[1])\n",
        "        most_different = max(distances.items(), key=lambda x: x[1])\n",
        "\n",
        "        print(f\"\\nüîç ACTIVIDADES M√ÅS SIMILARES: {most_similar[0]} (distancia: {most_similar[1]:.4f})\")\n",
        "        print(f\"üîç ACTIVIDADES M√ÅS DIFERENTES: {most_different[0]} (distancia: {most_different[1]:.4f})\")\n",
        "\n",
        "    return {\n",
        "        'activity_stats': activity_stats,\n",
        "        'distances': distances,\n",
        "        'most_similar': most_similar if distances else None,\n",
        "        'most_different': most_different if distances else None\n",
        "    }\n",
        "\n",
        "# Ejecutar an√°lisis discriminativo\n",
        "if landmarks_df is not None:\n",
        "    discriminative_results = discriminative_analysis(landmarks_df)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos para an√°lisis discriminativo\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYVm1EPSK7vU",
        "outputId": "dc6f00aa-ae37-48e2-a795-b29e1c997b37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos para an√°lisis discriminativo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RESUMEN EJECUTIVO Y CONCLUSIONES\n",
        "def generate_executive_summary(df, quality_analysis, movement_analysis, discriminative_results):\n",
        "    \"\"\"Generar resumen ejecutivo del EDA\"\"\"\n",
        "    print(\"üìã RESUMEN EJECUTIVO - EDA LANDMARKS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Estad√≠sticas generales\n",
        "    total_videos = df['video_file'].nunique()\n",
        "    total_frames = len(df)\n",
        "    total_participants = df['participant'].nunique()\n",
        "    total_activities = df['activity'].nunique()\n",
        "\n",
        "    print(f\"üìä ESTAD√çSTICAS GENERALES:\")\n",
        "    print(f\"   üé¨ Total de videos procesados: {total_videos}\")\n",
        "    print(f\"   üìä Total de frames analizados: {total_frames:,}\")\n",
        "    print(f\"   üë• Participantes del equipo: {total_participants}\")\n",
        "    print(f\"   üéØ Actividades diferentes: {total_activities}\")\n",
        "    print(f\"   ‚è±Ô∏è Promedio frames por video: {total_frames/total_videos:.0f}\")\n",
        "\n",
        "    # Calidad de datos\n",
        "    if quality_analysis:\n",
        "        avg_quality = quality_analysis['average_quality']\n",
        "        print(f\"\\n‚úÖ CALIDAD DE DETECCI√ìN MEDIAPIPE:\")\n",
        "        print(f\"   üìà Calidad promedio: {avg_quality:.1f}%\")\n",
        "        print(f\"   üéØ Calidad clasificaci√≥n: {'EXCELENTE' if avg_quality > 90 else 'BUENA' if avg_quality > 80 else 'ACEPTABLE'}\")\n",
        "\n",
        "        # Mejor y peor actividad en t√©rminos de calidad\n",
        "        if 'quality_by_activity' in quality_analysis:\n",
        "            activity_qualities = {k: v['mean'] for k, v in quality_analysis['quality_by_activity'].items()}\n",
        "            best_activity = max(activity_qualities.items(), key=lambda x: x[1])\n",
        "            worst_activity = min(activity_qualities.items(), key=lambda x: x[1])\n",
        "\n",
        "            print(f\"   ü•á Mejor actividad: {best_activity[0].replace('_', ' ').title()} ({best_activity[1]:.1f}%)\")\n",
        "            print(f\"   üìâ Actividad con desaf√≠os: {worst_activity[0].replace('_', ' ').title()} ({worst_activity[1]:.1f}%)\")\n",
        "\n",
        "    # Distribuci√≥n de datos\n",
        "    print(f\"\\nüìà DISTRIBUCI√ìN DE DATOS:\")\n",
        "    activity_distribution = df['activity'].value_counts()\n",
        "    most_represented = activity_distribution.iloc[0]\n",
        "    least_represented = activity_distribution.iloc[-1]\n",
        "    balance_ratio = least_represented / most_represented\n",
        "\n",
        "    print(f\"   ‚öñÔ∏è Balance del dataset: {balance_ratio:.2f} (1.0 = perfecto)\")\n",
        "    print(f\"   üìä Estado balance: {'‚úÖ BALANCEADO' if balance_ratio > 0.7 else '‚ö†Ô∏è DESBALANCEADO'}\")\n",
        "    print(f\"   üîù Actividad m√°s representada: {activity_distribution.index[0].replace('_', ' ').title()} ({most_represented} frames)\")\n",
        "    print(f\"   üìâ Actividad menos representada: {activity_distribution.index[-1].replace('_', ' ').title()} ({least_represented} frames)\")\n",
        "\n",
        "    # Hallazgos principales\n",
        "    print(f\"\\nüîç HALLAZGOS PRINCIPALES:\")\n",
        "\n",
        "    if discriminative_results and discriminative_results.get('most_similar') and discriminative_results.get('most_different'):\n",
        "        most_similar = discriminative_results['most_similar']\n",
        "        most_different = discriminative_results['most_different']\n",
        "\n",
        "        print(f\"   ü§ù Actividades m√°s similares: {most_similar[0]}\")\n",
        "        print(f\"   üÜö Actividades m√°s diferentes: {most_different[0]}\")\n",
        "        print(f\"   üìè Ratio separabilidad: {most_different[1]/most_similar[1]:.2f}x\")\n",
        "\n",
        "    if movement_analysis:\n",
        "        # Actividad con mayor variabilidad de movimiento\n",
        "        movement_vars = {k: v['avg_variability'] for k, v in movement_analysis.items()}\n",
        "        if movement_vars:\n",
        "            most_dynamic = max(movement_vars.items(), key=lambda x: x[1])\n",
        "            least_dynamic = min(movement_vars.items(), key=lambda x: x[1])\n",
        "\n",
        "            print(f\"   üèÉ Actividad m√°s din√°mica: {most_dynamic[0].replace('_', ' ').title()}\")\n",
        "            print(f\"   üßò Actividad m√°s est√°tica: {least_dynamic[0].replace('_', ' ').title()}\")\n",
        "\n",
        "    # Recomendaciones\n",
        "    print(f\"\\nüí° RECOMENDACIONES PARA MODELADO:\")\n",
        "\n",
        "    # Basado en calidad\n",
        "    if quality_analysis:\n",
        "        if quality_analysis['average_quality'] > 85:\n",
        "            print(f\"   ‚úÖ Dataset apto para entrenamiento directo\")\n",
        "        else:\n",
        "            print(f\"   üîß Considerar filtrado por calidad (umbral: 70%)\")\n",
        "\n",
        "    # Basado en balance\n",
        "    if balance_ratio < 0.6:\n",
        "        print(f\"   ‚öñÔ∏è Considerar aumentaci√≥n de datos para actividades menos representadas\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Balance adecuado entre actividades\")\n",
        "\n",
        "    # Basado en separabilidad\n",
        "    if discriminative_results and discriminative_results.get('most_similar'):\n",
        "        similar_distance = discriminative_results['most_similar'][1]\n",
        "        if similar_distance < 0.1:\n",
        "            print(f\"   üéØ Actividades similares pueden requerir caracter√≠sticas adicionales\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Actividades bien diferenciadas para clasificaci√≥n\")\n",
        "\n",
        "    # Preparaci√≥n para siguiente etapa\n",
        "    print(f\"\\nüöÄ PREPARACI√ìN PARA ENTREGA 2:\")\n",
        "    print(f\"   üìä Dataset validado y caracterizado\")\n",
        "    print(f\"   üéØ {total_videos} videos procesados exitosamente\")\n",
        "    print(f\"   üìà Patrones de movimiento identificados\")\n",
        "    print(f\"   ü§ñ Listo para entrenamiento de modelos ML\")\n",
        "\n",
        "    # Guardar resumen\n",
        "    summary_data = {\n",
        "        'generation_date': datetime.now().isoformat(),\n",
        "        'dataset_stats': {\n",
        "            'total_videos': total_videos,\n",
        "            'total_frames': total_frames,\n",
        "            'participants': total_participants,\n",
        "            'activities': total_activities\n",
        "        },\n",
        "        'quality_stats': quality_analysis if quality_analysis else {},\n",
        "        'movement_stats': movement_analysis if movement_analysis else {},\n",
        "        'discriminative_stats': {\n",
        "            'most_similar': discriminative_results.get('most_similar') if discriminative_results else None,\n",
        "            'most_different': discriminative_results.get('most_different') if discriminative_results else None\n",
        "        },\n",
        "        'balance_ratio': balance_ratio,\n",
        "        'recommendations': [\n",
        "            f\"Dataset apto para entrenamiento\" if quality_analysis and quality_analysis['average_quality'] > 85 else \"Filtrar por calidad\",\n",
        "            f\"Balance adecuado\" if balance_ratio > 0.6 else \"Considerar aumentaci√≥n de datos\",\n",
        "            \"Listo para modelado ML\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with open(f\"{EDA_CONFIG['paths']['results']}resumen_ejecutivo.json\", 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüíæ Resumen guardado en: {EDA_CONFIG['paths']['results']}resumen_ejecutivo.json\")\n",
        "\n",
        "    return summary_data\n",
        "\n",
        "# Generar resumen ejecutivo si tenemos todos los an√°lisis\n",
        "if landmarks_df is not None:\n",
        "    executive_summary = generate_executive_summary(\n",
        "        landmarks_df,\n",
        "        quality_analysis if 'quality_analysis' in locals() else None,\n",
        "        movement_analysis if 'movement_analysis' in locals() else None,\n",
        "        discriminative_results if 'discriminative_results' in locals() else None\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos suficientes para generar resumen ejecutivo\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7LEoRnUK9j8",
        "outputId": "3225c0d4-b8c6-4587-d9f1-f77b9651c36c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No hay datos suficientes para generar resumen ejecutivo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Checklist EDA Completado\n",
        "\n",
        "### An√°lisis Realizados:\n",
        "- [x] **Carga de datos** de landmarks\n",
        "- [x] **Informaci√≥n b√°sica** del dataset\n",
        "- [x] **Visualizaciones de distribuci√≥n** por actividad y participante\n",
        "- [x] **An√°lisis de calidad** de detecci√≥n MediaPipe\n",
        "- [x] **Patrones de movimiento** por actividad\n",
        "- [x] **Matriz de correlaci√≥n** entre landmarks\n",
        "- [x] **An√°lisis discriminativo** entre actividades\n",
        "- [x] **Resumen ejecutivo** con conclusiones\n",
        "\n",
        "### Resultados Generados:\n",
        "- üìä **Estad√≠sticas descriptivas** completas\n",
        "- üìà **Visualizaciones** guardadas en `data/eda_results/`\n",
        "- üîç **An√°lisis de calidad** MediaPipe por actividad\n",
        "- üéØ **Caracter√≠sticas discriminativas** identificadas\n",
        "- üí° **Recomendaciones** para modelado futuro\n",
        "\n",
        "### Archivos Generados:\n",
        "- `distribuciones_dataset.png` - Visualizaciones de distribuci√≥n\n",
        "- `calidad_deteccion.png` - An√°lisis de calidad MediaPipe\n",
        "- `correlacion_landmarks.png` - Matriz de correlaci√≥n\n",
        "- `analisis_discriminativo.png` - Caracter√≠sticas por actividad\n",
        "- `resumen_ejecutivo.json` - Resumen completo del an√°lisis\n",
        "\n",
        "---\n",
        "**Estado**: EDA completado y documentado\n",
        "**Siguiente paso**: Preparaci√≥n para Entrega 2 - Modelado ML\n"
      ],
      "metadata": {
        "id": "jukkfBCEK_gU"
      }
    }
  ]
}