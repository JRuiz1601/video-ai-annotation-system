# Sistema de Anotaci√≥n de Video para An√°lisis de Actividades

#### -- Estado del Proyecto: Activo ‚úÖ

## Miembros del Equipo

|Nombre     |  Email   | 
|-----------|-----------------|
[Juan Esteban Ruiz](https://github.com/JRuiz1601)| juan.ruizome@u.icesi.edu.co |
|[Juan David Quintero](https://github.com/Juanda2005123) | juan.quintero@u.icesi.edu.co |
|[Tomas Quintero](https://github.com/tomasquin2003) | tomas.quintero@u.icesi.edu.co |

## Introducci√≥n/Objetivo del Proyecto
El prop√≥sito de este proyecto es desarrollar un sistema automatizado de clasificaci√≥n de actividades humanas b√°sicas utilizando an√°lisis de coordenadas articulares extra√≠das mediante MediaPipe. El sistema identificar√° cinco actividades espec√≠ficas: caminar hacia la c√°mara, caminar de regreso, girar, sentarse y ponerse de pie, con una precisi√≥n superior al 85% y capacidad de procesamiento en tiempo real. Este desarrollo contribuye al avance de sistemas de an√°lisis de movimiento no invasivos aplicables en rehabilitaci√≥n, deporte e investigaci√≥n biomec√°nica.

### Metodolog√≠as Utilizadas
* An√°lisis Exploratorio de Datos (EDA)
* Aprendizaje Autom√°tico Supervisado
* Visualizaci√≥n de Datos
* Modelado Predictivo
* Procesamiento de Video en Tiempo Real
* Metodolog√≠a CRISP-DM
* Validaci√≥n Cruzada
* Feature Engineering
* SMOTE (Synthetic Minority Over-sampling Technique)
* PCA (Principal Component Analysis)

### Tecnolog√≠as
* Python 3.10
* MediaPipe 0.10.21 (Google)
* OpenCV
* Scikit-learn
* Random Forest
* Pandas, NumPy
* Matplotlib, Seaborn
* Jupyter Notebooks
* Gradio (Deployment)
* Git/GitHub

## Descripci√≥n del Proyecto
Este sistema utiliza la biblioteca MediaPipe de Google para extraer coordenadas de 33 puntos clave articulares de videos en tiempo real. A partir de estas coordenadas (x,y,z,visibility), se calculan 83 caracter√≠sticas geom√©tricas (distancias, √°ngulos, ratios) que se reducen a 16 componentes mediante PCA. El modelo Random Forest entrenado clasifica autom√°ticamente las actividades realizadas con **98.76% de accuracy** en test set.

**Fuentes de Datos**: 90 videos de 18 personas realizando las 5 actividades espec√≠ficas, capturados en condiciones controladas (√°ngulo frontal, iluminaci√≥n estable, fondo limpio). Dataset final de 7,352 frames balanceados mediante SMOTE.

**An√°lisis y Modelado**: 
- Extracci√≥n de 132 features crudas (33 landmarks √ó 4 coordenadas)
- C√°lculo de 83 caracter√≠sticas geom√©tricas (distancias, √°ngulos, ratios)
- Normalizaci√≥n StandardScaler
- Reducci√≥n PCA a 16 componentes (95.1% varianza explicada)
- Entrenamiento Random Forest con validaci√≥n cruzada
- Bootstrap confidence intervals (IC 95%: [98.0%, 99.4%])

**Desaf√≠os Principales**:
- ‚úÖ **Resueltos en Offline:**
  - Variabilidad en movimientos humanos entre diferentes usuarios
  - Normalizaci√≥n para diferentes tipos de cuerpo y distancias de c√°mara
  - Overfitting (validado con test accuracy superior a validation)
  - Data leakage (verificado con tests forenses)
  
- ‚ö†Ô∏è **Identificados en Deployment:**
  - **Gap offline-online cr√≠tico:** 98.76% offline vs ~40% online
  - Falta de contexto temporal (frame-by-frame vs secuencias)
  - Condiciones no controladas (iluminaci√≥n variable, √°ngulos diversos, fondos desordenados)
  - Baja diversidad de datos de entrenamiento (18 personas, 1 √°ngulo)

## Comenzando

### Opci√≥n 1: Clonar Repositorio (Para Desarrollo)

1. Clona este repositorio:
```bash
git clone https://github.com/JRuiz1601/video-ai-annotation-system.git
cd video-ai-annotation-system
```

2. Los datos sin procesar se mantienen en `Entrega1/data/videos/` dentro de este repositorio.
   *Los videos originales se almacenan localmente debido a su tama√±o. Para obtener acceso, contacta al equipo.*

3. Los scripts de procesamiento/transformaci√≥n de datos est√°n en `Entrega1/src/data/`

4. Los notebooks de an√°lisis est√°n distribuidos por entregas (ver secci√≥n siguiente)

5. **Instalaci√≥n y Setup**:
```bash
cd Entrega2/
pip install -r requirements.txt
```

### Opci√≥n 2: Probar el Sistema Desplegado (Recomendado para Demo)

‚ö†Ô∏è **IMPORTANTE:** El servidor Gradio en Google Colab se desconecta autom√°ticamente despu√©s de **90 minutos de inactividad**. Si el link caduca, sigue estos pasos para re-lanzar el demo:

#### Instrucciones de Despliegue R√°pido:

1Ô∏è‚É£ **Abre Google Colab:**
   - Ve a [https://colab.research.google.com/](https://colab.research.google.com/)
   - Click en "Archivo" ‚Üí "Abrir notebook"
   - Selecciona la pesta√±a **"GitHub"**
   - Pega la URL: `https://github.com/JRuiz1601/video-ai-annotation-system`
   - Abre el notebook: 07_gradio_webcam_demo.ipynb

2Ô∏è‚É£ **Sube los Modelos Entrenados:**
   
   Cuando ejecutes la **Celda 3** (secci√≥n "SUBIR Y CARGAR MODELOS"), se abrir√° un bot√≥n de carga. Debes subir estos **4 archivos** en orden:

   ```
   üì¶ Archivos requeridos (ubicados en el repositorio):
   
   1. randomforest_model.pkl       (Entrega2/data/trained_models/)
   2. scaler.pkl                    (Entrega2/data/models/transformers/)
   3. pca.pkl                       (Entrega2/data/models/transformers/)
   4. label_encoder.pkl             (Entrega2/data/models/transformers/)
   ```

   **Descarga directa desde GitHub:**
   - Opci√≥n A: Clona el repo y localiza los archivos
   - Opci√≥n B: [Descarga el ZIP del proyecto](https://github.com/JRuiz1601/video-ai-annotation-system/archive/refs/heads/main.zip) y extrae los archivos

3Ô∏è‚É£ **Ejecuta las Celdas en Orden:**

   - **Celda 1 (Instalaci√≥n de Dependencias):**
     ```python
     # Al ejecutar, aparecer√° una advertencia de NumPy
     ‚ö†Ô∏è ADVERTENCIA: "Restart session to use updated packages"
     
     üëâ Click en "RESTART SESSION" (bot√≥n rojo que aparece)
     üëâ Luego contin√∫a con la siguiente celda
     ```
   
   - **Celdas 2-6:** Ejecuta normalmente (Shift + Enter en cada una)
   
   - **Celda 7 (Lanzar Aplicaci√≥n):**
     ```python
     # Esta celda generar√°:
     ‚úÖ URL Local:   http://127.0.0.1:7860
     ‚úÖ URL P√∫blica: https://xxxxx.gradio.live  ‚Üê COMPARTE ESTE LINK
     ```

4Ô∏è‚É£ **Usa la Aplicaci√≥n:**
   - Click en la **URL P√∫blica** (`https://xxxxx.gradio.live`)
   - Permite acceso a tu c√°mara cuando el navegador lo solicite
   - Col√≥cate frente a la c√°mara (cuerpo completo visible)
   - Realiza alguna de las 5 actividades:
     - üö∂ Caminar hacia la c√°mara
     - üö∂‚Äç‚ôÇÔ∏è Caminar de regreso
     - üîÑ Girar
     - üßç Ponerse de pie
     - ü™ë Sentarse

5Ô∏è‚É£ **Si el Link Expira:**
   - Vuelve a Colab
   - Ejecuta solo la **Celda 7** nuevamente
   - Obt√©n un nuevo link p√∫blico

---

## Entregas y Documentaci√≥n Principal

### üìÇ Entrega 1 (13 octubre 2025) - ‚úÖ Completa
* Documento de Fundamentos - Preguntas, metodolog√≠a, m√©tricas y EDA
* 01_setup_mediapipe.ipynb - Configuraci√≥n inicial del pipeline
* 02_eda_inicial.ipynb - An√°lisis exploratorio de coordenadas
* **Resultados:** 90 videos procesados, 7,352 frames, 33 landmarks por frame

### üìÇ Entrega 2 (27 octubre 2025) - ‚úÖ Completa
* 03_data_preprocessing.ipynb - SMOTE, normalizaci√≥n, splits
* 04_feature_engineering.ipynb - 83 features geom√©tricas, PCA
* 05_model_training.ipynb - Random Forest vs MLP
* 06_model_evaluation_realistic.ipynb - Evaluaci√≥n en test, bootstrap
* Reporte de Evaluaci√≥n Random Forest
* **Resultados:** 
  - Random Forest: **98.76% test accuracy** (12 errores de 967 frames)
  - MLP: 98.97% test accuracy (10 errores)
  - Selecci√≥n: Random Forest (3x m√°s r√°pido, interpretable)

### üìÇ Entrega 3 (17 noviembre 2025) - ‚úÖ Completa
* 07_gradio_webcam_demo.ipynb - **Demo en vivo con webcam**
* Desaf√≠os de Deployment - Gap offline-online documentado
* An√°lisis de Impactos - Evaluaci√≥n en contexto real
* Plan de Despliegue - Arquitectura y estrategia
* **Resultados:**
  - ‚úÖ Deployment funcional en Gradio (Google Colab)
  - ‚ö†Ô∏è **Gap cr√≠tico:** 98.76% offline vs ~40% online
  - ‚ö†Ô∏è Solo "Caminar Hacia" funciona bien (~85%)
  - ‚ùå Otras 4 actividades: 20-40% accuracy online

---

## Estado del Proyecto por Entregas

| Entrega | Estado | Fecha L√≠mite | Completitud | M√©tricas Clave |
|---------|--------|--------------|-------------|----------------|
| **Entrega 1** | ‚úÖ Completa | 13 octubre 2025 | 100% | 90 videos, 7,352 frames |
| **Entrega 2** | ‚úÖ Completa | 27 octubre 2025 | 100% | 98.76% test accuracy |
| **Entrega 3** | ‚úÖ Completa | 17 noviembre 2025 | 100% | Deployment funcional, gap documentado |

---

## M√©tricas Alcanzadas del Proyecto

### ‚úÖ Offline (Test Set Controlado)
- **Accuracy Global**: **98.76%** ‚úÖ (objetivo: ‚â•85%)
- **F1-Score Promedio**: **98.76%** ‚úÖ (objetivo: ‚â•80%)
- **F1-Score por Clase**: 
  - Caminar Hacia: 98.9% ‚úÖ
  - Caminar Regreso: 100.0% ‚úÖ
  - Girar: 99.6% ‚úÖ
  - Ponerse Pie: 97.9% ‚úÖ
  - Sentarse: 97.3% ‚úÖ
- **Latencia de Inferencia**: **0.003s** (<3ms) ‚úÖ (objetivo: <100ms)
- **FPS Te√≥rico**: **333 fps** ‚úÖ (objetivo: ‚â•15 fps)

### ‚ö†Ô∏è Online (Webcam en Producci√≥n)
- **Accuracy Global Estimada**: **~40%** ‚ùå (objetivo: ‚â•85%)
- **Gap Offline-Online**: **-59%** (cr√≠tico)
- **Actividades Funcionales**: Solo "Caminar Hacia" (~85%)
- **Actividades Problem√°ticas**: 
  - Caminar Regreso: ~25% (-75% gap)
  - Girar: ~35% (-64% gap)
  - Sentarse: ~25% (-71% gap)
  - Ponerse Pie: ~30% (-68% gap)

### üîç Causas del Gap (Documentadas)
1. **Falta de √°ngulos diversos** (solo frontal en training)
2. **Iluminaci√≥n homog√©nea** (training controlado)
3. **Fondos limpios** (sin clutter en training)
4. **Baja diversidad demogr√°fica** (18 personas)
5. **Sin contexto temporal** (frame-by-frame)

### üöÄ Mejoras Propuestas
- **Corto plazo:** Buffer temporal (30 frames) ‚Üí +15-20%
- **Mediano plazo:** 1,800 videos (15 personas √ó 4 √°ngulos) ‚Üí +30-40%
- **Largo plazo:** LSTM temporal ‚Üí +40-50%

---

## Estructura del Repositorio

```
video-ai-annotation-system/
‚îÇ
‚îú‚îÄ‚îÄ Entrega1/                    # Fundamentos y EDA
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ videos/              # Videos originales (90 videos)
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_setup_mediapipe.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 02_eda_inicial.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îî‚îÄ‚îÄ entrega1_fundamentos.md
‚îÇ
‚îú‚îÄ‚îÄ Entrega2/                    # Modelado y Evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processed/           # X_train, y_train, etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trained_models/      # randomforest_model.pkl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ transformers/    # scaler.pkl, pca.pkl, label_encoder.pkl
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_data_preprocessing.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_feature_engineering.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_model_training.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 06_model_evaluation_realistic.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ random_forest_evaluation_report.md
‚îÇ       ‚îî‚îÄ‚îÄ deployment_plan.md
‚îÇ
‚îú‚îÄ‚îÄ Entrega3/                    # Despliegue
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 07_gradio_webcam_demo.ipynb  ‚Üê DEMO EN VIVO
‚îÇ   ‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment_challenges.md     ‚Üê Gap offline-online
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ impact_analysis.md           ‚Üê Evaluaci√≥n de impactos
‚îÇ   ‚îî‚îÄ‚îÄ video/
‚îÇ       ‚îî‚îÄ‚îÄ project_demo.mp4             ‚Üê Video de presentaci√≥n (10 min)
‚îÇ
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

---

## Lecciones Aprendidas

### üéì T√©cnicas
1. **"98% offline ‚â† 98% online"** - La evaluaci√≥n en test set controlado NO garantiza performance en producci√≥n
2. **Contexto temporal es cr√≠tico** - Actividades humanas son secuencias, no frames aislados
3. **Diversidad de datos > Cantidad** - 90 videos de 18 personas < 300 videos de 10 personas en 4 √°ngulos
4. **Prototipo ‚â† Producto** - Demo funcional requiere √≥rdenes de magnitud m√°s trabajo para producci√≥n

### üî¨ Metodol√≥gicas
5. **Data leakage forense** - Verificar matem√°ticamente 0 duplicados entre train/val/test
6. **Bootstrap confidence intervals** - Validar robustez estad√≠stica (IC 95%: [98.0%, 99.4%])
7. **Tests unitarios de features** - Asegurar feature parity entre training y serving
8. **Monitoreo continuo** - Detectar drift de features en producci√≥n

---

## Publicaciones y Referencias

### Papers Implementados
1. **MediaPipe Pose** - Bazarevsky et al. (2020) - Google Research
2. **SMOTE** - Chawla et al. (2002) - Journal of Artificial Intelligence Research
3. **Random Forest** - Breiman (2001) - Machine Learning

### Trabajos Relacionados Citados
4. **"Real-world HAR using Smartphone Sensors"** (IEEE Sensors 2019) - Documenta gap 30-40% lab vs wild
5. **"Temporal Segment Networks for Action Recognition"** (ECCV 2016) - Superioridad modelos temporales
6. **"Bridging the Gap between Training and Inference"** (CVPR 2022) - Data augmentation para producci√≥n

---

## Contribuciones

Este proyecto est√° abierto a contribuciones. Si encuentras bugs, tienes sugerencias o quieres mejorar el sistema:

1. Fork el repositorio
2. Crea una rama (`git checkout -b feature/mejora`)
3. Commit tus cambios (`git commit -m 'Agrega nueva feature'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

**√Åreas de mejora prioritarias:**
- üî¥ Solucionar gap offline-online (ver `deployment_challenges.md`)
- üü° Expandir dataset con m√°s personas y √°ngulos
- üü° Implementar buffer temporal (30 frames)
- üü¢ Migrar a modelos temporales (LSTM)

---

**Universidad ICESI** | **Facultad de Ingenier√≠a, Dise√±o y Ciencias Aplicadas** | **Inteligencia Artificial 1** | **2025-2**